{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import importlib\n",
    "import utils\n",
    "import models\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "\n",
    "# Assuming you know the dtype and shape, for example:\n",
    "# For trainX_sequences\n",
    "dtype_trainX_sequences = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_sequences = (3038976, 16)  # replace with the correct shape\n",
    "trainX_sequences = load_memmap('./../data/jan-march/trainX_sequences.memmap', dtype_trainX_sequences, shape_trainX_sequences)\n",
    "\n",
    "# For trainX\n",
    "dtype_trainX = np.int64  # or the correct dtype for your data\n",
    "shape_trainX = (3038976, 12, 8, 8)  # replace with the correct shape\n",
    "trainX_boards = load_memmap('./../data/jan-march/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int64  # or the correct dtype for your data\n",
    "shape_trainY = (3038976,)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/jan-march/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "# For trainX_seqlengths\n",
    "dtype_trainX_seqlengths = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_seqlengths = (3038976,)  # replace with the correct shape\n",
    "trainX_seqlengths = load_memmap('./../data/jan-march/trainX_seqlengths.memmap', dtype_trainX_seqlengths, shape_trainX_seqlengths)\n",
    "\n",
    "with open('./../data/jan-march/vocab.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw data, we need to be able to make sense of chess moves. Meaning, we're transforming our entire world from chess moves into numerical tokens that will serve as indices into unique embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just use nn.Embedding later when we pass the model a sequence of indices, but this is if we ever want to pre-train and have access to the matrix we've trained\n",
    "def get_embedding_matrix(vocab, d_embed):\n",
    "    n_embed = len(vocab.move_to_id)\n",
    "    return np.random.normal(0, 1, (n_embed, d_embed))\n",
    "# embedding_matrix = get_embedding_matrix(vocab, 64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8121\n",
      "torch.Size([12, 8, 8])\n",
      "[[[0 0 0 0 0 0 0 0]\n",
      "  [1 1 0 0 1 0 1 1]\n",
      "  [0 0 1 0 0 1 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 1 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 1 0 0]\n",
      "  [0 0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [1 1 1 1 0 1 1 1]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.id_to_move.keys()))\n",
    "print(torch.tensor(trainX_boards[0]).shape)\n",
    "print(trainX_boards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(trainY[:5000]),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for boards, sequences, lengths, labels in train_loader:\n",
    "            count += 1\n",
    "            boards, sequences, lengths, labels = boards.to(device, non_blocking=True), sequences.to(device, non_blocking=True), lengths, labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            output = model(boards, sequences, lengths)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                validation_loss = 0\n",
    "\n",
    "                for boards, sequences, lengths, labels in val_loader:\n",
    "                    boards, sequences, lengths, labels = boards.to(device, non_blocking=True), sequences.to(device, non_blocking=True), lengths, labels.to(device, non_blocking=True)\n",
    "                    outputs = model(boards, sequences, lengths)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    val_top3_correct += top_3_accuracy(labels, outputs) * labels.size(0)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    validation_loss += loss.item()\n",
    "\n",
    "                val_loss_values.append(validation_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        if epoch <= 10:\n",
    "            for op_params in optimizer.param_groups:\n",
    "                op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3038976\n"
     ]
    }
   ],
   "source": [
    "dataset = MultimodalDataset(trainX_sequences, trainX_boards, trainX_seqlengths, trainY)\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 (CNN has 2 Convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathaniel/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490555\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 72\n",
    "d_embed = 32\n",
    "NUM_EPOCHS = 25\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModal(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.7 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 6.682768297195435, Validation Error: 91.66666666666667, Validation Top-3 Accuracy: 8.33333358168602, Training Error: 98.64176570458405\n",
      "Epoch 2, Training Loss: 5.682501029968262, Validation Error: 91.66666666666667, Validation Top-3 Accuracy: 8.33333358168602, Training Error: 94.90662139219015\n",
      "Epoch 3, Training Loss: 4.880701231956482, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 92.69949066213923\n",
      "Epoch 4, Training Loss: 4.429151296615601, Validation Error: 91.66666666666667, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 89.64346349745331\n",
      "Epoch 5, Training Loss: 4.106862878799438, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 84.04074702886248\n",
      "Epoch 6, Training Loss: 3.8884287595748903, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 83.02207130730051\n",
      "Epoch 7, Training Loss: 3.7301934957504272, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 80.98471986417657\n",
      "Epoch 8, Training Loss: 3.617365038394928, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 77.58913412563668\n",
      "Epoch 9, Training Loss: 3.5376277327537538, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 75.72156196943973\n",
      "Epoch 10, Training Loss: 3.4814242839813234, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 74.87266553480475\n",
      "Epoch 11, Training Loss: 3.4420204639434813, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 74.36332767402376\n",
      "Epoch 12, Training Loss: 3.4145872712135317, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 73.85398981324278\n",
      "Epoch 13, Training Loss: 3.3931331992149354, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 73.3446519524618\n",
      "Epoch 14, Training Loss: 3.3715712785720826, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 72.66553480475382\n",
      "Epoch 15, Training Loss: 3.3506837368011473, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 72.32597623089983\n",
      "Epoch 16, Training Loss: 3.329383409023285, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 71.81663837011885\n",
      "Epoch 17, Training Loss: 3.3081316232681273, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 71.64685908319186\n",
      "Epoch 18, Training Loss: 3.2866799592971803, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 71.30730050933786\n",
      "Epoch 19, Training Loss: 3.2652185440063475, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 70.79796264855688\n",
      "Epoch 20, Training Loss: 3.2440505743026735, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 70.62818336162988\n",
      "Epoch 21, Training Loss: 3.2227064847946165, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 70.28862478777589\n",
      "Epoch 22, Training Loss: 3.201303219795227, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 70.11884550084889\n",
      "Epoch 23, Training Loss: 3.179571670293808, Validation Error: 87.5, Validation Top-3 Accuracy: 16.66666716337204, Training Error: 69.43972835314092\n",
      "Epoch 24, Training Loss: 3.158334803581238, Validation Error: 87.5, Validation Top-3 Accuracy: 12.5, Training Error: 69.26994906621393\n",
      "Epoch 25, Training Loss: 3.13666113615036, Validation Error: 87.5, Validation Top-3 Accuracy: 12.5, Training Error: 68.93039049235993\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we filter out illegal moves in our prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "Validation Accuracy: 35.87428571428571%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_size = int(total_size * 0.04)\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "if val_loader is not None:\n",
    "    with torch.no_grad():\n",
    "        for boards, sequences, lengths, labels in val_loader:\n",
    "            boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            outputs = model(boards, sequences, lengths)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            minus = 0\n",
    "            for idx, (sequence, label) in enumerate(zip(sequences, labels)):\n",
    "                # This tells us we're looking at games that include the opening but has developed more than the first 4 half-moves\n",
    "                if sequence[-1].item() == 0 and sequence[2].item() != 0 and sequence[3].item() != 0 and sequence[4].item() != 0:\n",
    "                    output = probabilities[idx]\n",
    "                    sorted_probs, sorted_indices = torch.sort(output, descending=True)\n",
    "                    predicted_move = sorted_indices[0]\n",
    "                    # print(predicted_move)\n",
    "                    chess_board = load_board_state_from_san(sequence, vocab)\n",
    "                    for move_idx in sorted_indices:\n",
    "                        move = vocab.get_move(move_idx.item()) # Convert index to move (e.g., 'e2e4')\n",
    "                        if is_legal_move(chess_board, move):\n",
    "                            # print(\"we found one\")\n",
    "                            predicted_move = vocab.get_id(move)\n",
    "                            break\n",
    "                    \n",
    "                    # Check if predicted move is correct\n",
    "                    correct_move = label.item() # Convert label to move\n",
    "                    # print(correct_move)\n",
    "                    if predicted_move == correct_move:\n",
    "                        val_correct += 1\n",
    "                else:\n",
    "                    minus += 1\n",
    "            val_total += (labels.size(0) - minus)\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multimodalmodel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 (CNN has 3 convolutions,SE, ReLu after combining inputs and larger RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634630\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131882\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 100\n",
    "d_embed = 48\n",
    "NUM_EPOCHS = 15\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalTwo(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 5.358224760532379\n",
      "Epoch 1, Batch: 2000| Training Loss: 5.12661018884182\n",
      "Epoch 1, Batch: 3000| Training Loss: 4.984487212657928\n",
      "Epoch 1, Batch: 4000| Training Loss: 4.880090206086636\n",
      "Epoch 1, Batch: 5000| Training Loss: 4.78517461733818\n",
      "Epoch 1, Batch: 6000| Training Loss: 4.7084750316937765\n",
      "Epoch 1, Batch: 7000| Training Loss: 4.644520375422069\n",
      "Epoch 1, Batch: 8000| Training Loss: 4.584236370325089\n",
      "Epoch 1, Batch: 9000| Training Loss: 4.53359999956025\n",
      "Epoch 1, Batch: 10000| Training Loss: 4.487822722411155\n",
      "Epoch 1, Batch: 11000| Training Loss: 4.445587667400186\n",
      "Epoch 1, Batch: 12000| Training Loss: 4.40721798068285\n",
      "Epoch 1, Batch: 13000| Training Loss: 4.372255387526292\n",
      "Epoch 1, Batch: 14000| Training Loss: 4.34038493510655\n",
      "Epoch 1, Batch: 15000| Training Loss: 4.310646494738261\n",
      "Epoch 1, Batch: 16000| Training Loss: 4.282993175700307\n",
      "Epoch 1, Batch: 17000| Training Loss: 4.257371399865431\n",
      "Epoch 1, Batch: 18000| Training Loss: 4.234443660351965\n",
      "Epoch 1, Batch: 19000| Training Loss: 4.212246252285807\n",
      "Epoch 1, Batch: 20000| Training Loss: 4.192180788624286\n",
      "Epoch 1, Batch: 21000| Training Loss: 4.1727453940822965\n",
      "Epoch 1, Batch: 22000| Training Loss: 4.153056076591665\n",
      "Epoch 1, Batch: 23000| Training Loss: 4.134850503247717\n",
      "Epoch 1, Batch: 24000| Training Loss: 4.117725422114134\n",
      "Epoch 1, Batch: 25000| Training Loss: 4.100960772876739\n",
      "Epoch 1, Training Loss: 4.092024691895884, Validation Error: 78.61563943858425, Validation Top-3 Accuracy: 38.39246796277512, Training Error: 81.66166043691845\n",
      "Epoch 2, Batch: 1000| Training Loss: 3.651757910966873\n",
      "Epoch 2, Batch: 2000| Training Loss: 3.6435871212482454\n",
      "Epoch 2, Batch: 3000| Training Loss: 3.636805051167806\n",
      "Epoch 2, Batch: 4000| Training Loss: 3.6328137121200563\n",
      "Epoch 2, Batch: 5000| Training Loss: 3.6196360639572145\n",
      "Epoch 2, Batch: 6000| Training Loss: 3.611985718011856\n",
      "Epoch 2, Batch: 7000| Training Loss: 3.606109873499189\n",
      "Epoch 2, Batch: 8000| Training Loss: 3.5995867279171945\n",
      "Epoch 2, Batch: 9000| Training Loss: 3.5946447857485877\n",
      "Epoch 2, Batch: 10000| Training Loss: 3.589093448472023\n",
      "Epoch 2, Batch: 11000| Training Loss: 3.5832591432874854\n",
      "Epoch 2, Batch: 12000| Training Loss: 3.5772508108615875\n",
      "Epoch 2, Batch: 13000| Training Loss: 3.572073632130256\n",
      "Epoch 2, Batch: 14000| Training Loss: 3.5667808487585613\n",
      "Epoch 2, Batch: 15000| Training Loss: 3.5611668466091158\n",
      "Epoch 2, Batch: 16000| Training Loss: 3.5557396154105665\n",
      "Epoch 2, Batch: 17000| Training Loss: 3.5509392418580896\n",
      "Epoch 2, Batch: 18000| Training Loss: 3.5468732714255653\n",
      "Epoch 2, Batch: 19000| Training Loss: 3.5419234688031045\n",
      "Epoch 2, Batch: 20000| Training Loss: 3.5383571269750593\n",
      "Epoch 2, Batch: 21000| Training Loss: 3.5342637304010847\n",
      "Epoch 2, Batch: 22000| Training Loss: 3.5291866980357605\n",
      "Epoch 2, Batch: 23000| Training Loss: 3.5244237489182018\n",
      "Epoch 2, Batch: 24000| Training Loss: 3.520382763793071\n",
      "Epoch 2, Batch: 25000| Training Loss: 3.51564538479805\n",
      "Epoch 2, Training Loss: 3.513015216336479, Validation Error: 76.1470955743469, Validation Top-3 Accuracy: 42.37352163435272, Training Error: 76.12114056391967\n",
      "Epoch 3, Batch: 1000| Training Loss: 3.3968165423870085\n",
      "Epoch 3, Batch: 2000| Training Loss: 3.3915503358840944\n",
      "Epoch 3, Batch: 3000| Training Loss: 3.388434473911921\n",
      "Epoch 3, Batch: 4000| Training Loss: 3.386762285888195\n",
      "Epoch 3, Batch: 5000| Training Loss: 3.376897746324539\n",
      "Epoch 3, Batch: 6000| Training Loss: 3.371298305829366\n",
      "Epoch 3, Batch: 7000| Training Loss: 3.367647926398686\n",
      "Epoch 3, Batch: 8000| Training Loss: 3.3633185525238516\n",
      "Epoch 3, Batch: 9000| Training Loss: 3.360056888686286\n",
      "Epoch 3, Batch: 10000| Training Loss: 3.356820752286911\n",
      "Epoch 3, Batch: 11000| Training Loss: 3.353009033203125\n",
      "Epoch 3, Batch: 12000| Training Loss: 3.349058154940605\n",
      "Epoch 3, Batch: 13000| Training Loss: 3.3461518198160025\n",
      "Epoch 3, Batch: 14000| Training Loss: 3.3431409624814985\n",
      "Epoch 3, Batch: 15000| Training Loss: 3.3394856364568075\n",
      "Epoch 3, Batch: 16000| Training Loss: 3.3361066677719355\n",
      "Epoch 3, Batch: 17000| Training Loss: 3.333304230633904\n",
      "Epoch 3, Batch: 18000| Training Loss: 3.330866246461868\n",
      "Epoch 3, Batch: 19000| Training Loss: 3.3272903495587802\n",
      "Epoch 3, Batch: 20000| Training Loss: 3.325440343296528\n",
      "Epoch 3, Batch: 21000| Training Loss: 3.3230360287825267\n",
      "Epoch 3, Batch: 22000| Training Loss: 3.3195801607262005\n",
      "Epoch 3, Batch: 23000| Training Loss: 3.316181686411733\n",
      "Epoch 3, Batch: 24000| Training Loss: 3.3136821582814058\n",
      "Epoch 3, Batch: 25000| Training Loss: 3.3103332914543153\n",
      "Epoch 3, Training Loss: 3.3084982360466917, Validation Error: 74.94406183709644, Validation Top-3 Accuracy: 44.36840728794351, Training Error: 73.71386797012168\n",
      "Epoch 4, Batch: 1000| Training Loss: 3.2426090804338457\n",
      "Epoch 4, Batch: 2000| Training Loss: 3.2394311973452568\n",
      "Epoch 4, Batch: 3000| Training Loss: 3.237845349272092\n",
      "Epoch 4, Batch: 4000| Training Loss: 3.2373722124397757\n",
      "Epoch 4, Batch: 5000| Training Loss: 3.2290585483789442\n",
      "Epoch 4, Batch: 6000| Training Loss: 3.224792125840982\n",
      "Epoch 4, Batch: 7000| Training Loss: 3.22172945197991\n",
      "Epoch 4, Batch: 8000| Training Loss: 3.2184972061961887\n",
      "Epoch 4, Batch: 9000| Training Loss: 3.2163732378085452\n",
      "Epoch 4, Batch: 10000| Training Loss: 3.2137737374186517\n",
      "Epoch 4, Batch: 11000| Training Loss: 3.2110257436253806\n",
      "Epoch 4, Batch: 12000| Training Loss: 3.2080346365670365\n",
      "Epoch 4, Batch: 13000| Training Loss: 3.2058979679896282\n",
      "Epoch 4, Batch: 14000| Training Loss: 3.2036454378621917\n",
      "Epoch 4, Batch: 15000| Training Loss: 3.2007091786146162\n",
      "Epoch 4, Batch: 16000| Training Loss: 3.198168506927788\n",
      "Epoch 4, Batch: 17000| Training Loss: 3.1960652034352806\n",
      "Epoch 4, Batch: 18000| Training Loss: 3.194248376482063\n",
      "Epoch 4, Batch: 19000| Training Loss: 3.191405326749149\n",
      "Epoch 4, Batch: 20000| Training Loss: 3.1901013551652433\n",
      "Epoch 4, Batch: 21000| Training Loss: 3.1883402648937134\n",
      "Epoch 4, Batch: 22000| Training Loss: 3.185604849192229\n",
      "Epoch 4, Batch: 23000| Training Loss: 3.1829309985896814\n",
      "Epoch 4, Batch: 24000| Training Loss: 3.181089966798822\n",
      "Epoch 4, Batch: 25000| Training Loss: 3.178336095890999\n",
      "Epoch 4, Training Loss: 3.1768458565552247, Validation Error: 74.01853950541947, Validation Top-3 Accuracy: 45.680411473447904, Training Error: 72.16097832537027\n",
      "Epoch 5, Batch: 1000| Training Loss: 3.140281164884567\n",
      "Epoch 5, Batch: 2000| Training Loss: 3.137476062178612\n",
      "Epoch 5, Batch: 3000| Training Loss: 3.136243986765544\n",
      "Epoch 5, Batch: 4000| Training Loss: 3.1367310014367105\n",
      "Epoch 5, Batch: 5000| Training Loss: 3.1296361626148226\n",
      "Epoch 5, Batch: 6000| Training Loss: 3.1260660774707794\n",
      "Epoch 5, Batch: 7000| Training Loss: 3.123539417709623\n",
      "Epoch 5, Batch: 8000| Training Loss: 3.120637203261256\n",
      "Epoch 5, Batch: 9000| Training Loss: 3.118758892522918\n",
      "Epoch 5, Batch: 10000| Training Loss: 3.11668400799036\n",
      "Epoch 5, Batch: 11000| Training Loss: 3.114330873803659\n",
      "Epoch 5, Batch: 12000| Training Loss: 3.1116949720084666\n",
      "Epoch 5, Batch: 13000| Training Loss: 3.1100655871446317\n",
      "Epoch 5, Batch: 14000| Training Loss: 3.108396685046809\n",
      "Epoch 5, Batch: 15000| Training Loss: 3.1060492034832636\n",
      "Epoch 5, Batch: 16000| Training Loss: 3.103933355100453\n",
      "Epoch 5, Batch: 17000| Training Loss: 3.1021872108052757\n",
      "Epoch 5, Batch: 18000| Training Loss: 3.1006701845261784\n",
      "Epoch 5, Batch: 19000| Training Loss: 3.098166624828389\n",
      "Epoch 5, Batch: 20000| Training Loss: 3.0972964865148067\n",
      "Epoch 5, Batch: 21000| Training Loss: 3.095809174202737\n",
      "Epoch 5, Batch: 22000| Training Loss: 3.093537267961285\n",
      "Epoch 5, Batch: 23000| Training Loss: 3.091270495036374\n",
      "Epoch 5, Batch: 24000| Training Loss: 3.0895970076074204\n",
      "Epoch 5, Batch: 25000| Training Loss: 3.087207551774979\n",
      "Epoch 5, Training Loss: 3.085907254332188, Validation Error: 73.4707813907535, Validation Top-3 Accuracy: 46.569610321507554, Training Error: 71.06703045949236\n",
      "Epoch 6, Batch: 1000| Training Loss: 3.0678865529298784\n",
      "Epoch 6, Batch: 2000| Training Loss: 3.065613861501217\n",
      "Epoch 6, Batch: 3000| Training Loss: 3.065330901503563\n",
      "Epoch 6, Batch: 4000| Training Loss: 3.0664185711443426\n",
      "Epoch 6, Batch: 5000| Training Loss: 3.059781267666817\n",
      "Epoch 6, Batch: 6000| Training Loss: 3.0567422559460002\n",
      "Epoch 6, Batch: 7000| Training Loss: 3.0544450075115477\n",
      "Epoch 6, Batch: 8000| Training Loss: 3.051874518662691\n",
      "Epoch 6, Batch: 9000| Training Loss: 3.0502714044517942\n",
      "Epoch 6, Batch: 10000| Training Loss: 3.048397197175026\n",
      "Epoch 6, Batch: 11000| Training Loss: 3.0463356130990116\n",
      "Epoch 6, Batch: 12000| Training Loss: 3.04400585069259\n",
      "Epoch 6, Batch: 13000| Training Loss: 3.042730453913028\n",
      "Epoch 6, Batch: 14000| Training Loss: 3.041364134669304\n",
      "Epoch 6, Batch: 15000| Training Loss: 3.0393445292949677\n",
      "Epoch 6, Batch: 16000| Training Loss: 3.0374716517180205\n",
      "Epoch 6, Batch: 17000| Training Loss: 3.0359388306000654\n",
      "Epoch 6, Batch: 18000| Training Loss: 3.0345618599587016\n",
      "Epoch 6, Batch: 19000| Training Loss: 3.032283503331636\n",
      "Epoch 6, Batch: 20000| Training Loss: 3.0315905069708826\n",
      "Epoch 6, Batch: 21000| Training Loss: 3.0304173103741237\n",
      "Epoch 6, Batch: 22000| Training Loss: 3.028403895486485\n",
      "Epoch 6, Batch: 23000| Training Loss: 3.0263572992967522\n",
      "Epoch 6, Batch: 24000| Training Loss: 3.024980157852173\n",
      "Epoch 6, Batch: 25000| Training Loss: 3.0228709458446503\n",
      "Epoch 6, Training Loss: 3.0217201649400587, Validation Error: 73.00148199808211, Validation Top-3 Accuracy: 47.232150640573025, Training Error: 70.22390388038883\n",
      "Epoch 7, Batch: 1000| Training Loss: 3.0201055139303206\n",
      "Epoch 7, Batch: 2000| Training Loss: 3.017170093357563\n",
      "Epoch 7, Batch: 3000| Training Loss: 3.0174953127702078\n",
      "Epoch 7, Batch: 4000| Training Loss: 3.0186905769705774\n",
      "Epoch 7, Batch: 5000| Training Loss: 3.0124560492038728\n",
      "Epoch 7, Batch: 6000| Training Loss: 3.0095274131298067\n",
      "Epoch 7, Batch: 7000| Training Loss: 3.007142053638186\n",
      "Epoch 7, Batch: 8000| Training Loss: 3.0049535977989437\n",
      "Epoch 7, Batch: 9000| Training Loss: 3.003604592680931\n",
      "Epoch 7, Batch: 10000| Training Loss: 3.0019830515146255\n",
      "Epoch 7, Batch: 11000| Training Loss: 3.0001343515136023\n",
      "Epoch 7, Batch: 12000| Training Loss: 2.9979772396186988\n",
      "Epoch 7, Batch: 13000| Training Loss: 2.996847433869655\n",
      "Epoch 7, Batch: 14000| Training Loss: 2.995643303470952\n",
      "Epoch 7, Batch: 15000| Training Loss: 2.9937524714549384\n",
      "Epoch 7, Batch: 16000| Training Loss: 2.991907784126699\n",
      "Epoch 7, Batch: 17000| Training Loss: 2.9905570759562887\n",
      "Epoch 7, Batch: 18000| Training Loss: 2.989292973856131\n",
      "Epoch 7, Batch: 19000| Training Loss: 2.9873047904403585\n",
      "Epoch 7, Batch: 20000| Training Loss: 2.986717792803049\n",
      "Epoch 7, Batch: 21000| Training Loss: 2.985636543364752\n",
      "Epoch 7, Batch: 22000| Training Loss: 2.98374988898364\n",
      "Epoch 7, Batch: 23000| Training Loss: 2.9818797529158383\n",
      "Epoch 7, Batch: 24000| Training Loss: 2.980530419498682\n",
      "Epoch 7, Batch: 25000| Training Loss: 2.9784792034959793\n",
      "Epoch 7, Training Loss: 2.9774368919852794, Validation Error: 72.72542353180484, Validation Top-3 Accuracy: 47.6709383092367, Training Error: 69.64713727265497\n",
      "Epoch 8, Batch: 1000| Training Loss: 2.985103551506996\n",
      "Epoch 8, Batch: 2000| Training Loss: 2.98260239225626\n",
      "Epoch 8, Batch: 3000| Training Loss: 2.9829917999505997\n",
      "Epoch 8, Batch: 4000| Training Loss: 2.9844755600988866\n",
      "Epoch 8, Batch: 5000| Training Loss: 2.9788212438821793\n",
      "Epoch 8, Batch: 6000| Training Loss: 2.9758359957734744\n",
      "Epoch 8, Batch: 7000| Training Loss: 2.9734476071596148\n",
      "Epoch 8, Batch: 8000| Training Loss: 2.971505431905389\n",
      "Epoch 8, Batch: 9000| Training Loss: 2.970245382944743\n",
      "Epoch 8, Batch: 10000| Training Loss: 2.9688278483510016\n",
      "Epoch 8, Batch: 11000| Training Loss: 2.9670583903030914\n",
      "Epoch 8, Batch: 12000| Training Loss: 2.9651006292104722\n",
      "Epoch 8, Batch: 13000| Training Loss: 2.963996638939931\n",
      "Epoch 8, Batch: 14000| Training Loss: 2.962999806829861\n",
      "Epoch 8, Batch: 15000| Training Loss: 2.961275273609161\n",
      "Epoch 8, Batch: 16000| Training Loss: 2.959512501217425\n",
      "Epoch 8, Batch: 17000| Training Loss: 2.958209812648156\n",
      "Epoch 8, Batch: 18000| Training Loss: 2.9569436461925505\n",
      "Epoch 8, Batch: 19000| Training Loss: 2.955014316207484\n",
      "Epoch 8, Batch: 20000| Training Loss: 2.9544911190867422\n",
      "Epoch 8, Batch: 21000| Training Loss: 2.953485906294414\n",
      "Epoch 8, Batch: 22000| Training Loss: 2.9517084162452005\n",
      "Epoch 8, Batch: 23000| Training Loss: 2.9498474678734072\n",
      "Epoch 8, Batch: 24000| Training Loss: 2.9485820445170003\n",
      "Epoch 8, Batch: 25000| Training Loss: 2.946619019021988\n",
      "Epoch 8, Training Loss: 2.945615731062451, Validation Error: 72.5089355766716, Validation Top-3 Accuracy: 47.69127945938345, Training Error: 69.20300006729352\n",
      "Epoch 9, Batch: 1000| Training Loss: 2.9591165297031403\n",
      "Epoch 9, Batch: 2000| Training Loss: 2.9565978664159775\n",
      "Epoch 9, Batch: 3000| Training Loss: 2.9579790262381236\n",
      "Epoch 9, Batch: 4000| Training Loss: 2.9592695457339286\n",
      "Epoch 9, Batch: 5000| Training Loss: 2.9535207255125044\n",
      "Epoch 9, Batch: 6000| Training Loss: 2.9507935391465825\n",
      "Epoch 9, Batch: 7000| Training Loss: 2.9486976746320726\n",
      "Epoch 9, Batch: 8000| Training Loss: 2.9469273146241903\n",
      "Epoch 9, Batch: 9000| Training Loss: 2.9459409682618247\n",
      "Epoch 9, Batch: 10000| Training Loss: 2.9443347998857496\n",
      "Epoch 9, Batch: 11000| Training Loss: 2.9425533046505667\n",
      "Epoch 9, Batch: 12000| Training Loss: 2.9405323606431484\n",
      "Epoch 9, Batch: 13000| Training Loss: 2.9396069645789953\n",
      "Epoch 9, Batch: 14000| Training Loss: 2.9387191323637962\n",
      "Epoch 9, Batch: 15000| Training Loss: 2.937059528652827\n",
      "Epoch 9, Batch: 16000| Training Loss: 2.9353375568389892\n",
      "Epoch 9, Batch: 17000| Training Loss: 2.9341450306387507\n",
      "Epoch 9, Batch: 18000| Training Loss: 2.9329819265007973\n",
      "Epoch 9, Batch: 19000| Training Loss: 2.9311470370355406\n",
      "Epoch 9, Batch: 20000| Training Loss: 2.9306889545619486\n",
      "Epoch 9, Batch: 21000| Training Loss: 2.9297191989478613\n",
      "Epoch 9, Batch: 22000| Training Loss: 2.9280191683389925\n",
      "Epoch 9, Batch: 23000| Training Loss: 2.9262417292439418\n",
      "Epoch 9, Batch: 24000| Training Loss: 2.925040630112092\n",
      "Epoch 9, Batch: 25000| Training Loss: 2.923190500454903\n",
      "Epoch 9, Training Loss: 2.9221982374027746, Validation Error: 72.37381222212537, Validation Top-3 Accuracy: 47.923749746774845, Training Error: 68.90715330074696\n",
      "Epoch 10, Batch: 1000| Training Loss: 2.942651938080788\n",
      "Epoch 10, Batch: 2000| Training Loss: 2.9388877754807474\n",
      "Epoch 10, Batch: 3000| Training Loss: 2.9402778798739115\n",
      "Epoch 10, Batch: 4000| Training Loss: 2.941494235098362\n",
      "Epoch 10, Batch: 5000| Training Loss: 2.9359844781398774\n",
      "Epoch 10, Batch: 6000| Training Loss: 2.9336939415534338\n",
      "Epoch 10, Batch: 7000| Training Loss: 2.931403711421149\n",
      "Epoch 10, Batch: 8000| Training Loss: 2.9296506073325874\n",
      "Epoch 10, Batch: 9000| Training Loss: 2.92880011733373\n",
      "Epoch 10, Batch: 10000| Training Loss: 2.927413677430153\n",
      "Epoch 10, Batch: 11000| Training Loss: 2.925694975571199\n",
      "Epoch 10, Batch: 12000| Training Loss: 2.9236992925703524\n",
      "Epoch 10, Batch: 13000| Training Loss: 2.922867253386057\n",
      "Epoch 10, Batch: 14000| Training Loss: 2.9219514468227112\n",
      "Epoch 10, Batch: 15000| Training Loss: 2.920351440747579\n",
      "Epoch 10, Batch: 16000| Training Loss: 2.918701268956065\n",
      "Epoch 10, Batch: 17000| Training Loss: 2.917517299567952\n",
      "Epoch 10, Batch: 18000| Training Loss: 2.916393088698387\n",
      "Epoch 10, Batch: 19000| Training Loss: 2.914512223877405\n",
      "Epoch 10, Batch: 20000| Training Loss: 2.914086051827669\n",
      "Epoch 10, Batch: 21000| Training Loss: 2.9131151449737094\n",
      "Epoch 10, Batch: 22000| Training Loss: 2.911441717326641\n",
      "Epoch 10, Batch: 23000| Training Loss: 2.909687483549118\n",
      "Epoch 10, Batch: 24000| Training Loss: 2.9084795691172283\n",
      "Epoch 10, Batch: 25000| Training Loss: 2.9066401192712785\n",
      "Epoch 10, Training Loss: 2.905672571250429, Validation Error: 72.27791822857641, Validation Top-3 Accuracy: 48.05015546554392, Training Error: 68.66850602276968\n",
      "Epoch 11, Batch: 1000| Training Loss: 2.9272338223457335\n",
      "Epoch 11, Batch: 2000| Training Loss: 2.9249668496251107\n",
      "Epoch 11, Batch: 3000| Training Loss: 2.926251265605291\n",
      "Epoch 11, Batch: 4000| Training Loss: 2.927783680379391\n",
      "Epoch 11, Batch: 5000| Training Loss: 2.9225695748805998\n",
      "Epoch 11, Batch: 6000| Training Loss: 2.9202721897761026\n",
      "Epoch 11, Batch: 7000| Training Loss: 2.918092726945877\n",
      "Epoch 11, Batch: 8000| Training Loss: 2.916610171958804\n",
      "Epoch 11, Batch: 9000| Training Loss: 2.9158207762108908\n",
      "Epoch 11, Batch: 10000| Training Loss: 2.9144918267607687\n",
      "Epoch 11, Batch: 11000| Training Loss: 2.9128593230139126\n",
      "Epoch 11, Batch: 12000| Training Loss: 2.911004111687342\n",
      "Epoch 11, Batch: 13000| Training Loss: 2.9100704071613457\n",
      "Epoch 11, Batch: 14000| Training Loss: 2.9092596410768374\n",
      "Epoch 11, Batch: 15000| Training Loss: 2.9076943257252377\n",
      "Epoch 11, Batch: 16000| Training Loss: 2.9060568145886063\n",
      "Epoch 11, Batch: 17000| Training Loss: 2.904954256709884\n",
      "Epoch 11, Batch: 18000| Training Loss: 2.903845960398515\n",
      "Epoch 11, Batch: 19000| Training Loss: 2.902109869411117\n",
      "Epoch 11, Batch: 20000| Training Loss: 2.901758336442709\n",
      "Epoch 11, Batch: 21000| Training Loss: 2.900886386825925\n",
      "Epoch 11, Batch: 22000| Training Loss: 2.899218969150023\n",
      "Epoch 11, Batch: 23000| Training Loss: 2.897462416845819\n",
      "Epoch 11, Batch: 24000| Training Loss: 2.8962819805194933\n",
      "Epoch 11, Batch: 25000| Training Loss: 2.8944008681297304\n",
      "Epoch 11, Training Loss: 2.893461563883835, Validation Error: 72.26920059279924, Validation Top-3 Accuracy: 48.324760992525, Training Error: 68.48773116852132\n",
      "Epoch 12, Batch: 1000| Training Loss: 2.9175214400291445\n",
      "Epoch 12, Batch: 2000| Training Loss: 2.915702455699444\n",
      "Epoch 12, Batch: 3000| Training Loss: 2.9174674254258472\n",
      "Epoch 12, Batch: 4000| Training Loss: 2.9191014917492866\n",
      "Epoch 12, Batch: 5000| Training Loss: 2.9140793596982957\n",
      "Epoch 12, Batch: 6000| Training Loss: 2.9115413689812026\n",
      "Epoch 12, Batch: 7000| Training Loss: 2.909352442758424\n",
      "Epoch 12, Batch: 8000| Training Loss: 2.907803991943598\n",
      "Epoch 12, Batch: 9000| Training Loss: 2.9071117030249702\n",
      "Epoch 12, Batch: 10000| Training Loss: 2.9057951046824457\n",
      "Epoch 12, Batch: 11000| Training Loss: 2.9041041990735312\n",
      "Epoch 12, Batch: 12000| Training Loss: 2.9022029966513316\n",
      "Epoch 12, Batch: 13000| Training Loss: 2.9011787192546405\n",
      "Epoch 12, Batch: 14000| Training Loss: 2.9003167059847286\n",
      "Epoch 12, Batch: 15000| Training Loss: 2.8988964504559833\n",
      "Epoch 12, Batch: 16000| Training Loss: 2.8972342068850994\n",
      "Epoch 12, Batch: 17000| Training Loss: 2.896018249778187\n",
      "Epoch 12, Batch: 18000| Training Loss: 2.89484561182393\n",
      "Epoch 12, Batch: 19000| Training Loss: 2.89304489679713\n",
      "Epoch 12, Batch: 20000| Training Loss: 2.8927132951796053\n",
      "Epoch 12, Batch: 21000| Training Loss: 2.891872513708614\n",
      "Epoch 12, Batch: 22000| Training Loss: 2.890222723857923\n",
      "Epoch 12, Batch: 23000| Training Loss: 2.888470413612283\n",
      "Epoch 12, Batch: 24000| Training Loss: 2.88723755514125\n",
      "Epoch 12, Batch: 25000| Training Loss: 2.885444519753456\n",
      "Epoch 12, Training Loss: 2.8845409466986762, Validation Error: 72.21253596024758, Validation Top-3 Accuracy: 48.37416092859567, Training Error: 68.37314866361194\n",
      "Epoch 13, Batch: 1000| Training Loss: 2.912703545808792\n",
      "Epoch 13, Batch: 2000| Training Loss: 2.909955084979534\n",
      "Epoch 13, Batch: 3000| Training Loss: 2.911555638829867\n",
      "Epoch 13, Batch: 4000| Training Loss: 2.913081691920757\n",
      "Epoch 13, Batch: 5000| Training Loss: 2.9080309290647506\n",
      "Epoch 13, Batch: 6000| Training Loss: 2.9057465114196144\n",
      "Epoch 13, Batch: 7000| Training Loss: 2.9037076154436385\n",
      "Epoch 13, Batch: 8000| Training Loss: 2.902227616533637\n",
      "Epoch 13, Batch: 9000| Training Loss: 2.9014974092377557\n",
      "Epoch 13, Batch: 10000| Training Loss: 2.9001386999726297\n",
      "Epoch 13, Batch: 11000| Training Loss: 2.8984868774955923\n",
      "Epoch 13, Batch: 12000| Training Loss: 2.896740030914545\n",
      "Epoch 13, Batch: 13000| Training Loss: 2.8958291242947944\n",
      "Epoch 13, Batch: 14000| Training Loss: 2.8951704015476363\n",
      "Epoch 13, Batch: 15000| Training Loss: 2.8937872382799785\n",
      "Epoch 13, Batch: 16000| Training Loss: 2.892190606854856\n",
      "Epoch 13, Batch: 17000| Training Loss: 2.8910519582874636\n",
      "Epoch 13, Batch: 18000| Training Loss: 2.889973071263896\n",
      "Epoch 13, Batch: 19000| Training Loss: 2.8882898202444376\n",
      "Epoch 13, Batch: 20000| Training Loss: 2.887992945200205\n",
      "Epoch 13, Batch: 21000| Training Loss: 2.887200176000595\n",
      "Epoch 13, Batch: 22000| Training Loss: 2.885629110878164\n",
      "Epoch 13, Batch: 23000| Training Loss: 2.8839206466052842\n",
      "Epoch 13, Batch: 24000| Training Loss: 2.8827505879650515\n",
      "Epoch 13, Batch: 25000| Training Loss: 2.8810259076309204\n",
      "Epoch 13, Training Loss: 2.8801704435786855, Validation Error: 72.17621247784268, Validation Top-3 Accuracy: 48.44390201481309, Training Error: 68.3078127772034\n",
      "Epoch 14, Batch: 1000| Training Loss: 2.9070361644029616\n",
      "Epoch 14, Batch: 2000| Training Loss: 2.9045500036478042\n",
      "Epoch 14, Batch: 3000| Training Loss: 2.9061776479482653\n",
      "Epoch 14, Batch: 4000| Training Loss: 2.907652519762516\n",
      "Epoch 14, Batch: 5000| Training Loss: 2.9027121810436247\n",
      "Epoch 14, Batch: 6000| Training Loss: 2.9004349587162337\n",
      "Epoch 14, Batch: 7000| Training Loss: 2.898411779386657\n",
      "Epoch 14, Batch: 8000| Training Loss: 2.897178705081344\n",
      "Epoch 14, Batch: 9000| Training Loss: 2.8964210275411606\n",
      "Epoch 14, Batch: 10000| Training Loss: 2.895313549041748\n",
      "Epoch 14, Batch: 11000| Training Loss: 2.8937483720129187\n",
      "Epoch 14, Batch: 12000| Training Loss: 2.891885655124982\n",
      "Epoch 14, Batch: 13000| Training Loss: 2.8911438416517696\n",
      "Epoch 14, Batch: 14000| Training Loss: 2.8904253344791275\n",
      "Epoch 14, Batch: 15000| Training Loss: 2.8889762285629907\n",
      "Epoch 14, Batch: 16000| Training Loss: 2.887486680150032\n",
      "Epoch 14, Batch: 17000| Training Loss: 2.8864215368944057\n",
      "Epoch 14, Batch: 18000| Training Loss: 2.885308835380607\n",
      "Epoch 14, Batch: 19000| Training Loss: 2.8836468281808654\n",
      "Epoch 14, Batch: 20000| Training Loss: 2.883354423236847\n",
      "Epoch 14, Batch: 21000| Training Loss: 2.8826268673226947\n",
      "Epoch 14, Batch: 22000| Training Loss: 2.881123961865902\n",
      "Epoch 14, Batch: 23000| Training Loss: 2.8795367946106456\n",
      "Epoch 14, Batch: 24000| Training Loss: 2.8784785576264063\n",
      "Epoch 14, Batch: 25000| Training Loss: 2.876788261680603\n",
      "Epoch 14, Training Loss: 2.875925107636436, Validation Error: 72.16313602417691, Validation Top-3 Accuracy: 48.426466743258736, Training Error: 68.23611459474009\n",
      "Epoch 15, Batch: 1000| Training Loss: 2.902768669247627\n",
      "Epoch 15, Batch: 2000| Training Loss: 2.900293594837189\n",
      "Epoch 15, Batch: 3000| Training Loss: 2.9017615470488867\n",
      "Epoch 15, Batch: 4000| Training Loss: 2.9035180234909057\n",
      "Epoch 15, Batch: 5000| Training Loss: 2.8984339817762375\n",
      "Epoch 15, Batch: 6000| Training Loss: 2.8959592772722242\n",
      "Epoch 15, Batch: 7000| Training Loss: 2.8938503274236407\n",
      "Epoch 15, Batch: 8000| Training Loss: 2.8925002371668818\n",
      "Epoch 15, Batch: 9000| Training Loss: 2.89177711851067\n",
      "Epoch 15, Batch: 10000| Training Loss: 2.8905411552786826\n",
      "Epoch 15, Batch: 11000| Training Loss: 2.888896894617514\n",
      "Epoch 15, Batch: 12000| Training Loss: 2.8871798782845337\n",
      "Epoch 15, Batch: 13000| Training Loss: 2.886488559456972\n",
      "Epoch 15, Batch: 14000| Training Loss: 2.885865281198706\n",
      "Epoch 15, Batch: 15000| Training Loss: 2.884509125614166\n",
      "Epoch 15, Batch: 16000| Training Loss: 2.8830463977232577\n",
      "Epoch 15, Batch: 17000| Training Loss: 2.8820447498419703\n",
      "Epoch 15, Batch: 18000| Training Loss: 2.8809907164904804\n",
      "Epoch 15, Batch: 19000| Training Loss: 2.879369472208776\n",
      "Epoch 15, Batch: 20000| Training Loss: 2.8791235761106013\n",
      "Epoch 15, Batch: 21000| Training Loss: 2.878373941557748\n",
      "Epoch 15, Batch: 22000| Training Loss: 2.8768955464363097\n",
      "Epoch 15, Batch: 23000| Training Loss: 2.8752972190069115\n",
      "Epoch 15, Batch: 24000| Training Loss: 2.8743170627256234\n",
      "Epoch 15, Batch: 25000| Training Loss: 2.8726384447431563\n",
      "Epoch 15, Training Loss: 2.8717943185432975, Validation Error: 72.09194199866329, Validation Top-3 Accuracy: 48.47005492214462, Training Error: 68.17389868043533\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 37.41142857142857%\n"
     ]
    }
   ],
   "source": [
    "def is_legal_move(chess_board, move_san):\n",
    "    try:\n",
    "        chess_move = chess_board.parse_san(move_san)\n",
    "        return chess_move in chess_board.legal_moves\n",
    "    except ValueError:\n",
    "        # This handles cases where the SAN move cannot be parsed or is not legal\n",
    "        return False\n",
    "\n",
    "def load_board_state_from_san(moves):\n",
    "    board = chess.Board()\n",
    "    for index in moves:\n",
    "        try:\n",
    "            if index == 0:\n",
    "                return board\n",
    "            else:\n",
    "                move_san = vocab.get_move(index.item())\n",
    "                move = board.parse_san(move_san)\n",
    "                board.push(move)\n",
    "        except ValueError:\n",
    "            # Handle invalid moves, e.g., break the loop or log an error\n",
    "            break\n",
    "    return board\n",
    "\n",
    "val_size = int(total_size * 0.04)\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "if val_loader is not None:\n",
    "    with torch.no_grad():\n",
    "        for boards, sequences, lengths, labels in val_loader:\n",
    "            boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            outputs = model(boards, sequences, lengths)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            minus = 0\n",
    "            for idx, (sequence, label) in enumerate(zip(sequences, labels)):\n",
    "                # This tells us we're looking at games that include the opening but has developed more than the first 4 half-moves\n",
    "                if sequence[-1].item() == 0 and sequence[2].item() != 0 and sequence[3].item() != 0 and sequence[4].item() != 0:\n",
    "                    output = probabilities[idx]\n",
    "                    sorted_probs, sorted_indices = torch.sort(output, descending=True)\n",
    "                    predicted_move = sorted_indices[0]\n",
    "                    # print(predicted_move)\n",
    "                    chess_board = load_board_state_from_san(sequence)\n",
    "                    for move_idx in sorted_indices:\n",
    "                        move = vocab.get_move(move_idx.item()) # Convert index to move (e.g., 'e2e4')\n",
    "                        if is_legal_move(chess_board, move):\n",
    "                            # print(\"we found one\")\n",
    "                            predicted_move = vocab.get_id(move)\n",
    "                            break\n",
    "                    \n",
    "                    # Check if predicted move is correct\n",
    "                    correct_move = label.item() # Convert label to move\n",
    "                    # print(correct_move)\n",
    "                    if predicted_move == correct_move:\n",
    "                        val_correct += 1\n",
    "                else:\n",
    "                    minus += 1\n",
    "            val_total += (labels.size(0) - minus)\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multimodalmodel-2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 (focal loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634630\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131882\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # alpha can be set to a constant, or it can be a tensor of shape (num_classes,)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 100\n",
    "d_embed = 48\n",
    "NUM_EPOCHS = 15\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalTwo(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(gamma=2, alpha=1, reduction='mean')\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 5.216632953643799\n",
      "Epoch 1, Batch: 2000| Training Loss: 4.9681913522481915\n",
      "Epoch 1, Batch: 3000| Training Loss: 4.809984962622325\n",
      "Epoch 1, Batch: 4000| Training Loss: 4.693838460147381\n",
      "Epoch 1, Batch: 5000| Training Loss: 4.583828910255432\n",
      "Epoch 1, Batch: 6000| Training Loss: 4.49359255596002\n",
      "Epoch 1, Batch: 7000| Training Loss: 4.419784398453576\n",
      "Epoch 1, Batch: 8000| Training Loss: 4.352040460258722\n",
      "Epoch 1, Batch: 9000| Training Loss: 4.294694580793381\n",
      "Epoch 1, Batch: 10000| Training Loss: 4.244223247122765\n",
      "Epoch 1, Batch: 11000| Training Loss: 4.197138833761215\n",
      "Epoch 1, Batch: 12000| Training Loss: 4.153256935497125\n",
      "Epoch 1, Batch: 13000| Training Loss: 4.113717842560548\n",
      "Epoch 1, Batch: 14000| Training Loss: 4.077679372889655\n",
      "Epoch 1, Batch: 15000| Training Loss: 4.044189786974589\n",
      "Epoch 1, Batch: 16000| Training Loss: 4.012455889597535\n",
      "Epoch 1, Batch: 17000| Training Loss: 3.9834087364673616\n",
      "Epoch 1, Batch: 18000| Training Loss: 3.956665287401941\n",
      "Epoch 1, Batch: 19000| Training Loss: 3.931393717213681\n",
      "Epoch 1, Batch: 20000| Training Loss: 3.9084694637537\n",
      "Epoch 1, Batch: 21000| Training Loss: 3.8859627670674097\n",
      "Epoch 1, Batch: 22000| Training Loss: 3.8635805377851833\n",
      "Epoch 1, Batch: 23000| Training Loss: 3.8423105001967888\n",
      "Epoch 1, Batch: 24000| Training Loss: 3.822287365158399\n",
      "Epoch 1, Batch: 25000| Training Loss: 3.8030709920835495\n",
      "Epoch 1, Training Loss: 3.7927171604092496, Validation Error: 77.91968151570627, Validation Top-3 Accuracy: 40.393165373723896, Training Error: 81.12129350372868\n",
      "Epoch 2, Batch: 1000| Training Loss: 3.2819662536382674\n",
      "Epoch 2, Batch: 2000| Training Loss: 3.275852493822575\n",
      "Epoch 2, Batch: 3000| Training Loss: 3.269808694163958\n",
      "Epoch 2, Batch: 4000| Training Loss: 3.2638935178220274\n",
      "Epoch 2, Batch: 5000| Training Loss: 3.249166759586334\n",
      "Epoch 2, Batch: 6000| Training Loss: 3.240282856941223\n",
      "Epoch 2, Batch: 7000| Training Loss: 3.233263347574643\n",
      "Epoch 2, Batch: 8000| Training Loss: 3.2249943661540748\n",
      "Epoch 2, Batch: 9000| Training Loss: 3.2195510797897975\n",
      "Epoch 2, Batch: 10000| Training Loss: 3.2134720901846885\n",
      "Epoch 2, Batch: 11000| Training Loss: 3.207195402741432\n",
      "Epoch 2, Batch: 12000| Training Loss: 3.2001376625299454\n",
      "Epoch 2, Batch: 13000| Training Loss: 3.194754500535818\n",
      "Epoch 2, Batch: 14000| Training Loss: 3.1895418390376227\n",
      "Epoch 2, Batch: 15000| Training Loss: 3.1838418961524964\n",
      "Epoch 2, Batch: 16000| Training Loss: 3.177613760218024\n",
      "Epoch 2, Batch: 17000| Training Loss: 3.1721128615772023\n",
      "Epoch 2, Batch: 18000| Training Loss: 3.1673498900665176\n",
      "Epoch 2, Batch: 19000| Training Loss: 3.1619891420351833\n",
      "Epoch 2, Batch: 20000| Training Loss: 3.1582417416751385\n",
      "Epoch 2, Batch: 21000| Training Loss: 3.153596747018042\n",
      "Epoch 2, Batch: 22000| Training Loss: 3.1481343298879536\n",
      "Epoch 2, Batch: 23000| Training Loss: 3.1426628705418627\n",
      "Epoch 2, Batch: 24000| Training Loss: 3.138012427295248\n",
      "Epoch 2, Batch: 25000| Training Loss: 3.1328575599241257\n",
      "Epoch 2, Training Loss: 3.1301114987528003, Validation Error: 75.44096707639555, Validation Top-3 Accuracy: 44.029872431929796, Training Error: 75.23476260682845\n",
      "Epoch 3, Batch: 1000| Training Loss: 3.000643640637398\n",
      "Epoch 3, Batch: 2000| Training Loss: 2.9942184072732925\n",
      "Epoch 3, Batch: 3000| Training Loss: 2.990476909081141\n",
      "Epoch 3, Batch: 4000| Training Loss: 2.98824698343873\n",
      "Epoch 3, Batch: 5000| Training Loss: 2.977424277639389\n",
      "Epoch 3, Batch: 6000| Training Loss: 2.9723902878959976\n",
      "Epoch 3, Batch: 7000| Training Loss: 2.968596156716347\n",
      "Epoch 3, Batch: 8000| Training Loss: 2.963587023794651\n",
      "Epoch 3, Batch: 9000| Training Loss: 2.9616011015706594\n",
      "Epoch 3, Batch: 10000| Training Loss: 2.9581176186800002\n",
      "Epoch 3, Batch: 11000| Training Loss: 2.954791423212398\n",
      "Epoch 3, Batch: 12000| Training Loss: 2.950438773840666\n",
      "Epoch 3, Batch: 13000| Training Loss: 2.9477586478636817\n",
      "Epoch 3, Batch: 14000| Training Loss: 2.9452212199143\n",
      "Epoch 3, Batch: 15000| Training Loss: 2.942076596911748\n",
      "Epoch 3, Batch: 16000| Training Loss: 2.9382585611194374\n",
      "Epoch 3, Batch: 17000| Training Loss: 2.935024822340292\n",
      "Epoch 3, Batch: 18000| Training Loss: 2.9325994193024107\n",
      "Epoch 3, Batch: 19000| Training Loss: 2.9291543554883255\n",
      "Epoch 3, Batch: 20000| Training Loss: 2.9274938245236872\n",
      "Epoch 3, Batch: 21000| Training Loss: 2.924983433411235\n",
      "Epoch 3, Batch: 22000| Training Loss: 2.9215648407177492\n",
      "Epoch 3, Batch: 23000| Training Loss: 2.918184255288995\n",
      "Epoch 3, Batch: 24000| Training Loss: 2.915475325147311\n",
      "Epoch 3, Batch: 25000| Training Loss: 2.912181992902756\n",
      "Epoch 3, Training Loss: 2.9106184281120004, Validation Error: 74.33673321128643, Validation Top-3 Accuracy: 45.90125824646973, Training Error: 72.88829888109235\n",
      "Epoch 4, Batch: 1000| Training Loss: 2.8472345204353333\n",
      "Epoch 4, Batch: 2000| Training Loss: 2.841963389873505\n",
      "Epoch 4, Batch: 3000| Training Loss: 2.840683022260666\n",
      "Epoch 4, Batch: 4000| Training Loss: 2.840290137499571\n",
      "Epoch 4, Batch: 5000| Training Loss: 2.8310180349826815\n",
      "Epoch 4, Batch: 6000| Training Loss: 2.826858473777771\n",
      "Epoch 4, Batch: 7000| Training Loss: 2.823676180209432\n",
      "Epoch 4, Batch: 8000| Training Loss: 2.8195690371245146\n",
      "Epoch 4, Batch: 9000| Training Loss: 2.818657794793447\n",
      "Epoch 4, Batch: 10000| Training Loss: 2.8161660451292994\n",
      "Epoch 4, Batch: 11000| Training Loss: 2.813642211480574\n",
      "Epoch 4, Batch: 12000| Training Loss: 2.810279815226793\n",
      "Epoch 4, Batch: 13000| Training Loss: 2.8083488430243273\n",
      "Epoch 4, Batch: 14000| Training Loss: 2.8067419288243567\n",
      "Epoch 4, Batch: 15000| Training Loss: 2.8043741691350936\n",
      "Epoch 4, Batch: 16000| Training Loss: 2.801354578591883\n",
      "Epoch 4, Batch: 17000| Training Loss: 2.798886652287315\n",
      "Epoch 4, Batch: 18000| Training Loss: 2.7970022896462017\n",
      "Epoch 4, Batch: 19000| Training Loss: 2.7941356245906728\n",
      "Epoch 4, Batch: 20000| Training Loss: 2.7931177794218063\n",
      "Epoch 4, Batch: 21000| Training Loss: 2.7914336742162704\n",
      "Epoch 4, Batch: 22000| Training Loss: 2.788882786349817\n",
      "Epoch 4, Batch: 23000| Training Loss: 2.7861152065111243\n",
      "Epoch 4, Batch: 24000| Training Loss: 2.7840932846864064\n",
      "Epoch 4, Batch: 25000| Training Loss: 2.78158749809742\n",
      "Epoch 4, Training Loss: 2.7803334090976835, Validation Error: 73.52308720541656, Validation Top-3 Accuracy: 46.97352744688932, Training Error: 71.3624489945737\n",
      "Epoch 5, Batch: 1000| Training Loss: 2.7503232909440993\n",
      "Epoch 5, Batch: 2000| Training Loss: 2.7431851140260695\n",
      "Epoch 5, Batch: 3000| Training Loss: 2.742925958673159\n",
      "Epoch 5, Batch: 4000| Training Loss: 2.7433976736366747\n",
      "Epoch 5, Batch: 5000| Training Loss: 2.735429484319687\n",
      "Epoch 5, Batch: 6000| Training Loss: 2.7319870011011758\n",
      "Epoch 5, Batch: 7000| Training Loss: 2.729166055253574\n",
      "Epoch 5, Batch: 8000| Training Loss: 2.7258504909723995\n",
      "Epoch 5, Batch: 9000| Training Loss: 2.7255642856491935\n",
      "Epoch 5, Batch: 10000| Training Loss: 2.72330943107605\n",
      "Epoch 5, Batch: 11000| Training Loss: 2.7212300214659084\n",
      "Epoch 5, Batch: 12000| Training Loss: 2.718216303706169\n",
      "Epoch 5, Batch: 13000| Training Loss: 2.7166780581841103\n",
      "Epoch 5, Batch: 14000| Training Loss: 2.7155301693422453\n",
      "Epoch 5, Batch: 15000| Training Loss: 2.7136465473413467\n",
      "Epoch 5, Batch: 16000| Training Loss: 2.710937145344913\n",
      "Epoch 5, Batch: 17000| Training Loss: 2.708697263801799\n",
      "Epoch 5, Batch: 18000| Training Loss: 2.707256145026949\n",
      "Epoch 5, Batch: 19000| Training Loss: 2.7046667062546077\n",
      "Epoch 5, Batch: 20000| Training Loss: 2.7040714063823224\n",
      "Epoch 5, Batch: 21000| Training Loss: 2.7027382278896512\n",
      "Epoch 5, Batch: 22000| Training Loss: 2.7006037548292765\n",
      "Epoch 5, Batch: 23000| Training Loss: 2.6983664329414783\n",
      "Epoch 5, Batch: 24000| Training Loss: 2.6966489586333435\n",
      "Epoch 5, Batch: 25000| Training Loss: 2.694535411877632\n",
      "Epoch 5, Training Loss: 2.693509607515379, Validation Error: 72.97678203004679, Validation Top-3 Accuracy: 47.714526486736965, Training Error: 70.37861779118211\n",
      "Epoch 6, Batch: 1000| Training Loss: 2.685300169467926\n",
      "Epoch 6, Batch: 2000| Training Loss: 2.677582105219364\n",
      "Epoch 6, Batch: 3000| Training Loss: 2.6778926556507745\n",
      "Epoch 6, Batch: 4000| Training Loss: 2.6790279506742953\n",
      "Epoch 6, Batch: 5000| Training Loss: 2.6716868950128556\n",
      "Epoch 6, Batch: 6000| Training Loss: 2.6688768330613772\n",
      "Epoch 6, Batch: 7000| Training Loss: 2.666120795420238\n",
      "Epoch 6, Batch: 8000| Training Loss: 2.663067936077714\n",
      "Epoch 6, Batch: 9000| Training Loss: 2.66277545773983\n",
      "Epoch 6, Batch: 10000| Training Loss: 2.6609107632756235\n",
      "Epoch 6, Batch: 11000| Training Loss: 2.6589836594191465\n",
      "Epoch 6, Batch: 12000| Training Loss: 2.6563256346186\n",
      "Epoch 6, Batch: 13000| Training Loss: 2.655052075817035\n",
      "Epoch 6, Batch: 14000| Training Loss: 2.654100790696485\n",
      "Epoch 6, Batch: 15000| Training Loss: 2.6523516079505285\n",
      "Epoch 6, Batch: 16000| Training Loss: 2.649903248369694\n",
      "Epoch 6, Batch: 17000| Training Loss: 2.6478227971161115\n",
      "Epoch 6, Batch: 18000| Training Loss: 2.6466453631652724\n",
      "Epoch 6, Batch: 19000| Training Loss: 2.644197550434815\n",
      "Epoch 6, Batch: 20000| Training Loss: 2.6437614857912064\n",
      "Epoch 6, Batch: 21000| Training Loss: 2.6426062157154084\n",
      "Epoch 6, Batch: 22000| Training Loss: 2.640676648286256\n",
      "Epoch 6, Batch: 23000| Training Loss: 2.638580253445584\n",
      "Epoch 6, Batch: 24000| Training Loss: 2.637031800394257\n",
      "Epoch 6, Batch: 25000| Training Loss: 2.6350659330511093\n",
      "Epoch 6, Training Loss: 2.634147851060714, Validation Error: 72.64696480981024, Validation Top-3 Accuracy: 48.118443611079506, Training Error: 69.65557955011226\n",
      "Epoch 7, Batch: 1000| Training Loss: 2.639013529062271\n",
      "Epoch 7, Batch: 2000| Training Loss: 2.632335887491703\n",
      "Epoch 7, Batch: 3000| Training Loss: 2.6334518223603567\n",
      "Epoch 7, Batch: 4000| Training Loss: 2.6345638402104377\n",
      "Epoch 7, Batch: 5000| Training Loss: 2.6274857970952987\n",
      "Epoch 7, Batch: 6000| Training Loss: 2.624602499385675\n",
      "Epoch 7, Batch: 7000| Training Loss: 2.621914629970278\n",
      "Epoch 7, Batch: 8000| Training Loss: 2.6191388222426175\n",
      "Epoch 7, Batch: 9000| Training Loss: 2.618940722372797\n",
      "Epoch 7, Batch: 10000| Training Loss: 2.616948248231411\n",
      "Epoch 7, Batch: 11000| Training Loss: 2.6151848739277233\n",
      "Epoch 7, Batch: 12000| Training Loss: 2.6126033655603726\n",
      "Epoch 7, Batch: 13000| Training Loss: 2.6115063859316017\n",
      "Epoch 7, Batch: 14000| Training Loss: 2.6106998269472803\n",
      "Epoch 7, Batch: 15000| Training Loss: 2.6090445515473686\n",
      "Epoch 7, Batch: 16000| Training Loss: 2.6066624905765057\n",
      "Epoch 7, Batch: 17000| Training Loss: 2.604841964819852\n",
      "Epoch 7, Batch: 18000| Training Loss: 2.603607551323043\n",
      "Epoch 7, Batch: 19000| Training Loss: 2.601259569419058\n",
      "Epoch 7, Batch: 20000| Training Loss: 2.6008927312552927\n",
      "Epoch 7, Batch: 21000| Training Loss: 2.5998962810096287\n",
      "Epoch 7, Batch: 22000| Training Loss: 2.598103395012292\n",
      "Epoch 7, Batch: 23000| Training Loss: 2.5960418548376665\n",
      "Epoch 7, Batch: 24000| Training Loss: 2.5946545254141093\n",
      "Epoch 7, Batch: 25000| Training Loss: 2.59269545627594\n",
      "Epoch 7, Training Loss: 2.591847537001842, Validation Error: 72.3578298898672, Validation Top-3 Accuracy: 48.34655508179474, Training Error: 69.11967845934554\n",
      "Epoch 8, Batch: 1000| Training Loss: 2.6064755642414092\n",
      "Epoch 8, Batch: 2000| Training Loss: 2.599916649401188\n",
      "Epoch 8, Batch: 3000| Training Loss: 2.6016660457452137\n",
      "Epoch 8, Batch: 4000| Training Loss: 2.602395138710737\n",
      "Epoch 8, Batch: 5000| Training Loss: 2.5955066236257553\n",
      "Epoch 8, Batch: 6000| Training Loss: 2.592838813463847\n",
      "Epoch 8, Batch: 7000| Training Loss: 2.5901660614865167\n",
      "Epoch 8, Batch: 8000| Training Loss: 2.58733392752707\n",
      "Epoch 8, Batch: 9000| Training Loss: 2.587564178850916\n",
      "Epoch 8, Batch: 10000| Training Loss: 2.5856376228809355\n",
      "Epoch 8, Batch: 11000| Training Loss: 2.5840525516379964\n",
      "Epoch 8, Batch: 12000| Training Loss: 2.5814534273942313\n",
      "Epoch 8, Batch: 13000| Training Loss: 2.5803328416164106\n",
      "Epoch 8, Batch: 14000| Training Loss: 2.5795928243398665\n",
      "Epoch 8, Batch: 15000| Training Loss: 2.577925791533788\n",
      "Epoch 8, Batch: 16000| Training Loss: 2.575623089849949\n",
      "Epoch 8, Batch: 17000| Training Loss: 2.573853932983735\n",
      "Epoch 8, Batch: 18000| Training Loss: 2.572701970981227\n",
      "Epoch 8, Batch: 19000| Training Loss: 2.5703974293156673\n",
      "Epoch 8, Batch: 20000| Training Loss: 2.5700979127943517\n",
      "Epoch 8, Batch: 21000| Training Loss: 2.5692663190251306\n",
      "Epoch 8, Batch: 22000| Training Loss: 2.5675760295174337\n",
      "Epoch 8, Batch: 23000| Training Loss: 2.565718868058661\n",
      "Epoch 8, Batch: 24000| Training Loss: 2.5643945365498464\n",
      "Epoch 8, Batch: 25000| Training Loss: 2.5625151151800156\n",
      "Epoch 8, Training Loss: 2.561702743463178, Validation Error: 72.16023014558452, Validation Top-3 Accuracy: 48.64150175771014, Training Error: 68.7438135847256\n",
      "Epoch 9, Batch: 1000| Training Loss: 2.5845847001075746\n",
      "Epoch 9, Batch: 2000| Training Loss: 2.577542078256607\n",
      "Epoch 9, Batch: 3000| Training Loss: 2.5787126348813376\n",
      "Epoch 9, Batch: 4000| Training Loss: 2.579888223975897\n",
      "Epoch 9, Batch: 5000| Training Loss: 2.573327467107773\n",
      "Epoch 9, Batch: 6000| Training Loss: 2.570638845205307\n",
      "Epoch 9, Batch: 7000| Training Loss: 2.5675133563791004\n",
      "Epoch 9, Batch: 8000| Training Loss: 2.5651421091258526\n",
      "Epoch 9, Batch: 9000| Training Loss: 2.5650546063582103\n",
      "Epoch 9, Batch: 10000| Training Loss: 2.563202931725979\n",
      "Epoch 9, Batch: 11000| Training Loss: 2.5614651249863885\n",
      "Epoch 9, Batch: 12000| Training Loss: 2.558929222593705\n",
      "Epoch 9, Batch: 13000| Training Loss: 2.5578428015525523\n",
      "Epoch 9, Batch: 14000| Training Loss: 2.5570287166322982\n",
      "Epoch 9, Batch: 15000| Training Loss: 2.55551614716053\n",
      "Epoch 9, Batch: 16000| Training Loss: 2.553303094774485\n",
      "Epoch 9, Batch: 17000| Training Loss: 2.551567654266077\n",
      "Epoch 9, Batch: 18000| Training Loss: 2.5503882195750873\n",
      "Epoch 9, Batch: 19000| Training Loss: 2.5481156351942764\n",
      "Epoch 9, Batch: 20000| Training Loss: 2.5478197799682616\n",
      "Epoch 9, Batch: 21000| Training Loss: 2.5470049419119243\n",
      "Epoch 9, Batch: 22000| Training Loss: 2.545286584165963\n",
      "Epoch 9, Batch: 23000| Training Loss: 2.5433601272158\n",
      "Epoch 9, Batch: 24000| Training Loss: 2.5420123490641515\n",
      "Epoch 9, Batch: 25000| Training Loss: 2.540224390501976\n",
      "Epoch 9, Training Loss: 2.5394104992192426, Validation Error: 72.09920669514428, Validation Top-3 Accuracy: 48.78824862662596, Training Error: 68.41976471739783\n",
      "Epoch 10, Batch: 1000| Training Loss: 2.56681403195858\n",
      "Epoch 10, Batch: 2000| Training Loss: 2.560149531841278\n",
      "Epoch 10, Batch: 3000| Training Loss: 2.561791538874308\n",
      "Epoch 10, Batch: 4000| Training Loss: 2.5628273230195044\n",
      "Epoch 10, Batch: 5000| Training Loss: 2.5566755803823473\n",
      "Epoch 10, Batch: 6000| Training Loss: 2.5542537104884784\n",
      "Epoch 10, Batch: 7000| Training Loss: 2.551227181008884\n",
      "Epoch 10, Batch: 8000| Training Loss: 2.5485607362091542\n",
      "Epoch 10, Batch: 9000| Training Loss: 2.548568366540803\n",
      "Epoch 10, Batch: 10000| Training Loss: 2.5467330105900765\n",
      "Epoch 10, Batch: 11000| Training Loss: 2.5449853281649677\n",
      "Epoch 10, Batch: 12000| Training Loss: 2.542459654589494\n",
      "Epoch 10, Batch: 13000| Training Loss: 2.5414291294996554\n",
      "Epoch 10, Batch: 14000| Training Loss: 2.5406890376039915\n",
      "Epoch 10, Batch: 15000| Training Loss: 2.5390507150014243\n",
      "Epoch 10, Batch: 16000| Training Loss: 2.5368099968954922\n",
      "Epoch 10, Batch: 17000| Training Loss: 2.535053325737224\n",
      "Epoch 10, Batch: 18000| Training Loss: 2.5339546286066374\n",
      "Epoch 10, Batch: 19000| Training Loss: 2.531782887007061\n",
      "Epoch 10, Batch: 20000| Training Loss: 2.5315011556088924\n",
      "Epoch 10, Batch: 21000| Training Loss: 2.530646411214556\n",
      "Epoch 10, Batch: 22000| Training Loss: 2.5289936655109577\n",
      "Epoch 10, Batch: 23000| Training Loss: 2.5271431494536607\n",
      "Epoch 10, Batch: 24000| Training Loss: 2.5257807784080506\n",
      "Epoch 10, Batch: 25000| Training Loss: 2.524061273508072\n",
      "Epoch 10, Training Loss: 2.5232615483487635, Validation Error: 72.00621858018772, Validation Top-3 Accuracy: 48.94371313131895, Training Error: 68.21042070682662\n",
      "Epoch 11, Batch: 1000| Training Loss: 2.553467501282692\n",
      "Epoch 11, Batch: 2000| Training Loss: 2.5473553225398065\n",
      "Epoch 11, Batch: 3000| Training Loss: 2.5490424966812135\n",
      "Epoch 11, Batch: 4000| Training Loss: 2.5502828776538373\n",
      "Epoch 11, Batch: 5000| Training Loss: 2.544268540596962\n",
      "Epoch 11, Batch: 6000| Training Loss: 2.541955498437087\n",
      "Epoch 11, Batch: 7000| Training Loss: 2.5392436019352504\n",
      "Epoch 11, Batch: 8000| Training Loss: 2.5366732833087444\n",
      "Epoch 11, Batch: 9000| Training Loss: 2.5366269638140997\n",
      "Epoch 11, Batch: 10000| Training Loss: 2.5349003220200537\n",
      "Epoch 11, Batch: 11000| Training Loss: 2.5331794048331\n",
      "Epoch 11, Batch: 12000| Training Loss: 2.530729010651509\n",
      "Epoch 11, Batch: 13000| Training Loss: 2.5298722680256915\n",
      "Epoch 11, Batch: 14000| Training Loss: 2.5290286399722097\n",
      "Epoch 11, Batch: 15000| Training Loss: 2.5274874745051066\n",
      "Epoch 11, Batch: 16000| Training Loss: 2.5252337026298046\n",
      "Epoch 11, Batch: 17000| Training Loss: 2.523595171430532\n",
      "Epoch 11, Batch: 18000| Training Loss: 2.5224497066338856\n",
      "Epoch 11, Batch: 19000| Training Loss: 2.5202910674998633\n",
      "Epoch 11, Batch: 20000| Training Loss: 2.5199994537711143\n",
      "Epoch 11, Batch: 21000| Training Loss: 2.519219618320465\n",
      "Epoch 11, Batch: 22000| Training Loss: 2.517544999637387\n",
      "Epoch 11, Batch: 23000| Training Loss: 2.5156791921491206\n",
      "Epoch 11, Batch: 24000| Training Loss: 2.5142452359199523\n",
      "Epoch 11, Batch: 25000| Training Loss: 2.512482847576141\n",
      "Epoch 11, Training Loss: 2.511693900683111, Validation Error: 71.94374219045127, Validation Top-3 Accuracy: 48.94952488850374, Training Error: 68.0267094082453\n",
      "Epoch 12, Batch: 1000| Training Loss: 2.543293838620186\n",
      "Epoch 12, Batch: 2000| Training Loss: 2.5373702046871185\n",
      "Epoch 12, Batch: 3000| Training Loss: 2.5392526483535764\n",
      "Epoch 12, Batch: 4000| Training Loss: 2.540057442396879\n",
      "Epoch 12, Batch: 5000| Training Loss: 2.5342420295238495\n",
      "Epoch 12, Batch: 6000| Training Loss: 2.5319489828745523\n",
      "Epoch 12, Batch: 7000| Training Loss: 2.5293134885685786\n",
      "Epoch 12, Batch: 8000| Training Loss: 2.5269579568207265\n",
      "Epoch 12, Batch: 9000| Training Loss: 2.527011247171296\n",
      "Epoch 12, Batch: 10000| Training Loss: 2.525254525077343\n",
      "Epoch 12, Batch: 11000| Training Loss: 2.523629799398509\n",
      "Epoch 12, Batch: 12000| Training Loss: 2.521135803103447\n",
      "Epoch 12, Batch: 13000| Training Loss: 2.5202142969553285\n",
      "Epoch 12, Batch: 14000| Training Loss: 2.519520762673446\n",
      "Epoch 12, Batch: 15000| Training Loss: 2.5180878323396048\n",
      "Epoch 12, Batch: 16000| Training Loss: 2.5159356627017258\n",
      "Epoch 12, Batch: 17000| Training Loss: 2.5143566462993623\n",
      "Epoch 12, Batch: 18000| Training Loss: 2.5132497075862354\n",
      "Epoch 12, Batch: 19000| Training Loss: 2.511050005222622\n",
      "Epoch 12, Batch: 20000| Training Loss: 2.5108310621142387\n",
      "Epoch 12, Batch: 21000| Training Loss: 2.510173068676676\n",
      "Epoch 12, Batch: 22000| Training Loss: 2.5085379406593065\n",
      "Epoch 12, Batch: 23000| Training Loss: 2.5066986654582233\n",
      "Epoch 12, Batch: 24000| Training Loss: 2.5053915408551695\n",
      "Epoch 12, Batch: 25000| Training Loss: 2.50370935195446\n",
      "Epoch 12, Training Loss: 2.502957239267494, Validation Error: 71.90451282945398, Validation Top-3 Accuracy: 49.04541888205269, Training Error: 67.93194790258346\n",
      "Epoch 13, Batch: 1000| Training Loss: 2.5370378701686858\n",
      "Epoch 13, Batch: 2000| Training Loss: 2.5322136712670327\n",
      "Epoch 13, Batch: 3000| Training Loss: 2.533961107691129\n",
      "Epoch 13, Batch: 4000| Training Loss: 2.535274796783924\n",
      "Epoch 13, Batch: 5000| Training Loss: 2.5295150082588194\n",
      "Epoch 13, Batch: 6000| Training Loss: 2.526884123543898\n",
      "Epoch 13, Batch: 7000| Training Loss: 2.524285138266427\n",
      "Epoch 13, Batch: 8000| Training Loss: 2.521792883276939\n",
      "Epoch 13, Batch: 9000| Training Loss: 2.521888610985544\n",
      "Epoch 13, Batch: 10000| Training Loss: 2.5201230486273767\n",
      "Epoch 13, Batch: 11000| Training Loss: 2.5184295614741066\n",
      "Epoch 13, Batch: 12000| Training Loss: 2.5160482522249223\n",
      "Epoch 13, Batch: 13000| Training Loss: 2.5152424144378074\n",
      "Epoch 13, Batch: 14000| Training Loss: 2.514633881568909\n",
      "Epoch 13, Batch: 15000| Training Loss: 2.5131953220685324\n",
      "Epoch 13, Batch: 16000| Training Loss: 2.5110844136923554\n",
      "Epoch 13, Batch: 17000| Training Loss: 2.5095186686656055\n",
      "Epoch 13, Batch: 18000| Training Loss: 2.5085785607496898\n",
      "Epoch 13, Batch: 19000| Training Loss: 2.5064251913773385\n",
      "Epoch 13, Batch: 20000| Training Loss: 2.5063172518253327\n",
      "Epoch 13, Batch: 21000| Training Loss: 2.5056253214166277\n",
      "Epoch 13, Batch: 22000| Training Loss: 2.5040595415938984\n",
      "Epoch 13, Batch: 23000| Training Loss: 2.5022736714404563\n",
      "Epoch 13, Batch: 24000| Training Loss: 2.501023840546608\n",
      "Epoch 13, Batch: 25000| Training Loss: 2.4993077702903745\n",
      "Epoch 13, Training Loss: 2.4985281681129567, Validation Error: 71.93357161537791, Validation Top-3 Accuracy: 49.0396071248679, Training Error: 67.8664896643277\n",
      "Epoch 14, Batch: 1000| Training Loss: 2.534256413698196\n",
      "Epoch 14, Batch: 2000| Training Loss: 2.527533575296402\n",
      "Epoch 14, Batch: 3000| Training Loss: 2.528925926407178\n",
      "Epoch 14, Batch: 4000| Training Loss: 2.5303451940715314\n",
      "Epoch 14, Batch: 5000| Training Loss: 2.524634659767151\n",
      "Epoch 14, Batch: 6000| Training Loss: 2.522538906951745\n",
      "Epoch 14, Batch: 7000| Training Loss: 2.519612705860819\n",
      "Epoch 14, Batch: 8000| Training Loss: 2.517171920314431\n",
      "Epoch 14, Batch: 9000| Training Loss: 2.517205743485027\n",
      "Epoch 14, Batch: 10000| Training Loss: 2.5155436821699144\n",
      "Epoch 14, Batch: 11000| Training Loss: 2.5138857527212664\n",
      "Epoch 14, Batch: 12000| Training Loss: 2.5117356292307376\n",
      "Epoch 14, Batch: 13000| Training Loss: 2.510960995564094\n",
      "Epoch 14, Batch: 14000| Training Loss: 2.5103062401413916\n",
      "Epoch 14, Batch: 15000| Training Loss: 2.5089368913412096\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4 (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744238\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1331642\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # alpha can be set to a constant, or it can be a tensor of shape (num_classes,)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 128\n",
    "d_embed = 64\n",
    "NUM_EPOCHS = 15\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalThree(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(gamma=2, alpha=1, reduction='mean')\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [64, 1] but got: [64, 64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[45], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay)\u001b[0m\n\u001b[1;32m     22\u001b[0m boards, sequences, lengths, labels \u001b[38;5;241m=\u001b[39m boards\u001b[38;5;241m.\u001b[39mto(device), sequences\u001b[38;5;241m.\u001b[39mto(device), lengths, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backpropogate & Optimize\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 59\u001b[0m, in \u001b[0;36mMultiModalThree.forward\u001b[0;34m(self, board, sequence, seq_lengths)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Attention mechanism\u001b[39;00m\n\u001b[1;32m     58\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mbmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_query(seq_encoding)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_keys(cnn_encoding)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m attention_applied \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Combining both encodings with attention applied\u001b[39;00m\n\u001b[1;32m     62\u001b[0m combined_encoding \u001b[38;5;241m=\u001b[39m attention_applied\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [64, 1] but got: [64, 64]."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2887027\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257737\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # alpha can be set to a constant, or it can be a tensor of shape (num_classes,)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "# Reload the data with particular batch size\n",
    "torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 256\n",
    "d_embed = 64\n",
    "NUM_EPOCHS = 10\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalTwo(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(gamma=2, alpha=1, reduction='mean')\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('multi-modal-jan-march-g5-feb-27.png')  # This will save the plot as an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
