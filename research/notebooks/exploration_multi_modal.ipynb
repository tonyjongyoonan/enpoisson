{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Games of Elo ~1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import dask.dataframe as dd \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import chess\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/ubuntu/enpoisson/research/notebooks/../data/lichess_db_standard_rated_2019-01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/dask/backends.py:141\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/dask/dataframe/io/csv.py:763\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    751\u001b[0m     urlpath,\n\u001b[1;32m    752\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    762\u001b[0m ):\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/dask/dataframe/io/csv.py:563\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m b_lineterminator \u001b[38;5;241m=\u001b[39m lineterminator\u001b[38;5;241m.\u001b[39mencode()\n\u001b[0;32m--> 563\u001b[0m b_out \u001b[38;5;241m=\u001b[39m \u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_lineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_path_column:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/dask/bytes/core.py:111\u001b[0m, in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot do chunked reads on compressed files. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo read, set blocksize=None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[0;32m--> 111\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/fsspec/implementations/local.py:83\u001b[0m, in \u001b[0;36mLocalFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m---> 83\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m link \u001b[38;5;241m=\u001b[39m stat\u001b[38;5;241m.\u001b[39mS_ISLNK(out\u001b[38;5;241m.\u001b[39mst_mode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/enpoisson/research/notebooks/../data/lichess_db_standard_rated_2019-01.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import CSV File (from Maia: http://csslab.cs.toronto.edu/datasets/#monthly_chess_csv)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The CSV has 151,072,060 rows\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data_types \u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclock\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopp_clock\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopp_clock_percent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/lichess_db_standard_rated_2019-01.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m64e6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Filter out quick games (Bullet and HyperBullet) and take out moves that happened in the last XX seconds (this won't affect how many games we import but the # of moves we look at)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m condition_time_control \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_control\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBullet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHyperBullet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/dask/backends.py:143\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/ubuntu/enpoisson/research/notebooks/../data/lichess_db_standard_rated_2019-01.csv'"
     ]
    }
   ],
   "source": [
    "# Import CSV File (from Maia: http://csslab.cs.toronto.edu/datasets/#monthly_chess_csv)\n",
    "# The CSV has 151,072,060 rows\n",
    "data_types ={'clock': 'float32',\n",
    "       'cp': 'object',\n",
    "       'opp_clock': 'float32',\n",
    "       'opp_clock_percent': 'float32'}\n",
    "df = dd.read_csv('../data/lichess_db_standard_rated_2019-01.csv', blocksize='64e6', dtype= data_types, low_memory=False)\n",
    "\n",
    "# Filter out quick games (Bullet and HyperBullet) and take out moves that happened in the last XX seconds (this won't affect how many games we import but the # of moves we look at)\n",
    "condition_time_control = ~df['time_control'].isin(['Bullet', 'HyperBullet'])\n",
    "condition_clock = df['clock'] > 45\n",
    "# condition_plays = df['num_ply'] < 80\n",
    "filtered_df = df[condition_time_control & condition_clock]\n",
    "\n",
    "# Select Relevant Columns\n",
    "selected_columns = ['game_id','white_elo','black_elo','move','white_active','board']\n",
    "filtered_df = filtered_df[selected_columns]\n",
    "\n",
    "# Filter only games of Elo 1100-1199\n",
    "filtered_df = filtered_df[(filtered_df['white_elo'].between(1500, 1599)) & (filtered_df['black_elo'].between(1500, 1599))]\n",
    "\n",
    "# Group Same Games Together \n",
    "def aggregate_moves(group):\n",
    "    moves = ' '.join(group['move'])  # Concatenate moves into a single string\n",
    "    white_elo = group['white_elo'].iloc[0]  # Get the first white_elo\n",
    "    black_elo = group['black_elo'].iloc[0]  # Get the first black_elo\n",
    "    white_active = group['white_active'].iloc[0]  # Get the first num_ply\n",
    "    board = '*'.join(group['board'])  # Get the first num_ply\n",
    "    return pd.Series({'moves': moves, 'white_elo': white_elo, 'black_elo': black_elo, 'white_active': white_active, 'board': board})\n",
    "\n",
    "grouped_df = filtered_df.groupby('game_id',sort=True).apply(aggregate_moves, meta={'moves': 'str', 'white_elo': 'int', 'black_elo': 'int', 'white_active': 'str', 'board': 'str'}).compute()\n",
    "\n",
    "# This gives us 99,300 Games when we don't filter games with more than 80 half-moves\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.to_csv('haha_longer_1500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.read_csv(\"haha_longer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        game_id                                              moves  white_elo  \\\n",
      "0      001QYJhZ  d2d4 f7f6 c2c4 e7e5 d4e5 f6e5 e2e3 b8c6 b1c3 f...       1127   \n",
      "1      0073Uoft  e2e4 e7e5 g1f3 b8c6 f1c4 g8f6 f3g5 d7d5 g5f7 e...       1142   \n",
      "2      008AlITg  e2e4 e7e5 g1f3 b8c6 b1c3 g8f6 f3e5 f8b4 e5c6 d...       1187   \n",
      "3      00911g2y  e2e4 d7d5 e4d5 d8d5 g1f3 d5a5 b1c3 g8f6 d2d4 c...       1135   \n",
      "4      00A6eGbe  b2b4 e7e5 c1b2 g8f6 b2e5 b8c6 e5c3 d7d5 d2d4 f...       1112   \n",
      "...         ...                                                ...        ...   \n",
      "99290  zzo2DSPt  d2d4 g8f6 c1f4 g7g6 g1f3 f8g7 b1c3 e8g8 e2e3 d...       1141   \n",
      "99291  zzo5kmuO  e2e4 e7e5 g1f3 d7d6 b1c3 c8g4 f1e2 g4f3 e2f3 d...       1112   \n",
      "99292  zztPJDK8  e2e4 c7c5 g1f3 b8c6 f1c4 e7e6 d2d3 d7d5 e4d5 e...       1178   \n",
      "99293  zzx4qVJx  e2e3 d7d5 f2f4 g8f6 b2b3 e7e6 c1b2 f6e4 g2g4 d...       1197   \n",
      "99294  zzxkXY9h  d2d4 d7d5 g1f3 g8f6 c1f4 b8c6 e2e3 c8f5 f1b5 f...       1140   \n",
      "\n",
      "       black_elo  white_active  \\\n",
      "0           1165          True   \n",
      "1           1179          True   \n",
      "2           1168          True   \n",
      "3           1126          True   \n",
      "4           1107          True   \n",
      "...          ...           ...   \n",
      "99290       1149          True   \n",
      "99291       1123          True   \n",
      "99292       1151          True   \n",
      "99293       1197          True   \n",
      "99294       1124          True   \n",
      "\n",
      "                                                   board  \n",
      "0      rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "1      rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2      rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "3      rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "4      rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                  ...  \n",
      "99290  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "99291  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "99292  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "99293  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "99294  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[99295 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw data, we need to be able to make sense of chess moves. Meaning, we're transforming our entire world from chess moves into numerical tokens that will serve as indices into unique embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a mapping from each move to a unique embedding. In order to index into our matrix of \n",
    "# embeddings (matrix format so it's something we can tune), we'll also want a mapping from each move to a unique ID\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.move_to_id = {\"<UNK>\": 0}\n",
    "        self.id_to_move = {0: \"<UNK>\"}\n",
    "        self.index = 1  # Start indexing from 1\n",
    "\n",
    "    def add_move(self, move):\n",
    "        if move not in self.move_to_id:\n",
    "            self.move_to_id[move] = self.index\n",
    "            self.id_to_move[self.index] = move\n",
    "            self.index += 1\n",
    "\n",
    "    def get_id(self, move):\n",
    "        return self.move_to_id.get(move, self.move_to_id[\"<UNK>\"])\n",
    "\n",
    "    def get_move(self, id):\n",
    "        return self.id_to_move.get(id, self.id_to_move[0])\n",
    "\n",
    "# We can just use nn.Embedding later when we pass the model a sequence of indices, but this is if we ever want to pre-train and have access to the matrix we've trained\n",
    "def get_embedding_matrix(vocab, d_embed):\n",
    "    n_embed = len(vocab.move_to_id)\n",
    "    return np.random.normal(0, 1, (n_embed, d_embed))\n",
    "# embedding_matrix = get_embedding_matrix(vocab, 64)\n",
    "    \n",
    "piece_to_index = {\n",
    "    'p' : 0,\n",
    "    'r' : 1,\n",
    "    'b' : 2,\n",
    "    'n' : 3,\n",
    "    'q' : 4,\n",
    "    'k' : 5,\n",
    "}\n",
    "\n",
    "def string_to_array(string):\n",
    "    rows = string.split(\"/\")\n",
    "    ans = [[[0 for a in range(8)] for b in range(8)] for c in range(6)]\n",
    "    for row in range(8):\n",
    "        curr_row = rows[row]\n",
    "        #print(curr_row)\n",
    "        offset = 0\n",
    "        for piece in range(len(curr_row)):\n",
    "            curr_piece = curr_row[piece]\n",
    "            sign = 1 if curr_piece.lower() == curr_piece else -1 # check if the piece is capitalized\n",
    "            curr_piece = curr_piece.lower() # after storing whether or not capitalized, standardize it to lower case for easy processing\n",
    "            if curr_piece not in piece_to_index.keys():\n",
    "                offset += int(curr_piece) - 1\n",
    "            else:\n",
    "                current_board = ans[piece_to_index[curr_piece]]\n",
    "                current_board[row][offset + piece] = 1 * sign\n",
    "                ans[piece_to_index[curr_piece]] = current_board\n",
    "    return ans\n",
    "\n",
    "# Function to pad move sequences & get their sequence lengths\n",
    "def pad_sequences(sequences, max_len=None, pad_id=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = np.full((len(sequences), max_len), pad_id, dtype=int)\n",
    "    sequence_lengths = np.zeros(len(sequences), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = seq[:length]\n",
    "        sequence_lengths[i] = length\n",
    "    return padded_sequences, sequence_lengths\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, sequences, boards, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.boards = boards\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        boards, sequences, lengths, labels = self.boards[idx], self.sequences[idx], self.lengths[idx], self.labels[idx]\n",
    "        return torch.tensor(boards, dtype=torch.float32), torch.tensor(sequences, dtype=torch.long), torch.tensor(lengths, dtype=torch.long), torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_data(df, sampling_rate=1, fixed_window = True, fixed_window_size = 16, algebraic_notation=True):\n",
    "    \"\"\"\n",
    "    Input: Dataframe of training data in which each row represents a full game played between players\n",
    "    Output: List in which each item represents some game's history up until a particular move, List in the same order in which the associated label is the following move\n",
    "    \"\"\"\n",
    "    board_states = []\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "    vocab = Vocabulary()\n",
    "    chess_board = chess.Board()\n",
    "    for game_board, game_moves in zip(df['board'],df['moves']):\n",
    "        moves = game_moves.split()\n",
    "        boards = game_board.split('*')\n",
    "        # Encode the moves into SAN notation and then into corresponding indices\n",
    "        encoded_moves = []\n",
    "        for move in moves:\n",
    "            # Create a move object from the coordinate notation\n",
    "            move_obj = chess.Move.from_uci(move)\n",
    "            if move_obj not in chess_board.legal_moves:\n",
    "                break \n",
    "            else:\n",
    "                if algebraic_notation:\n",
    "                    algebraic_move = chess_board.san(move_obj)\n",
    "                    chess_board.push(move_obj)\n",
    "                    vocab.add_move(algebraic_move)\n",
    "                    encoded_move = vocab.get_id(algebraic_move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "                else:\n",
    "                    encoded_move = vocab.get_id(move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "        chess_board.reset()\n",
    "        boards = boards[:len(encoded_moves)]\n",
    "        # Now generate X,Y with sampling\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            #TODO: Figure out how to deal with black orientation 'seeing' a different board\n",
    "            if random.uniform(0, 1) <= sampling_rate and 'w' in boards[i]:\n",
    "                # Board\n",
    "                board_states.append(string_to_array(boards[i].split(' ')[0]))\n",
    "                # Sequence of Moves\n",
    "                subseq = encoded_moves[0:i+1]\n",
    "                if fixed_window and len(subseq) > fixed_window_size:\n",
    "                    subseq = subseq[-fixed_window_size:]\n",
    "                subsequences.append(subseq)\n",
    "                # Label\n",
    "                label = encoded_moves[i+1]\n",
    "                next_moves.append(label)\n",
    "\n",
    "    return subsequences, board_states, next_moves, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_sequences, trainX_boards, trainY, vocab = df_to_data(grouped_df, fixed_window=True, sampling_rate=0.8)\n",
    "trainX_sequences, trainX_seqlengths  = pad_sequences(trainX_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5690\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.id_to_move.keys()))\n",
    "print(len(trainX_sequences[140]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1836041\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX_boards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModal(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn = RNNModel(vocab,d_embed, d_hidden, 16, dropout=dropout)\n",
    "        self.cnn = ChessCNN(32)\n",
    "        self.fc = nn.Linear(48,d_out)\n",
    "\n",
    "    def forward(self, board, sequence, seq_lengths):\n",
    "        seq_encoding = self.rnn(sequence, seq_lengths)\n",
    "        cnn_encoding = self.cnn(board)\n",
    "        pred = self.fc(torch.cat((seq_encoding,cnn_encoding),dim=1))\n",
    "        return pred\n",
    "    \n",
    "class MultiModalTwo(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn = RNNModel(vocab,d_embed, d_hidden, 16, dropout=dropout)\n",
    "        self.cnn = SENet(64)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16+64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, board, sequence, seq_lengths):\n",
    "        seq_encoding = self.rnn(sequence, seq_lengths)\n",
    "        cnn_encoding = self.cnn(board)\n",
    "        pred = self.fc(torch.cat((seq_encoding,cnn_encoding),dim=1))\n",
    "        return pred\n",
    "    \n",
    "class MultiModalThree(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        d_encoding = 64\n",
    "        self.rnn = RNNModel(vocab,d_embed, d_hidden, d_encoding, dropout=dropout)\n",
    "        self.cnn = SENet(d_encoding)\n",
    "\n",
    "        # Attention layers\n",
    "        self.seq_query = nn.Linear(d_encoding, d_encoding)\n",
    "        self.img_keys = nn.Linear(d_encoding, d_encoding)\n",
    "        self.img_values = nn.Linear(d_encoding, d_encoding)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_encoding, 64),  # Adjusted to combine attention outputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, board, sequence, seq_lengths):\n",
    "        # Encode sequence of moves\n",
    "        seq_encoding = self.rnn(sequence, seq_lengths)\n",
    "        \n",
    "        # Encode board state\n",
    "        cnn_encoding = self.cnn(board)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_weights = F.softmax(torch.bmm(self.seq_query(seq_encoding).unsqueeze(1),self.img_keys(cnn_encoding).unsqueeze(2)).squeeze(2), dim=1)\n",
    "        attention_applied = torch.bmm(attn_weights.unsqueeze(1), self.img_values(cnn_encoding).unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        # Combining both encodings with attention applied\n",
    "        combined_encoding = attention_applied\n",
    "        \n",
    "        # Prediction\n",
    "        pred = self.fc(combined_encoding)\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "    \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5, num_layers = 2, bidirectional = False, embedding_matrix = None):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(vocab.move_to_id), d_embed)\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(d_embed, d_hidden, dropout = dropout, bidirectional=bidirectional, num_layers = num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_hidden,d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = self.embeddings(x)\n",
    "        # Sort x and seq_lengths in descending order\n",
    "        # This is required for packing the sequence\n",
    "        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        x = x[perm_idx]\n",
    "        # Pack the sequence\n",
    "        packed_input = pack_padded_sequence(x, seq_lengths, batch_first=True)\n",
    "        # Pass the packed sequence through the LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        # Unpack the sequence\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True,total_length = x.size()[1])\n",
    "        _, unperm_idx = perm_idx.sort(0)\n",
    "        unperm_idx = unperm_idx.to(device)\n",
    "        output = output.index_select(0, unperm_idx)\n",
    "        #This takes all the outputs across the cells\n",
    "        mean_pooled = torch.mean(output, dim=1)\n",
    "        #output = torch.cat((mean_pooled,hidden[-1]),dim=1)\n",
    "        output = self.fc(mean_pooled)\n",
    "        return output\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.se = SELayer(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "class SENet(nn.Module):\n",
    "    def __init__(self, d_out):\n",
    "        super(SENet, self).__init__()\n",
    "        self.conv1 = ConvBlock(6, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = ConvBlock(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = ConvBlock(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64*8*8, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x       \n",
    "    \n",
    "class ChessCNN(nn.Module):\n",
    "    def __init__(self, d_out):\n",
    "        super(ChessCNN, self).__init__()\n",
    "        # Assuming each channel represents a different piece type (e.g., 6 channels for 6 types each)\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization for first conv layer\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization for second conv layer\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)  # Batch normalization for second conv layer\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)  # Assuming an 8x8 chess board\n",
    "        self.fc2 = nn.Linear(64, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, followed by batch norm, then ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Apply second convolution, followed by batch norm, then ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(trainY[:5000]),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate top-3 accuracy\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    top3 = torch.topk(y_pred, 3, dim=1).indices\n",
    "    correct = top3.eq(y_true.view(-1, 1).expand_as(top3))\n",
    "    return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "def train(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for boards, sequences, lengths, labels in train_loader:\n",
    "            count += 1\n",
    "            boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths, labels.to(device)\n",
    "            # Forward Pass\n",
    "            output = model(boards, sequences, lengths)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                validation_loss = 0\n",
    "\n",
    "                for boards, sequences, lengths, labels in val_loader:\n",
    "                    boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths, labels.to(device)\n",
    "                    outputs = model(boards, sequences, lengths)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    val_top3_correct += top_3_accuracy(labels, outputs) * labels.size(0)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    validation_loss += loss.item()\n",
    "\n",
    "                val_loss_values.append(validation_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        if epoch <= 10:\n",
    "            for op_params in optimizer.param_groups:\n",
    "                op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1836041\n"
     ]
    }
   ],
   "source": [
    "dataset = MultimodalDataset(trainX_sequences, trainX_boards, trainX_seqlengths, trainY)\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 (CNN has 2 Convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634630\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993054\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 72\n",
    "d_embed = 32\n",
    "NUM_EPOCHS = 25\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModal(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.7 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 5.3776393618583676\n",
      "Epoch 1, Batch: 2000| Training Loss: 5.148660890340805\n",
      "Epoch 1, Batch: 3000| Training Loss: 5.008869136333465\n",
      "Epoch 1, Batch: 4000| Training Loss: 4.90768612074852\n",
      "Epoch 1, Batch: 5000| Training Loss: 4.822340172863006\n",
      "Epoch 1, Batch: 6000| Training Loss: 4.755600390513738\n",
      "Epoch 1, Batch: 7000| Training Loss: 4.701854768855231\n",
      "Epoch 1, Batch: 8000| Training Loss: 4.6509501395523545\n",
      "Epoch 1, Batch: 9000| Training Loss: 4.608193051099777\n",
      "Epoch 1, Batch: 10000| Training Loss: 4.568666521668434\n",
      "Epoch 1, Batch: 11000| Training Loss: 4.532528639186513\n",
      "Epoch 1, Batch: 12000| Training Loss: 4.498760126252969\n",
      "Epoch 1, Batch: 13000| Training Loss: 4.468972160651133\n",
      "Epoch 1, Batch: 14000| Training Loss: 4.4421408867836\n",
      "Epoch 1, Batch: 15000| Training Loss: 4.417240078322092\n",
      "Epoch 1, Batch: 16000| Training Loss: 4.394133803024888\n",
      "Epoch 1, Batch: 17000| Training Loss: 4.373170186645845\n",
      "Epoch 1, Batch: 18000| Training Loss: 4.35459246439404\n",
      "Epoch 1, Batch: 19000| Training Loss: 4.336680315544731\n",
      "Epoch 1, Batch: 20000| Training Loss: 4.320931764805317\n",
      "Epoch 1, Batch: 21000| Training Loss: 4.305459042583193\n",
      "Epoch 1, Batch: 22000| Training Loss: 4.2896047235944055\n",
      "Epoch 1, Batch: 23000| Training Loss: 4.275040728765985\n",
      "Epoch 1, Batch: 24000| Training Loss: 4.26135839899381\n",
      "Epoch 1, Batch: 25000| Training Loss: 4.248005154294968\n",
      "Epoch 1, Training Loss: 4.240891174313867, Validation Error: 80.42454886234853, Validation Top-3 Accuracy: 35.3354836835783, Training Error: 83.16909637043246\n",
      "Epoch 2, Batch: 1000| Training Loss: 3.8947526326179505\n",
      "Epoch 2, Batch: 2000| Training Loss: 3.8931626502275467\n",
      "Epoch 2, Batch: 3000| Training Loss: 3.8856371714274087\n",
      "Epoch 2, Batch: 4000| Training Loss: 3.881563696503639\n",
      "Epoch 2, Batch: 5000| Training Loss: 3.869561003637314\n",
      "Epoch 2, Batch: 6000| Training Loss: 3.86171338947614\n",
      "Epoch 2, Batch: 7000| Training Loss: 3.857014669077737\n",
      "Epoch 2, Batch: 8000| Training Loss: 3.8507370053231718\n",
      "Epoch 2, Batch: 9000| Training Loss: 3.847168283356561\n",
      "Epoch 2, Batch: 10000| Training Loss: 3.8423706159591675\n",
      "Epoch 2, Batch: 11000| Training Loss: 3.8378522130576047\n",
      "Epoch 2, Batch: 12000| Training Loss: 3.832757584055265\n",
      "Epoch 2, Batch: 13000| Training Loss: 3.828193236589432\n",
      "Epoch 2, Batch: 14000| Training Loss: 3.824352010726929\n",
      "Epoch 2, Batch: 15000| Training Loss: 3.8199313018480936\n",
      "Epoch 2, Batch: 16000| Training Loss: 3.815777197852731\n",
      "Epoch 2, Batch: 17000| Training Loss: 3.8117296205688924\n",
      "Epoch 2, Batch: 18000| Training Loss: 3.8088331150081425\n",
      "Epoch 2, Batch: 19000| Training Loss: 3.8052808979185\n",
      "Epoch 2, Batch: 20000| Training Loss: 3.8026617864847183\n",
      "Epoch 2, Batch: 21000| Training Loss: 3.7993369262218475\n",
      "Epoch 2, Batch: 22000| Training Loss: 3.7949166041829367\n",
      "Epoch 2, Batch: 23000| Training Loss: 3.7910796794269395\n",
      "Epoch 2, Batch: 24000| Training Loss: 3.7876595894296963\n",
      "Epoch 2, Batch: 25000| Training Loss: 3.784007432451248\n",
      "Epoch 2, Training Loss: 3.7819294663941454, Validation Error: 78.2553104931276, Validation Top-3 Accuracy: 38.39246796277512, Training Error: 78.47678067819629\n",
      "Epoch 3, Batch: 1000| Training Loss: 3.6906444866657258\n",
      "Epoch 3, Batch: 2000| Training Loss: 3.6920419754981997\n",
      "Epoch 3, Batch: 3000| Training Loss: 3.686445578893026\n",
      "Epoch 3, Batch: 4000| Training Loss: 3.6841544363498686\n",
      "Epoch 3, Batch: 5000| Training Loss: 3.6741195709228514\n",
      "Epoch 3, Batch: 6000| Training Loss: 3.668339399298032\n",
      "Epoch 3, Batch: 7000| Training Loss: 3.6658571934700013\n",
      "Epoch 3, Batch: 8000| Training Loss: 3.661564088433981\n",
      "Epoch 3, Batch: 9000| Training Loss: 3.659744343333774\n",
      "Epoch 3, Batch: 10000| Training Loss: 3.6568897661447526\n",
      "Epoch 3, Batch: 11000| Training Loss: 3.654187175208872\n",
      "Epoch 3, Batch: 12000| Training Loss: 3.6508312974770862\n",
      "Epoch 3, Batch: 13000| Training Loss: 3.6482347716918357\n",
      "Epoch 3, Batch: 14000| Training Loss: 3.6462554720129288\n",
      "Epoch 3, Batch: 15000| Training Loss: 3.6435669055779774\n",
      "Epoch 3, Batch: 16000| Training Loss: 3.6413881939202546\n",
      "Epoch 3, Batch: 17000| Training Loss: 3.639002285859164\n",
      "Epoch 3, Batch: 18000| Training Loss: 3.637969186888801\n",
      "Epoch 3, Batch: 19000| Training Loss: 3.6358605631527148\n",
      "Epoch 3, Batch: 20000| Training Loss: 3.6349527294158936\n",
      "Epoch 3, Batch: 21000| Training Loss: 3.6331885712033225\n",
      "Epoch 3, Batch: 22000| Training Loss: 3.630454562934962\n",
      "Epoch 3, Batch: 23000| Training Loss: 3.6281821711581688\n",
      "Epoch 3, Batch: 24000| Training Loss: 3.6263938610752424\n",
      "Epoch 3, Batch: 25000| Training Loss: 3.6241708958053587\n",
      "Epoch 3, Training Loss: 3.6229454323190855, Validation Error: 77.09877081335542, Validation Top-3 Accuracy: 39.95728358469183, Training Error: 76.54845439029016\n",
      "Epoch 4, Batch: 1000| Training Loss: 3.5819134035110474\n",
      "Epoch 4, Batch: 2000| Training Loss: 3.5830100947618484\n",
      "Epoch 4, Batch: 3000| Training Loss: 3.579934163649877\n",
      "Epoch 4, Batch: 4000| Training Loss: 3.5791403488516806\n",
      "Epoch 4, Batch: 5000| Training Loss: 3.5698736426353457\n",
      "Epoch 4, Batch: 6000| Training Loss: 3.564986297051112\n",
      "Epoch 4, Batch: 7000| Training Loss: 3.5635304953711375\n",
      "Epoch 4, Batch: 8000| Training Loss: 3.560080125153065\n",
      "Epoch 4, Batch: 9000| Training Loss: 3.558824081659317\n",
      "Epoch 4, Batch: 10000| Training Loss: 3.5565023228645325\n",
      "Epoch 4, Batch: 11000| Training Loss: 3.554438588142395\n",
      "Epoch 4, Batch: 12000| Training Loss: 3.5521155350406963\n",
      "Epoch 4, Batch: 13000| Training Loss: 3.5502581579318413\n",
      "Epoch 4, Batch: 14000| Training Loss: 3.549131539446967\n",
      "Epoch 4, Batch: 15000| Training Loss: 3.546985395606359\n",
      "Epoch 4, Batch: 16000| Training Loss: 3.5453190937042236\n",
      "Epoch 4, Batch: 17000| Training Loss: 3.54384805041201\n",
      "Epoch 4, Batch: 18000| Training Loss: 3.543403020792537\n",
      "Epoch 4, Batch: 19000| Training Loss: 3.5418449915459282\n",
      "Epoch 4, Batch: 20000| Training Loss: 3.5415765585780146\n",
      "Epoch 4, Batch: 21000| Training Loss: 3.5403407636142914\n",
      "Epoch 4, Batch: 22000| Training Loss: 3.5382000290805644\n",
      "Epoch 4, Batch: 23000| Training Loss: 3.5361915889097295\n",
      "Epoch 4, Batch: 24000| Training Loss: 3.534894611765941\n",
      "Epoch 4, Batch: 25000| Training Loss: 3.5332614540863037\n",
      "Epoch 4, Training Loss: 3.532291978315276, Validation Error: 76.5335774271351, Validation Top-3 Accuracy: 41.03245866491625, Training Error: 75.4116222019662\n",
      "Epoch 5, Batch: 1000| Training Loss: 3.517629984140396\n",
      "Epoch 5, Batch: 2000| Training Loss: 3.5177531876564028\n",
      "Epoch 5, Batch: 3000| Training Loss: 3.5153843546708425\n",
      "Epoch 5, Batch: 4000| Training Loss: 3.515063734829426\n",
      "Epoch 5, Batch: 5000| Training Loss: 3.5062608085632325\n",
      "Epoch 5, Batch: 6000| Training Loss: 3.501996314605077\n",
      "Epoch 5, Batch: 7000| Training Loss: 3.501017062834331\n",
      "Epoch 5, Batch: 8000| Training Loss: 3.4979609921872616\n",
      "Epoch 5, Batch: 9000| Training Loss: 3.4973030326101515\n",
      "Epoch 5, Batch: 10000| Training Loss: 3.4956571652412416\n",
      "Epoch 5, Batch: 11000| Training Loss: 3.493895795063539\n",
      "Epoch 5, Batch: 12000| Training Loss: 3.4918190317749978\n",
      "Epoch 5, Batch: 13000| Training Loss: 3.4901419378060563\n",
      "Epoch 5, Batch: 14000| Training Loss: 3.4892871602433067\n",
      "Epoch 5, Batch: 15000| Training Loss: 3.4873255663553873\n",
      "Epoch 5, Batch: 16000| Training Loss: 3.486013016670942\n",
      "Epoch 5, Batch: 17000| Training Loss: 3.4847641283904807\n",
      "Epoch 5, Batch: 18000| Training Loss: 3.4844817790057925\n",
      "Epoch 5, Batch: 19000| Training Loss: 3.4828886357859563\n",
      "Epoch 5, Batch: 20000| Training Loss: 3.4828632608413694\n",
      "Epoch 5, Batch: 21000| Training Loss: 3.481906497989382\n",
      "Epoch 5, Batch: 22000| Training Loss: 3.4799705356142736\n",
      "Epoch 5, Batch: 23000| Training Loss: 3.478169713994731\n",
      "Epoch 5, Batch: 24000| Training Loss: 3.477219427893559\n",
      "Epoch 5, Batch: 25000| Training Loss: 3.4757094971179963\n",
      "Epoch 5, Training Loss: 3.4749398524419, Validation Error: 76.13547205997733, Validation Top-3 Accuracy: 41.677563712427364, Training Error: 74.69268274777778\n",
      "Epoch 6, Batch: 1000| Training Loss: 3.4726063299179075\n",
      "Epoch 6, Batch: 2000| Training Loss: 3.473359079003334\n",
      "Epoch 6, Batch: 3000| Training Loss: 3.4714119470914206\n",
      "Epoch 6, Batch: 4000| Training Loss: 3.4713863302469252\n",
      "Epoch 6, Batch: 5000| Training Loss: 3.463516700553894\n",
      "Epoch 6, Batch: 6000| Training Loss: 3.459270151456197\n",
      "Epoch 6, Batch: 7000| Training Loss: 3.4584004892621723\n",
      "Epoch 6, Batch: 8000| Training Loss: 3.4560113179385663\n",
      "Epoch 6, Batch: 9000| Training Loss: 3.455197237359153\n",
      "Epoch 6, Batch: 10000| Training Loss: 3.453780189371109\n",
      "Epoch 6, Batch: 11000| Training Loss: 3.4522897460027173\n",
      "Epoch 6, Batch: 12000| Training Loss: 3.4502814999421436\n",
      "Epoch 6, Batch: 13000| Training Loss: 3.448795547907169\n",
      "Epoch 6, Batch: 14000| Training Loss: 3.448191969718252\n",
      "Epoch 6, Batch: 15000| Training Loss: 3.4464158906141917\n",
      "Epoch 6, Batch: 16000| Training Loss: 3.4451074860095976\n",
      "Epoch 6, Batch: 17000| Training Loss: 3.4439337359456457\n",
      "Epoch 6, Batch: 18000| Training Loss: 3.4437715561257467\n",
      "Epoch 6, Batch: 19000| Training Loss: 3.4422923128102956\n",
      "Epoch 6, Batch: 20000| Training Loss: 3.4424135036826136\n",
      "Epoch 6, Batch: 21000| Training Loss: 3.441441750265303\n",
      "Epoch 6, Batch: 22000| Training Loss: 3.439615019700744\n",
      "Epoch 6, Batch: 23000| Training Loss: 3.4379916783104774\n",
      "Epoch 6, Batch: 24000| Training Loss: 3.437086973865827\n",
      "Epoch 6, Batch: 25000| Training Loss: 3.435735661802292\n",
      "Epoch 6, Training Loss: 3.4349613249063884, Validation Error: 75.8419783221457, Validation Top-3 Accuracy: 42.011739750552486, Training Error: 74.16864978619014\n",
      "Epoch 7, Batch: 1000| Training Loss: 3.442293680429459\n",
      "Epoch 7, Batch: 2000| Training Loss: 3.44220216691494\n",
      "Epoch 7, Batch: 3000| Training Loss: 3.4415765504042306\n",
      "Epoch 7, Batch: 4000| Training Loss: 3.442252022445202\n",
      "Epoch 7, Batch: 5000| Training Loss: 3.434340809059143\n",
      "Epoch 7, Batch: 6000| Training Loss: 3.4302933348417284\n",
      "Epoch 7, Batch: 7000| Training Loss: 3.42956912619727\n",
      "Epoch 7, Batch: 8000| Training Loss: 3.4271065486073495\n",
      "Epoch 7, Batch: 9000| Training Loss: 3.426807271454069\n",
      "Epoch 7, Batch: 10000| Training Loss: 3.425457600593567\n",
      "Epoch 7, Batch: 11000| Training Loss: 3.4239689275568184\n",
      "Epoch 7, Batch: 12000| Training Loss: 3.422099128405253\n",
      "Epoch 7, Batch: 13000| Training Loss: 3.420683603653541\n",
      "Epoch 7, Batch: 14000| Training Loss: 3.4199811770745687\n",
      "Epoch 7, Batch: 15000| Training Loss: 3.4183773105462394\n",
      "Epoch 7, Batch: 16000| Training Loss: 3.4171176779717207\n",
      "Epoch 7, Batch: 17000| Training Loss: 3.416188703214421\n",
      "Epoch 7, Batch: 18000| Training Loss: 3.4161144553290472\n",
      "Epoch 7, Batch: 19000| Training Loss: 3.4146325259459647\n",
      "Epoch 7, Batch: 20000| Training Loss: 3.4147013130307196\n",
      "Epoch 7, Batch: 21000| Training Loss: 3.413843163126991\n",
      "Epoch 7, Batch: 22000| Training Loss: 3.411919428077611\n",
      "Epoch 7, Batch: 23000| Training Loss: 3.41032318450057\n",
      "Epoch 7, Batch: 24000| Training Loss: 3.4094712291459244\n",
      "Epoch 7, Batch: 25000| Training Loss: 3.4082292511844634\n",
      "Epoch 7, Training Loss: 3.407518117012073, Validation Error: 75.62694330630866, Validation Top-3 Accuracy: 42.22386888779713, Training Error: 73.82918458611428\n",
      "Epoch 8, Batch: 1000| Training Loss: 3.4215732989311216\n",
      "Epoch 8, Batch: 2000| Training Loss: 3.421617731332779\n",
      "Epoch 8, Batch: 3000| Training Loss: 3.4212666828632354\n",
      "Epoch 8, Batch: 4000| Training Loss: 3.4213512721657753\n",
      "Epoch 8, Batch: 5000| Training Loss: 3.413719646835327\n",
      "Epoch 8, Batch: 6000| Training Loss: 3.409808665593465\n",
      "Epoch 8, Batch: 7000| Training Loss: 3.409195627587182\n",
      "Epoch 8, Batch: 8000| Training Loss: 3.4071765629649162\n",
      "Epoch 8, Batch: 9000| Training Loss: 3.406905236509111\n",
      "Epoch 8, Batch: 10000| Training Loss: 3.405689662194252\n",
      "Epoch 8, Batch: 11000| Training Loss: 3.404175712672147\n",
      "Epoch 8, Batch: 12000| Training Loss: 3.4023540091117224\n",
      "Epoch 8, Batch: 13000| Training Loss: 3.4009400651821724\n",
      "Epoch 8, Batch: 14000| Training Loss: 3.400346817748887\n",
      "Epoch 8, Batch: 15000| Training Loss: 3.3987755682150524\n",
      "Epoch 8, Batch: 16000| Training Loss: 3.3974950553476813\n",
      "Epoch 8, Batch: 17000| Training Loss: 3.3964245097356685\n",
      "Epoch 8, Batch: 18000| Training Loss: 3.396362799697452\n",
      "Epoch 8, Batch: 19000| Training Loss: 3.3949794800783457\n",
      "Epoch 8, Batch: 20000| Training Loss: 3.39525477848053\n",
      "Epoch 8, Batch: 21000| Training Loss: 3.3943885315940494\n",
      "Epoch 8, Batch: 22000| Training Loss: 3.392594867240299\n",
      "Epoch 8, Batch: 23000| Training Loss: 3.390978767540144\n",
      "Epoch 8, Batch: 24000| Training Loss: 3.3901153155863284\n",
      "Epoch 8, Batch: 25000| Training Loss: 3.388753599538803\n",
      "Epoch 8, Training Loss: 3.388042752823603, Validation Error: 75.5760904309418, Validation Top-3 Accuracy: 42.311045245568906, Training Error: 73.56178462404336\n",
      "Epoch 9, Batch: 1000| Training Loss: 3.408994202852249\n",
      "Epoch 9, Batch: 2000| Training Loss: 3.40791131067276\n",
      "Epoch 9, Batch: 3000| Training Loss: 3.4065129261811573\n",
      "Epoch 9, Batch: 4000| Training Loss: 3.4067017473578454\n",
      "Epoch 9, Batch: 5000| Training Loss: 3.3984888742923736\n",
      "Epoch 9, Batch: 6000| Training Loss: 3.3944783895015718\n",
      "Epoch 9, Batch: 7000| Training Loss: 3.39412956738472\n",
      "Epoch 9, Batch: 8000| Training Loss: 3.3919238570332526\n",
      "Epoch 9, Batch: 9000| Training Loss: 3.3917762802706823\n",
      "Epoch 9, Batch: 10000| Training Loss: 3.390747598195076\n",
      "Epoch 9, Batch: 11000| Training Loss: 3.389080206480893\n",
      "Epoch 9, Batch: 12000| Training Loss: 3.3872965266108515\n",
      "Epoch 9, Batch: 13000| Training Loss: 3.385928525539545\n",
      "Epoch 9, Batch: 14000| Training Loss: 3.3852870017290115\n",
      "Epoch 9, Batch: 15000| Training Loss: 3.3839014611562095\n",
      "Epoch 9, Batch: 16000| Training Loss: 3.382702288046479\n",
      "Epoch 9, Batch: 17000| Training Loss: 3.3817497550599716\n",
      "Epoch 9, Batch: 18000| Training Loss: 3.3817174499697154\n",
      "Epoch 9, Batch: 19000| Training Loss: 3.380270288668181\n",
      "Epoch 9, Batch: 20000| Training Loss: 3.380416530811787\n",
      "Epoch 9, Batch: 21000| Training Loss: 3.3794622064885638\n",
      "Epoch 9, Batch: 22000| Training Loss: 3.3777153286825525\n",
      "Epoch 9, Batch: 23000| Training Loss: 3.3762139455028204\n",
      "Epoch 9, Batch: 24000| Training Loss: 3.37531040156881\n",
      "Epoch 9, Batch: 25000| Training Loss: 3.3739530810022353\n",
      "Epoch 9, Training Loss: 3.373330732251976, Validation Error: 75.45694940865371, Validation Top-3 Accuracy: 42.37933339249013, Training Error: 73.37152750163645\n",
      "Epoch 10, Batch: 1000| Training Loss: 3.397297998428345\n",
      "Epoch 10, Batch: 2000| Training Loss: 3.3972192482948302\n",
      "Epoch 10, Batch: 3000| Training Loss: 3.395571287870407\n",
      "Epoch 10, Batch: 4000| Training Loss: 3.395730437397957\n",
      "Epoch 10, Batch: 5000| Training Loss: 3.3882103073596954\n",
      "Epoch 10, Batch: 6000| Training Loss: 3.3838876801729203\n",
      "Epoch 10, Batch: 7000| Training Loss: 3.383531534092767\n",
      "Epoch 10, Batch: 8000| Training Loss: 3.3813681329786776\n",
      "Epoch 10, Batch: 9000| Training Loss: 3.38104017774264\n",
      "Epoch 10, Batch: 10000| Training Loss: 3.379854340219498\n",
      "Epoch 10, Batch: 11000| Training Loss: 3.3787184387770566\n",
      "Epoch 10, Batch: 12000| Training Loss: 3.3769789523681006\n",
      "Epoch 10, Batch: 13000| Training Loss: 3.375473934687101\n",
      "Epoch 10, Batch: 14000| Training Loss: 3.3749531873464584\n",
      "Epoch 10, Batch: 15000| Training Loss: 3.3733725954532625\n",
      "Epoch 10, Batch: 16000| Training Loss: 3.3721381806582214\n",
      "Epoch 10, Batch: 17000| Training Loss: 3.3711427779197694\n",
      "Epoch 10, Batch: 18000| Training Loss: 3.371185441109869\n",
      "Epoch 10, Batch: 19000| Training Loss: 3.3696875759802367\n",
      "Epoch 10, Batch: 20000| Training Loss: 3.3699604096531868\n",
      "Epoch 10, Batch: 21000| Training Loss: 3.3690897306714738\n",
      "Epoch 10, Batch: 22000| Training Loss: 3.3673138811479917\n",
      "Epoch 10, Batch: 23000| Training Loss: 3.365693357353625\n",
      "Epoch 10, Batch: 24000| Training Loss: 3.3648139708240827\n",
      "Epoch 10, Batch: 25000| Training Loss: 3.363553768749237\n",
      "Epoch 10, Training Loss: 3.3628830659194406, Validation Error: 75.38284950454769, Validation Top-3 Accuracy: 42.478133264631474, Training Error: 73.25125563583197\n",
      "Epoch 11, Batch: 1000| Training Loss: 3.3887377305030824\n",
      "Epoch 11, Batch: 2000| Training Loss: 3.388735743999481\n",
      "Epoch 11, Batch: 3000| Training Loss: 3.389076589822769\n",
      "Epoch 11, Batch: 4000| Training Loss: 3.3891844986081123\n",
      "Epoch 11, Batch: 5000| Training Loss: 3.381442674255371\n",
      "Epoch 11, Batch: 6000| Training Loss: 3.3772636825243634\n",
      "Epoch 11, Batch: 7000| Training Loss: 3.3765286323343005\n",
      "Epoch 11, Batch: 8000| Training Loss: 3.3742678523957728\n",
      "Epoch 11, Batch: 9000| Training Loss: 3.374108341985279\n",
      "Epoch 11, Batch: 10000| Training Loss: 3.3728200422525405\n",
      "Epoch 11, Batch: 11000| Training Loss: 3.3712601325078446\n",
      "Epoch 11, Batch: 12000| Training Loss: 3.3696377818783123\n",
      "Epoch 11, Batch: 13000| Training Loss: 3.3683720826002266\n",
      "Epoch 11, Batch: 14000| Training Loss: 3.367983376945768\n",
      "Epoch 11, Batch: 15000| Training Loss: 3.366485020494461\n",
      "Epoch 11, Batch: 16000| Training Loss: 3.3653195447325706\n",
      "Epoch 11, Batch: 17000| Training Loss: 3.364431396189858\n",
      "Epoch 11, Batch: 18000| Training Loss: 3.364415770928065\n",
      "Epoch 11, Batch: 19000| Training Loss: 3.363148084464826\n",
      "Epoch 11, Batch: 20000| Training Loss: 3.3633473042964934\n",
      "Epoch 11, Batch: 21000| Training Loss: 3.3624182932603928\n",
      "Epoch 11, Batch: 22000| Training Loss: 3.3605946048606525\n",
      "Epoch 11, Batch: 23000| Training Loss: 3.359043776512146\n",
      "Epoch 11, Batch: 24000| Training Loss: 3.358249874462684\n",
      "Epoch 11, Batch: 25000| Training Loss: 3.3568964149045946\n",
      "Epoch 11, Training Loss: 3.3561971249805156, Validation Error: 75.35379071862377, Validation Top-3 Accuracy: 42.54932729014509, Training Error: 73.1681787315784\n",
      "Epoch 12, Batch: 1000| Training Loss: 3.3831208815574647\n",
      "Epoch 12, Batch: 2000| Training Loss: 3.384091275691986\n",
      "Epoch 12, Batch: 3000| Training Loss: 3.383263659397761\n",
      "Epoch 12, Batch: 4000| Training Loss: 3.383315740764141\n",
      "Epoch 12, Batch: 5000| Training Loss: 3.3751510875701904\n",
      "Epoch 12, Batch: 6000| Training Loss: 3.3715927408138913\n",
      "Epoch 12, Batch: 7000| Training Loss: 3.370921217679977\n",
      "Epoch 12, Batch: 8000| Training Loss: 3.368830232411623\n",
      "Epoch 12, Batch: 9000| Training Loss: 3.3687903679476845\n",
      "Epoch 12, Batch: 10000| Training Loss: 3.367481933808327\n",
      "Epoch 12, Batch: 11000| Training Loss: 3.3658280475139617\n",
      "Epoch 12, Batch: 12000| Training Loss: 3.364061447342237\n",
      "Epoch 12, Batch: 13000| Training Loss: 3.362705013696964\n",
      "Epoch 12, Batch: 14000| Training Loss: 3.362262967858996\n",
      "Epoch 12, Batch: 15000| Training Loss: 3.3608466438452402\n",
      "Epoch 12, Batch: 16000| Training Loss: 3.359651110813022\n",
      "Epoch 12, Batch: 17000| Training Loss: 3.358673517661936\n",
      "Epoch 12, Batch: 18000| Training Loss: 3.3585942578580643\n",
      "Epoch 12, Batch: 19000| Training Loss: 3.3572682759887296\n",
      "Epoch 12, Batch: 20000| Training Loss: 3.3575527969837187\n",
      "Epoch 12, Batch: 21000| Training Loss: 3.356715665715081\n",
      "Epoch 12, Batch: 22000| Training Loss: 3.354950495275584\n",
      "Epoch 12, Batch: 23000| Training Loss: 3.353455019816108\n",
      "Epoch 12, Batch: 24000| Training Loss: 3.3525975790917872\n",
      "Epoch 12, Batch: 25000| Training Loss: 3.3512717881202696\n",
      "Epoch 12, Training Loss: 3.350562766718235, Validation Error: 75.35669659721617, Validation Top-3 Accuracy: 42.587103710633755, Training Error: 73.08730416057456\n",
      "Epoch 13, Batch: 1000| Training Loss: 3.381025858402252\n",
      "Epoch 13, Batch: 2000| Training Loss: 3.3811211092472075\n",
      "Epoch 13, Batch: 3000| Training Loss: 3.3799030640125274\n",
      "Epoch 13, Batch: 4000| Training Loss: 3.380045457303524\n",
      "Epoch 13, Batch: 5000| Training Loss: 3.372190407037735\n",
      "Epoch 13, Batch: 6000| Training Loss: 3.3683611756165823\n",
      "Epoch 13, Batch: 7000| Training Loss: 3.3677867123399463\n",
      "Epoch 13, Batch: 8000| Training Loss: 3.365658379405737\n",
      "Epoch 13, Batch: 9000| Training Loss: 3.3654721569220225\n",
      "Epoch 13, Batch: 10000| Training Loss: 3.3643511156082155\n",
      "Epoch 13, Batch: 11000| Training Loss: 3.3629473804994063\n",
      "Epoch 13, Batch: 12000| Training Loss: 3.3612729408740996\n",
      "Epoch 13, Batch: 13000| Training Loss: 3.360043887120027\n",
      "Epoch 13, Batch: 14000| Training Loss: 3.3597895707062313\n",
      "Epoch 13, Batch: 15000| Training Loss: 3.3584105151494343\n",
      "Epoch 13, Batch: 16000| Training Loss: 3.357183927997947\n",
      "Epoch 13, Batch: 17000| Training Loss: 3.356361785355736\n",
      "Epoch 13, Batch: 18000| Training Loss: 3.3562516029013527\n",
      "Epoch 13, Batch: 19000| Training Loss: 3.354956307850386\n",
      "Epoch 13, Batch: 20000| Training Loss: 3.3552823310017588\n",
      "Epoch 13, Batch: 21000| Training Loss: 3.354502017055239\n",
      "Epoch 13, Batch: 22000| Training Loss: 3.3527899533293466\n",
      "Epoch 13, Batch: 23000| Training Loss: 3.3512710345724352\n",
      "Epoch 13, Batch: 24000| Training Loss: 3.350583932816982\n",
      "Epoch 13, Batch: 25000| Training Loss: 3.3493239521837235\n",
      "Epoch 13, Training Loss: 3.348713709099087, Validation Error: 75.36250835440096, Validation Top-3 Accuracy: 42.59727428570713, Training Error: 73.05671619877282\n",
      "Epoch 14, Batch: 1000| Training Loss: 3.37526766371727\n",
      "Epoch 14, Batch: 2000| Training Loss: 3.375697869181633\n",
      "Epoch 14, Batch: 3000| Training Loss: 3.375999509334564\n",
      "Epoch 14, Batch: 4000| Training Loss: 3.376389667391777\n",
      "Epoch 14, Batch: 5000| Training Loss: 3.3684628628730775\n",
      "Epoch 14, Batch: 6000| Training Loss: 3.3647863724629086\n",
      "Epoch 14, Batch: 7000| Training Loss: 3.364827629021236\n",
      "Epoch 14, Batch: 8000| Training Loss: 3.362844745606184\n",
      "Epoch 14, Batch: 9000| Training Loss: 3.3627584929731156\n",
      "Epoch 14, Batch: 10000| Training Loss: 3.3619132745742797\n",
      "Epoch 14, Batch: 11000| Training Loss: 3.3603543684482573\n",
      "Epoch 14, Batch: 12000| Training Loss: 3.3586232769886655\n",
      "Epoch 14, Batch: 13000| Training Loss: 3.3574712084440086\n",
      "Epoch 14, Batch: 14000| Training Loss: 3.357166506120137\n",
      "Epoch 14, Batch: 15000| Training Loss: 3.35573506360054\n",
      "Epoch 14, Batch: 16000| Training Loss: 3.3544257269948723\n",
      "Epoch 14, Batch: 17000| Training Loss: 3.353509646962671\n",
      "Epoch 14, Batch: 18000| Training Loss: 3.3537049085034263\n",
      "Epoch 14, Batch: 19000| Training Loss: 3.3523334769449735\n",
      "Epoch 14, Batch: 20000| Training Loss: 3.3526666329860686\n",
      "Epoch 14, Batch: 21000| Training Loss: 3.3518667406014035\n",
      "Epoch 14, Batch: 22000| Training Loss: 3.350244588288394\n",
      "Epoch 14, Batch: 23000| Training Loss: 3.3488445067820343\n",
      "Epoch 14, Batch: 24000| Training Loss: 3.348162063578765\n",
      "Epoch 14, Batch: 25000| Training Loss: 3.3469600102949144\n",
      "Epoch 14, Training Loss: 3.3463013707055955, Validation Error: 75.33780838636562, Validation Top-3 Accuracy: 42.59872722500333, Training Error: 73.03328582003266\n",
      "Epoch 15, Batch: 1000| Training Loss: 3.374075851917267\n",
      "Epoch 15, Batch: 2000| Training Loss: 3.374687724709511\n",
      "Epoch 15, Batch: 3000| Training Loss: 3.3740690233707427\n",
      "Epoch 15, Batch: 4000| Training Loss: 3.374221047818661\n",
      "Epoch 15, Batch: 5000| Training Loss: 3.366527789592743\n",
      "Epoch 15, Batch: 6000| Training Loss: 3.362908080736796\n",
      "Epoch 15, Batch: 7000| Training Loss: 3.362713510274887\n",
      "Epoch 15, Batch: 8000| Training Loss: 3.3607041574269534\n",
      "Epoch 15, Batch: 9000| Training Loss: 3.360605512181918\n",
      "Epoch 15, Batch: 10000| Training Loss: 3.359812222659588\n",
      "Epoch 15, Batch: 11000| Training Loss: 3.358326220089739\n",
      "Epoch 15, Batch: 12000| Training Loss: 3.356710547953844\n",
      "Epoch 15, Batch: 13000| Training Loss: 3.3555075182456235\n",
      "Epoch 15, Batch: 14000| Training Loss: 3.3550787592019353\n",
      "Epoch 15, Batch: 15000| Training Loss: 3.353758081698418\n",
      "Epoch 15, Batch: 16000| Training Loss: 3.3526778761073945\n",
      "Epoch 15, Batch: 17000| Training Loss: 3.3517070794456147\n",
      "Epoch 15, Batch: 18000| Training Loss: 3.351630159980721\n",
      "Epoch 15, Batch: 19000| Training Loss: 3.3503398658790084\n",
      "Epoch 15, Batch: 20000| Training Loss: 3.350770119982958\n",
      "Epoch 15, Batch: 21000| Training Loss: 3.350011229123388\n",
      "Epoch 15, Batch: 22000| Training Loss: 3.348293217285113\n",
      "Epoch 15, Batch: 23000| Training Loss: 3.346860416552295\n",
      "Epoch 15, Batch: 24000| Training Loss: 3.346181789423029\n",
      "Epoch 15, Batch: 25000| Training Loss: 3.3449957143878937\n",
      "Epoch 15, Training Loss: 3.344373699091827, Validation Error: 75.34216720425421, Validation Top-3 Accuracy: 42.6103507393729, Training Error: 73.02178474639521\n",
      "Epoch 16, Batch: 1000| Training Loss: 3.3748588042259215\n",
      "Epoch 16, Batch: 2000| Training Loss: 3.373599182009697\n",
      "Epoch 16, Batch: 3000| Training Loss: 3.3728229121367135\n",
      "Epoch 16, Batch: 4000| Training Loss: 3.3731586323976517\n",
      "Epoch 16, Batch: 5000| Training Loss: 3.3650453073978426\n",
      "Epoch 16, Batch: 6000| Training Loss: 3.36089866288503\n",
      "Epoch 16, Batch: 7000| Training Loss: 3.3604016997814177\n",
      "Epoch 16, Batch: 8000| Training Loss: 3.358182969123125\n",
      "Epoch 16, Batch: 9000| Training Loss: 3.3580623973740473\n",
      "Epoch 16, Batch: 10000| Training Loss: 3.3571331023454665\n",
      "Epoch 16, Batch: 11000| Training Loss: 3.3558359832980416\n",
      "Epoch 16, Batch: 12000| Training Loss: 3.354067856232325\n",
      "Epoch 16, Batch: 13000| Training Loss: 3.352946065407533\n",
      "Epoch 16, Batch: 14000| Training Loss: 3.3526643975802832\n",
      "Epoch 16, Batch: 15000| Training Loss: 3.3515318745454152\n",
      "Epoch 16, Batch: 16000| Training Loss: 3.35049979904294\n",
      "Epoch 16, Batch: 17000| Training Loss: 3.3497240129779366\n",
      "Epoch 16, Batch: 18000| Training Loss: 3.349883293178346\n",
      "Epoch 16, Batch: 19000| Training Loss: 3.3487386885944166\n",
      "Epoch 16, Batch: 20000| Training Loss: 3.349140863955021\n",
      "Epoch 16, Batch: 21000| Training Loss: 3.348401772362845\n",
      "Epoch 16, Batch: 22000| Training Loss: 3.3468192145065827\n",
      "Epoch 16, Batch: 23000| Training Loss: 3.3454343024544095\n",
      "Epoch 16, Batch: 24000| Training Loss: 3.344766669978698\n",
      "Epoch 16, Batch: 25000| Training Loss: 3.3435403653955458\n",
      "Epoch 16, Training Loss: 3.342948313238626, Validation Error: 75.32182605410746, Validation Top-3 Accuracy: 42.608897800076704, Training Error: 72.98006276649761\n",
      "Epoch 17, Batch: 1000| Training Loss: 3.3692818377017977\n",
      "Epoch 17, Batch: 2000| Training Loss: 3.369740189552307\n",
      "Epoch 17, Batch: 3000| Training Loss: 3.369296759525935\n",
      "Epoch 17, Batch: 4000| Training Loss: 3.3701555582880975\n",
      "Epoch 17, Batch: 5000| Training Loss: 3.3623758316516876\n",
      "Epoch 17, Batch: 6000| Training Loss: 3.3587157630523046\n",
      "Epoch 17, Batch: 7000| Training Loss: 3.3586198806762697\n",
      "Epoch 17, Batch: 8000| Training Loss: 3.356572747603059\n",
      "Epoch 17, Batch: 9000| Training Loss: 3.356512700200081\n",
      "Epoch 17, Batch: 10000| Training Loss: 3.355494879567623\n",
      "Epoch 17, Batch: 11000| Training Loss: 3.354194337552244\n",
      "Epoch 17, Batch: 12000| Training Loss: 3.352588744411866\n",
      "Epoch 17, Batch: 13000| Training Loss: 3.351307874578696\n",
      "Epoch 17, Batch: 14000| Training Loss: 3.3509938438705036\n",
      "Epoch 17, Batch: 15000| Training Loss: 3.349728453310331\n",
      "Epoch 17, Batch: 16000| Training Loss: 3.348693676061928\n",
      "Epoch 17, Batch: 17000| Training Loss: 3.3478699387031443\n",
      "Epoch 17, Batch: 18000| Training Loss: 3.3479002255797385\n",
      "Epoch 17, Batch: 19000| Training Loss: 3.3467469760806936\n",
      "Epoch 17, Batch: 20000| Training Loss: 3.3472595305621624\n",
      "Epoch 17, Batch: 21000| Training Loss: 3.3465878517343883\n",
      "Epoch 17, Batch: 22000| Training Loss: 3.3450283091122452\n",
      "Epoch 17, Batch: 23000| Training Loss: 3.343652666034906\n",
      "Epoch 17, Batch: 24000| Training Loss: 3.3431112132718166\n",
      "Epoch 17, Batch: 25000| Training Loss: 3.341934199166298\n",
      "Epoch 17, Training Loss: 3.341332265465609, Validation Error: 75.34071426495801, Validation Top-3 Accuracy: 42.61761543585388, Training Error: 72.97492398891492\n",
      "Epoch 18, Batch: 1000| Training Loss: 3.367490643262863\n",
      "Epoch 18, Batch: 2000| Training Loss: 3.367441787600517\n",
      "Epoch 18, Batch: 3000| Training Loss: 3.3668267946243287\n",
      "Epoch 18, Batch: 4000| Training Loss: 3.3673426113128664\n",
      "Epoch 18, Batch: 5000| Training Loss: 3.359833994150162\n",
      "Epoch 18, Batch: 6000| Training Loss: 3.3562259187698364\n",
      "Epoch 18, Batch: 7000| Training Loss: 3.35622301871436\n",
      "Epoch 18, Batch: 8000| Training Loss: 3.3542324379086494\n",
      "Epoch 18, Batch: 9000| Training Loss: 3.3543637359407215\n",
      "Epoch 18, Batch: 10000| Training Loss: 3.3533730350494384\n",
      "Epoch 18, Batch: 11000| Training Loss: 3.3520542582815342\n",
      "Epoch 18, Batch: 12000| Training Loss: 3.350327890455723\n",
      "Epoch 18, Batch: 13000| Training Loss: 3.349317480912575\n",
      "Epoch 18, Batch: 14000| Training Loss: 3.3489253198249\n",
      "Epoch 18, Batch: 15000| Training Loss: 3.347670682954788\n",
      "Epoch 18, Batch: 16000| Training Loss: 3.346680754393339\n",
      "Epoch 18, Batch: 17000| Training Loss: 3.345935650657205\n",
      "Epoch 18, Batch: 18000| Training Loss: 3.3459927553335826\n",
      "Epoch 18, Batch: 19000| Training Loss: 3.3447212666712307\n",
      "Epoch 18, Batch: 20000| Training Loss: 3.34512256115675\n",
      "Epoch 18, Batch: 21000| Training Loss: 3.3445279138882955\n",
      "Epoch 18, Batch: 22000| Training Loss: 3.342890842687\n",
      "Epoch 18, Batch: 23000| Training Loss: 3.341508288362752\n",
      "Epoch 18, Batch: 24000| Training Loss: 3.340936884144942\n",
      "Epoch 18, Batch: 25000| Training Loss: 3.3398419702911375\n",
      "Epoch 18, Training Loss: 3.339178025036427, Validation Error: 75.30293784325691, Validation Top-3 Accuracy: 42.63214482881584, Training Error: 72.9328349534757\n",
      "Epoch 19, Batch: 1000| Training Loss: 3.3657438688278196\n",
      "Epoch 19, Batch: 2000| Training Loss: 3.3661524044275284\n",
      "Epoch 19, Batch: 3000| Training Loss: 3.366019217411677\n",
      "Epoch 19, Batch: 4000| Training Loss: 3.3660936080813406\n",
      "Epoch 19, Batch: 5000| Training Loss: 3.3583882362365722\n",
      "Epoch 19, Batch: 6000| Training Loss: 3.354450847864151\n",
      "Epoch 19, Batch: 7000| Training Loss: 3.35444262439864\n",
      "Epoch 19, Batch: 8000| Training Loss: 3.3523413923233747\n",
      "Epoch 19, Batch: 9000| Training Loss: 3.3523877545860077\n",
      "Epoch 19, Batch: 10000| Training Loss: 3.351457864487171\n",
      "Epoch 19, Batch: 11000| Training Loss: 3.3502397728941657\n",
      "Epoch 19, Batch: 12000| Training Loss: 3.348512713720401\n",
      "Epoch 19, Batch: 13000| Training Loss: 3.347240898508292\n",
      "Epoch 19, Batch: 14000| Training Loss: 3.3468975140963284\n",
      "Epoch 19, Batch: 15000| Training Loss: 3.3458279307126997\n",
      "Epoch 19, Batch: 16000| Training Loss: 3.344846097804606\n",
      "Epoch 19, Batch: 17000| Training Loss: 3.344070733568248\n",
      "Epoch 19, Batch: 18000| Training Loss: 3.344237194253339\n",
      "Epoch 19, Batch: 19000| Training Loss: 3.343039619125818\n",
      "Epoch 19, Batch: 20000| Training Loss: 3.343540402907133\n",
      "Epoch 19, Batch: 21000| Training Loss: 3.342944347841399\n",
      "Epoch 19, Batch: 22000| Training Loss: 3.3414137958667496\n",
      "Epoch 19, Batch: 23000| Training Loss: 3.340115547257921\n",
      "Epoch 19, Batch: 24000| Training Loss: 3.3395506137659154\n",
      "Epoch 19, Batch: 25000| Training Loss: 3.338387453627586\n",
      "Epoch 19, Training Loss: 3.337831500968137, Validation Error: 75.32037311481126, Validation Top-3 Accuracy: 42.61180367866909, Training Error: 72.91650098187358\n",
      "Epoch 20, Batch: 1000| Training Loss: 3.363843069076538\n",
      "Epoch 20, Batch: 2000| Training Loss: 3.3637422263622283\n",
      "Epoch 20, Batch: 3000| Training Loss: 3.363807341893514\n",
      "Epoch 20, Batch: 4000| Training Loss: 3.3643797880411146\n",
      "Epoch 20, Batch: 5000| Training Loss: 3.3564297893047335\n",
      "Epoch 20, Batch: 6000| Training Loss: 3.352929673989614\n",
      "Epoch 20, Batch: 7000| Training Loss: 3.3527081493309567\n",
      "Epoch 20, Batch: 8000| Training Loss: 3.3505070393681526\n",
      "Epoch 20, Batch: 9000| Training Loss: 3.3503929217126633\n",
      "Epoch 20, Batch: 10000| Training Loss: 3.3493347730636596\n",
      "Epoch 20, Batch: 11000| Training Loss: 3.3481045847372575\n",
      "Epoch 20, Batch: 12000| Training Loss: 3.3465929047465326\n",
      "Epoch 20, Batch: 13000| Training Loss: 3.3456746076620543\n",
      "Epoch 20, Batch: 14000| Training Loss: 3.3456218091079166\n",
      "Epoch 20, Batch: 15000| Training Loss: 3.344279132223129\n",
      "Epoch 20, Batch: 16000| Training Loss: 3.3432713390886786\n",
      "Epoch 20, Batch: 17000| Training Loss: 3.3425231293650235\n",
      "Epoch 20, Batch: 18000| Training Loss: 3.3426539042923187\n",
      "Epoch 20, Batch: 19000| Training Loss: 3.341572943122763\n",
      "Epoch 20, Batch: 20000| Training Loss: 3.3420216386198995\n",
      "Epoch 20, Batch: 21000| Training Loss: 3.3414078594843546\n",
      "Epoch 20, Batch: 22000| Training Loss: 3.3399190464019775\n",
      "Epoch 20, Batch: 23000| Training Loss: 3.338580493087354\n",
      "Epoch 20, Batch: 24000| Training Loss: 3.337995784352223\n",
      "Epoch 20, Batch: 25000| Training Loss: 3.336877641839981\n",
      "Epoch 20, Training Loss: 3.3362995041031853, Validation Error: 75.27533199662918, Validation Top-3 Accuracy: 42.616162496557685, Training Error: 72.88884946440479\n",
      "Epoch 21, Batch: 1000| Training Loss: 3.361098330974579\n",
      "Epoch 21, Batch: 2000| Training Loss: 3.3618108103275297\n",
      "Epoch 21, Batch: 3000| Training Loss: 3.3629358604749044\n",
      "Epoch 21, Batch: 4000| Training Loss: 3.3634327482581137\n",
      "Epoch 21, Batch: 5000| Training Loss: 3.3554605823516845\n",
      "Epoch 21, Batch: 6000| Training Loss: 3.351652542591095\n",
      "Epoch 21, Batch: 7000| Training Loss: 3.3511290192944663\n",
      "Epoch 21, Batch: 8000| Training Loss: 3.3490827033519746\n",
      "Epoch 21, Batch: 9000| Training Loss: 3.349202705833647\n",
      "Epoch 21, Batch: 10000| Training Loss: 3.3482404400110246\n",
      "Epoch 21, Batch: 11000| Training Loss: 3.3469181464151903\n",
      "Epoch 21, Batch: 12000| Training Loss: 3.3452001730799674\n",
      "Epoch 21, Batch: 13000| Training Loss: 3.3441078253342553\n",
      "Epoch 21, Batch: 14000| Training Loss: 3.3438701756170817\n",
      "Epoch 21, Batch: 15000| Training Loss: 3.342734320116043\n",
      "Epoch 21, Batch: 16000| Training Loss: 3.341670429676771\n",
      "Epoch 21, Batch: 17000| Training Loss: 3.340854621999404\n",
      "Epoch 21, Batch: 18000| Training Loss: 3.341057626114951\n",
      "Epoch 21, Batch: 19000| Training Loss: 3.340034809438806\n",
      "Epoch 21, Batch: 20000| Training Loss: 3.3404897574424743\n",
      "Epoch 21, Batch: 21000| Training Loss: 3.33991648040499\n",
      "Epoch 21, Batch: 22000| Training Loss: 3.338391159361059\n",
      "Epoch 21, Batch: 23000| Training Loss: 3.3371013576569766\n",
      "Epoch 21, Batch: 24000| Training Loss: 3.3365527774194876\n",
      "Epoch 21, Batch: 25000| Training Loss: 3.3353702292060854\n",
      "Epoch 21, Training Loss: 3.3348044862426836, Validation Error: 75.28550257170255, Validation Top-3 Accuracy: 42.59146252852235, Training Error: 72.875084881594\n",
      "Epoch 22, Batch: 1000| Training Loss: 3.358034113407135\n",
      "Epoch 22, Batch: 2000| Training Loss: 3.359350085258484\n",
      "Epoch 22, Batch: 3000| Training Loss: 3.358883742252986\n",
      "Epoch 22, Batch: 4000| Training Loss: 3.359805571615696\n",
      "Epoch 22, Batch: 5000| Training Loss: 3.3526419744491576\n",
      "Epoch 22, Batch: 6000| Training Loss: 3.3490876307487487\n",
      "Epoch 22, Batch: 7000| Training Loss: 3.3489297625337326\n",
      "Epoch 22, Batch: 8000| Training Loss: 3.346879396393895\n",
      "Epoch 22, Batch: 9000| Training Loss: 3.346864468826188\n",
      "Epoch 22, Batch: 10000| Training Loss: 3.3462114741683004\n",
      "Epoch 22, Batch: 11000| Training Loss: 3.345043644677509\n",
      "Epoch 22, Batch: 12000| Training Loss: 3.3434110259115695\n",
      "Epoch 22, Batch: 13000| Training Loss: 3.342391834451602\n",
      "Epoch 22, Batch: 14000| Training Loss: 3.3422275110908917\n",
      "Epoch 22, Batch: 15000| Training Loss: 3.34107265856266\n",
      "Epoch 22, Batch: 16000| Training Loss: 3.340131715707481\n",
      "Epoch 22, Batch: 17000| Training Loss: 3.3394555155740067\n",
      "Epoch 22, Batch: 18000| Training Loss: 3.3396399120820894\n",
      "Epoch 22, Batch: 19000| Training Loss: 3.3384752529232125\n",
      "Epoch 22, Batch: 20000| Training Loss: 3.338983497387171\n",
      "Epoch 22, Batch: 21000| Training Loss: 3.3383238736845198\n",
      "Epoch 22, Batch: 22000| Training Loss: 3.3367657281431287\n",
      "Epoch 22, Batch: 23000| Training Loss: 3.3354824658735938\n",
      "Epoch 22, Batch: 24000| Training Loss: 3.3349941020260254\n",
      "Epoch 22, Batch: 25000| Training Loss: 3.3339440146398545\n",
      "Epoch 22, Training Loss: 3.3333624821608687, Validation Error: 75.29131432888734, Validation Top-3 Accuracy: 42.63505070740823, Training Error: 72.85673210451294\n",
      "Epoch 23, Batch: 1000| Training Loss: 3.3569315259456634\n",
      "Epoch 23, Batch: 2000| Training Loss: 3.3586174483299254\n",
      "Epoch 23, Batch: 3000| Training Loss: 3.3578607749938967\n",
      "Epoch 23, Batch: 4000| Training Loss: 3.3586583131551744\n",
      "Epoch 23, Batch: 5000| Training Loss: 3.351147189664841\n",
      "Epoch 23, Batch: 6000| Training Loss: 3.347170107404391\n",
      "Epoch 23, Batch: 7000| Training Loss: 3.3470907792704447\n",
      "Epoch 23, Batch: 8000| Training Loss: 3.3451333355754613\n",
      "Epoch 23, Batch: 9000| Training Loss: 3.345182374967469\n",
      "Epoch 23, Batch: 10000| Training Loss: 3.344546049845219\n",
      "Epoch 23, Batch: 11000| Training Loss: 3.343502777565609\n",
      "Epoch 23, Batch: 12000| Training Loss: 3.3419898734390734\n",
      "Epoch 23, Batch: 13000| Training Loss: 3.340905927007015\n",
      "Epoch 23, Batch: 14000| Training Loss: 3.3406332311715397\n",
      "Epoch 23, Batch: 15000| Training Loss: 3.3395306438525516\n",
      "Epoch 23, Batch: 16000| Training Loss: 3.33857958804816\n",
      "Epoch 23, Batch: 17000| Training Loss: 3.337717854324509\n",
      "Epoch 23, Batch: 18000| Training Loss: 3.337948820716805\n",
      "Epoch 23, Batch: 19000| Training Loss: 3.3368180197602824\n",
      "Epoch 23, Batch: 20000| Training Loss: 3.33732023422122\n",
      "Epoch 23, Batch: 21000| Training Loss: 3.336655586407298\n",
      "Epoch 23, Batch: 22000| Training Loss: 3.3351773454763673\n",
      "Epoch 23, Batch: 23000| Training Loss: 3.333856246849765\n",
      "Epoch 23, Batch: 24000| Training Loss: 3.333420738115907\n",
      "Epoch 23, Batch: 25000| Training Loss: 3.332353629364967\n",
      "Epoch 23, Training Loss: 3.3317654692205374, Validation Error: 75.3072966611455, Validation Top-3 Accuracy: 42.64086246459302, Training Error: 72.84143812361208\n",
      "Epoch 24, Batch: 1000| Training Loss: 3.3554180684089663\n",
      "Epoch 24, Batch: 2000| Training Loss: 3.3555175864696505\n",
      "Epoch 24, Batch: 3000| Training Loss: 3.3551999797821046\n",
      "Epoch 24, Batch: 4000| Training Loss: 3.3557075480818748\n",
      "Epoch 24, Batch: 5000| Training Loss: 3.3481910626888274\n",
      "Epoch 24, Batch: 6000| Training Loss: 3.3447139204740526\n",
      "Epoch 24, Batch: 7000| Training Loss: 3.3446187339850835\n",
      "Epoch 24, Batch: 8000| Training Loss: 3.3429704873263835\n",
      "Epoch 24, Batch: 9000| Training Loss: 3.3431039536794027\n",
      "Epoch 24, Batch: 10000| Training Loss: 3.3422256268262864\n",
      "Epoch 24, Batch: 11000| Training Loss: 3.340976062492891\n",
      "Epoch 24, Batch: 12000| Training Loss: 3.3393210770090422\n",
      "Epoch 24, Batch: 13000| Training Loss: 3.338297306445929\n",
      "Epoch 24, Batch: 14000| Training Loss: 3.3381227439812253\n",
      "Epoch 24, Batch: 15000| Training Loss: 3.336924490213394\n",
      "Epoch 24, Batch: 16000| Training Loss: 3.336039903476834\n",
      "Epoch 24, Batch: 17000| Training Loss: 3.3353784975304324\n",
      "Epoch 24, Batch: 18000| Training Loss: 3.335539206094212\n",
      "Epoch 24, Batch: 19000| Training Loss: 3.3344870901107786\n",
      "Epoch 24, Batch: 20000| Training Loss: 3.3350742826104165\n",
      "Epoch 24, Batch: 21000| Training Loss: 3.3345659755525134\n",
      "Epoch 24, Batch: 22000| Training Loss: 3.3330862879969856\n",
      "Epoch 24, Batch: 23000| Training Loss: 3.331792494141537\n",
      "Epoch 24, Batch: 24000| Training Loss: 3.331283400128285\n",
      "Epoch 24, Batch: 25000| Training Loss: 3.330191961174011\n",
      "Epoch 24, Training Loss: 3.329574962984413, Validation Error: 75.30148490396071, Validation Top-3 Accuracy: 42.648127161074, Training Error: 72.78399393134838\n",
      "Epoch 25, Batch: 1000| Training Loss: 3.3532452297210695\n",
      "Epoch 25, Batch: 2000| Training Loss: 3.353822329759598\n",
      "Epoch 25, Batch: 3000| Training Loss: 3.3535044040679933\n",
      "Epoch 25, Batch: 4000| Training Loss: 3.353989459872246\n",
      "Epoch 25, Batch: 5000| Training Loss: 3.3466798419475556\n",
      "Epoch 25, Batch: 6000| Training Loss: 3.343151657581329\n",
      "Epoch 25, Batch: 7000| Training Loss: 3.342891827753612\n",
      "Epoch 25, Batch: 8000| Training Loss: 3.341138042002916\n",
      "Epoch 25, Batch: 9000| Training Loss: 3.341122297339969\n",
      "Epoch 25, Batch: 10000| Training Loss: 3.3401271013736724\n",
      "Epoch 25, Batch: 11000| Training Loss: 3.338909462061795\n",
      "Epoch 25, Batch: 12000| Training Loss: 3.337155324916045\n",
      "Epoch 25, Batch: 13000| Training Loss: 3.3361762617918163\n",
      "Epoch 25, Batch: 14000| Training Loss: 3.3360464477368765\n",
      "Epoch 25, Batch: 15000| Training Loss: 3.33482899851799\n",
      "Epoch 25, Batch: 16000| Training Loss: 3.333818777561188\n",
      "Epoch 25, Batch: 17000| Training Loss: 3.3331765327453615\n",
      "Epoch 25, Batch: 18000| Training Loss: 3.3331987126138474\n",
      "Epoch 25, Batch: 19000| Training Loss: 3.3320325988342887\n",
      "Epoch 25, Batch: 20000| Training Loss: 3.3325910290002825\n",
      "Epoch 25, Batch: 21000| Training Loss: 3.331853336788359\n",
      "Epoch 25, Batch: 22000| Training Loss: 3.330237082102082\n",
      "Epoch 25, Batch: 23000| Training Loss: 3.3289543785115945\n",
      "Epoch 25, Batch: 24000| Training Loss: 3.3283613411188124\n",
      "Epoch 25, Batch: 25000| Training Loss: 3.327192245297432\n",
      "Epoch 25, Training Loss: 3.326602897832784, Validation Error: 75.26080260366722, Validation Top-3 Accuracy: 42.65975067544357, Training Error: 72.77952808892532\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we filter out illegal moves in our prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n",
      "Validation Accuracy: 35.87428571428571%\n"
     ]
    }
   ],
   "source": [
    "def is_legal_move(chess_board, move_san):\n",
    "    try:\n",
    "        chess_move = chess_board.parse_san(move_san)\n",
    "        return chess_move in chess_board.legal_moves\n",
    "    except ValueError:\n",
    "        # This handles cases where the SAN move cannot be parsed or is not legal\n",
    "        return False\n",
    "\n",
    "def load_board_state_from_san(moves):\n",
    "    board = chess.Board()\n",
    "    for index in moves:\n",
    "        try:\n",
    "            if index == 0:\n",
    "                return board\n",
    "            else:\n",
    "                move_san = vocab.get_move(index.item())\n",
    "                move = board.parse_san(move_san)\n",
    "                board.push(move)\n",
    "        except ValueError:\n",
    "            # Handle invalid moves, e.g., break the loop or log an error\n",
    "            break\n",
    "    return board\n",
    "\n",
    "val_size = int(total_size * 0.04)\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "if val_loader is not None:\n",
    "    with torch.no_grad():\n",
    "        for boards, sequences, lengths, labels in val_loader:\n",
    "            boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            outputs = model(boards, sequences, lengths)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            minus = 0\n",
    "            for idx, (sequence, label) in enumerate(zip(sequences, labels)):\n",
    "                # This tells us we're looking at games that include the opening but has developed more than the first 4 half-moves\n",
    "                if sequence[-1].item() == 0 and sequence[2].item() != 0 and sequence[3].item() != 0 and sequence[4].item() != 0:\n",
    "                    output = probabilities[idx]\n",
    "                    sorted_probs, sorted_indices = torch.sort(output, descending=True)\n",
    "                    predicted_move = sorted_indices[0]\n",
    "                    # print(predicted_move)\n",
    "                    chess_board = load_board_state_from_san(sequence)\n",
    "                    for move_idx in sorted_indices:\n",
    "                        move = vocab.get_move(move_idx.item()) # Convert index to move (e.g., 'e2e4')\n",
    "                        if is_legal_move(chess_board, move):\n",
    "                            # print(\"we found one\")\n",
    "                            predicted_move = vocab.get_id(move)\n",
    "                            break\n",
    "                    \n",
    "                    # Check if predicted move is correct\n",
    "                    correct_move = label.item() # Convert label to move\n",
    "                    # print(correct_move)\n",
    "                    if predicted_move == correct_move:\n",
    "                        val_correct += 1\n",
    "                else:\n",
    "                    minus += 1\n",
    "            val_total += (labels.size(0) - minus)\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multimodalmodel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 (CNN has 3 convolutions,SE, ReLu after combining inputs and larger RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634630\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131882\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 100\n",
    "d_embed = 48\n",
    "NUM_EPOCHS = 15\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalTwo(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 5.358224760532379\n",
      "Epoch 1, Batch: 2000| Training Loss: 5.12661018884182\n",
      "Epoch 1, Batch: 3000| Training Loss: 4.984487212657928\n",
      "Epoch 1, Batch: 4000| Training Loss: 4.880090206086636\n",
      "Epoch 1, Batch: 5000| Training Loss: 4.78517461733818\n",
      "Epoch 1, Batch: 6000| Training Loss: 4.7084750316937765\n",
      "Epoch 1, Batch: 7000| Training Loss: 4.644520375422069\n",
      "Epoch 1, Batch: 8000| Training Loss: 4.584236370325089\n",
      "Epoch 1, Batch: 9000| Training Loss: 4.53359999956025\n",
      "Epoch 1, Batch: 10000| Training Loss: 4.487822722411155\n",
      "Epoch 1, Batch: 11000| Training Loss: 4.445587667400186\n",
      "Epoch 1, Batch: 12000| Training Loss: 4.40721798068285\n",
      "Epoch 1, Batch: 13000| Training Loss: 4.372255387526292\n",
      "Epoch 1, Batch: 14000| Training Loss: 4.34038493510655\n",
      "Epoch 1, Batch: 15000| Training Loss: 4.310646494738261\n",
      "Epoch 1, Batch: 16000| Training Loss: 4.282993175700307\n",
      "Epoch 1, Batch: 17000| Training Loss: 4.257371399865431\n",
      "Epoch 1, Batch: 18000| Training Loss: 4.234443660351965\n",
      "Epoch 1, Batch: 19000| Training Loss: 4.212246252285807\n",
      "Epoch 1, Batch: 20000| Training Loss: 4.192180788624286\n",
      "Epoch 1, Batch: 21000| Training Loss: 4.1727453940822965\n",
      "Epoch 1, Batch: 22000| Training Loss: 4.153056076591665\n",
      "Epoch 1, Batch: 23000| Training Loss: 4.134850503247717\n",
      "Epoch 1, Batch: 24000| Training Loss: 4.117725422114134\n",
      "Epoch 1, Batch: 25000| Training Loss: 4.100960772876739\n",
      "Epoch 1, Training Loss: 4.092024691895884, Validation Error: 78.61563943858425, Validation Top-3 Accuracy: 38.39246796277512, Training Error: 81.66166043691845\n",
      "Epoch 2, Batch: 1000| Training Loss: 3.651757910966873\n",
      "Epoch 2, Batch: 2000| Training Loss: 3.6435871212482454\n",
      "Epoch 2, Batch: 3000| Training Loss: 3.636805051167806\n",
      "Epoch 2, Batch: 4000| Training Loss: 3.6328137121200563\n",
      "Epoch 2, Batch: 5000| Training Loss: 3.6196360639572145\n",
      "Epoch 2, Batch: 6000| Training Loss: 3.611985718011856\n",
      "Epoch 2, Batch: 7000| Training Loss: 3.606109873499189\n",
      "Epoch 2, Batch: 8000| Training Loss: 3.5995867279171945\n",
      "Epoch 2, Batch: 9000| Training Loss: 3.5946447857485877\n",
      "Epoch 2, Batch: 10000| Training Loss: 3.589093448472023\n",
      "Epoch 2, Batch: 11000| Training Loss: 3.5832591432874854\n",
      "Epoch 2, Batch: 12000| Training Loss: 3.5772508108615875\n",
      "Epoch 2, Batch: 13000| Training Loss: 3.572073632130256\n",
      "Epoch 2, Batch: 14000| Training Loss: 3.5667808487585613\n",
      "Epoch 2, Batch: 15000| Training Loss: 3.5611668466091158\n",
      "Epoch 2, Batch: 16000| Training Loss: 3.5557396154105665\n",
      "Epoch 2, Batch: 17000| Training Loss: 3.5509392418580896\n",
      "Epoch 2, Batch: 18000| Training Loss: 3.5468732714255653\n",
      "Epoch 2, Batch: 19000| Training Loss: 3.5419234688031045\n",
      "Epoch 2, Batch: 20000| Training Loss: 3.5383571269750593\n",
      "Epoch 2, Batch: 21000| Training Loss: 3.5342637304010847\n",
      "Epoch 2, Batch: 22000| Training Loss: 3.5291866980357605\n",
      "Epoch 2, Batch: 23000| Training Loss: 3.5244237489182018\n",
      "Epoch 2, Batch: 24000| Training Loss: 3.520382763793071\n",
      "Epoch 2, Batch: 25000| Training Loss: 3.51564538479805\n",
      "Epoch 2, Training Loss: 3.513015216336479, Validation Error: 76.1470955743469, Validation Top-3 Accuracy: 42.37352163435272, Training Error: 76.12114056391967\n",
      "Epoch 3, Batch: 1000| Training Loss: 3.3968165423870085\n",
      "Epoch 3, Batch: 2000| Training Loss: 3.3915503358840944\n",
      "Epoch 3, Batch: 3000| Training Loss: 3.388434473911921\n",
      "Epoch 3, Batch: 4000| Training Loss: 3.386762285888195\n",
      "Epoch 3, Batch: 5000| Training Loss: 3.376897746324539\n",
      "Epoch 3, Batch: 6000| Training Loss: 3.371298305829366\n",
      "Epoch 3, Batch: 7000| Training Loss: 3.367647926398686\n",
      "Epoch 3, Batch: 8000| Training Loss: 3.3633185525238516\n",
      "Epoch 3, Batch: 9000| Training Loss: 3.360056888686286\n",
      "Epoch 3, Batch: 10000| Training Loss: 3.356820752286911\n",
      "Epoch 3, Batch: 11000| Training Loss: 3.353009033203125\n",
      "Epoch 3, Batch: 12000| Training Loss: 3.349058154940605\n",
      "Epoch 3, Batch: 13000| Training Loss: 3.3461518198160025\n",
      "Epoch 3, Batch: 14000| Training Loss: 3.3431409624814985\n",
      "Epoch 3, Batch: 15000| Training Loss: 3.3394856364568075\n",
      "Epoch 3, Batch: 16000| Training Loss: 3.3361066677719355\n",
      "Epoch 3, Batch: 17000| Training Loss: 3.333304230633904\n",
      "Epoch 3, Batch: 18000| Training Loss: 3.330866246461868\n",
      "Epoch 3, Batch: 19000| Training Loss: 3.3272903495587802\n",
      "Epoch 3, Batch: 20000| Training Loss: 3.325440343296528\n",
      "Epoch 3, Batch: 21000| Training Loss: 3.3230360287825267\n",
      "Epoch 3, Batch: 22000| Training Loss: 3.3195801607262005\n",
      "Epoch 3, Batch: 23000| Training Loss: 3.316181686411733\n",
      "Epoch 3, Batch: 24000| Training Loss: 3.3136821582814058\n",
      "Epoch 3, Batch: 25000| Training Loss: 3.3103332914543153\n",
      "Epoch 3, Training Loss: 3.3084982360466917, Validation Error: 74.94406183709644, Validation Top-3 Accuracy: 44.36840728794351, Training Error: 73.71386797012168\n",
      "Epoch 4, Batch: 1000| Training Loss: 3.2426090804338457\n",
      "Epoch 4, Batch: 2000| Training Loss: 3.2394311973452568\n",
      "Epoch 4, Batch: 3000| Training Loss: 3.237845349272092\n",
      "Epoch 4, Batch: 4000| Training Loss: 3.2373722124397757\n",
      "Epoch 4, Batch: 5000| Training Loss: 3.2290585483789442\n",
      "Epoch 4, Batch: 6000| Training Loss: 3.224792125840982\n",
      "Epoch 4, Batch: 7000| Training Loss: 3.22172945197991\n",
      "Epoch 4, Batch: 8000| Training Loss: 3.2184972061961887\n",
      "Epoch 4, Batch: 9000| Training Loss: 3.2163732378085452\n",
      "Epoch 4, Batch: 10000| Training Loss: 3.2137737374186517\n",
      "Epoch 4, Batch: 11000| Training Loss: 3.2110257436253806\n",
      "Epoch 4, Batch: 12000| Training Loss: 3.2080346365670365\n",
      "Epoch 4, Batch: 13000| Training Loss: 3.2058979679896282\n",
      "Epoch 4, Batch: 14000| Training Loss: 3.2036454378621917\n",
      "Epoch 4, Batch: 15000| Training Loss: 3.2007091786146162\n",
      "Epoch 4, Batch: 16000| Training Loss: 3.198168506927788\n",
      "Epoch 4, Batch: 17000| Training Loss: 3.1960652034352806\n",
      "Epoch 4, Batch: 18000| Training Loss: 3.194248376482063\n",
      "Epoch 4, Batch: 19000| Training Loss: 3.191405326749149\n",
      "Epoch 4, Batch: 20000| Training Loss: 3.1901013551652433\n",
      "Epoch 4, Batch: 21000| Training Loss: 3.1883402648937134\n",
      "Epoch 4, Batch: 22000| Training Loss: 3.185604849192229\n",
      "Epoch 4, Batch: 23000| Training Loss: 3.1829309985896814\n",
      "Epoch 4, Batch: 24000| Training Loss: 3.181089966798822\n",
      "Epoch 4, Batch: 25000| Training Loss: 3.178336095890999\n",
      "Epoch 4, Training Loss: 3.1768458565552247, Validation Error: 74.01853950541947, Validation Top-3 Accuracy: 45.680411473447904, Training Error: 72.16097832537027\n",
      "Epoch 5, Batch: 1000| Training Loss: 3.140281164884567\n",
      "Epoch 5, Batch: 2000| Training Loss: 3.137476062178612\n",
      "Epoch 5, Batch: 3000| Training Loss: 3.136243986765544\n",
      "Epoch 5, Batch: 4000| Training Loss: 3.1367310014367105\n",
      "Epoch 5, Batch: 5000| Training Loss: 3.1296361626148226\n",
      "Epoch 5, Batch: 6000| Training Loss: 3.1260660774707794\n",
      "Epoch 5, Batch: 7000| Training Loss: 3.123539417709623\n",
      "Epoch 5, Batch: 8000| Training Loss: 3.120637203261256\n",
      "Epoch 5, Batch: 9000| Training Loss: 3.118758892522918\n",
      "Epoch 5, Batch: 10000| Training Loss: 3.11668400799036\n",
      "Epoch 5, Batch: 11000| Training Loss: 3.114330873803659\n",
      "Epoch 5, Batch: 12000| Training Loss: 3.1116949720084666\n",
      "Epoch 5, Batch: 13000| Training Loss: 3.1100655871446317\n",
      "Epoch 5, Batch: 14000| Training Loss: 3.108396685046809\n",
      "Epoch 5, Batch: 15000| Training Loss: 3.1060492034832636\n",
      "Epoch 5, Batch: 16000| Training Loss: 3.103933355100453\n",
      "Epoch 5, Batch: 17000| Training Loss: 3.1021872108052757\n",
      "Epoch 5, Batch: 18000| Training Loss: 3.1006701845261784\n",
      "Epoch 5, Batch: 19000| Training Loss: 3.098166624828389\n",
      "Epoch 5, Batch: 20000| Training Loss: 3.0972964865148067\n",
      "Epoch 5, Batch: 21000| Training Loss: 3.095809174202737\n",
      "Epoch 5, Batch: 22000| Training Loss: 3.093537267961285\n",
      "Epoch 5, Batch: 23000| Training Loss: 3.091270495036374\n",
      "Epoch 5, Batch: 24000| Training Loss: 3.0895970076074204\n",
      "Epoch 5, Batch: 25000| Training Loss: 3.087207551774979\n",
      "Epoch 5, Training Loss: 3.085907254332188, Validation Error: 73.4707813907535, Validation Top-3 Accuracy: 46.569610321507554, Training Error: 71.06703045949236\n",
      "Epoch 6, Batch: 1000| Training Loss: 3.0678865529298784\n",
      "Epoch 6, Batch: 2000| Training Loss: 3.065613861501217\n",
      "Epoch 6, Batch: 3000| Training Loss: 3.065330901503563\n",
      "Epoch 6, Batch: 4000| Training Loss: 3.0664185711443426\n",
      "Epoch 6, Batch: 5000| Training Loss: 3.059781267666817\n",
      "Epoch 6, Batch: 6000| Training Loss: 3.0567422559460002\n",
      "Epoch 6, Batch: 7000| Training Loss: 3.0544450075115477\n",
      "Epoch 6, Batch: 8000| Training Loss: 3.051874518662691\n",
      "Epoch 6, Batch: 9000| Training Loss: 3.0502714044517942\n",
      "Epoch 6, Batch: 10000| Training Loss: 3.048397197175026\n",
      "Epoch 6, Batch: 11000| Training Loss: 3.0463356130990116\n",
      "Epoch 6, Batch: 12000| Training Loss: 3.04400585069259\n",
      "Epoch 6, Batch: 13000| Training Loss: 3.042730453913028\n",
      "Epoch 6, Batch: 14000| Training Loss: 3.041364134669304\n",
      "Epoch 6, Batch: 15000| Training Loss: 3.0393445292949677\n",
      "Epoch 6, Batch: 16000| Training Loss: 3.0374716517180205\n",
      "Epoch 6, Batch: 17000| Training Loss: 3.0359388306000654\n",
      "Epoch 6, Batch: 18000| Training Loss: 3.0345618599587016\n",
      "Epoch 6, Batch: 19000| Training Loss: 3.032283503331636\n",
      "Epoch 6, Batch: 20000| Training Loss: 3.0315905069708826\n",
      "Epoch 6, Batch: 21000| Training Loss: 3.0304173103741237\n",
      "Epoch 6, Batch: 22000| Training Loss: 3.028403895486485\n",
      "Epoch 6, Batch: 23000| Training Loss: 3.0263572992967522\n",
      "Epoch 6, Batch: 24000| Training Loss: 3.024980157852173\n",
      "Epoch 6, Batch: 25000| Training Loss: 3.0228709458446503\n",
      "Epoch 6, Training Loss: 3.0217201649400587, Validation Error: 73.00148199808211, Validation Top-3 Accuracy: 47.232150640573025, Training Error: 70.22390388038883\n",
      "Epoch 7, Batch: 1000| Training Loss: 3.0201055139303206\n",
      "Epoch 7, Batch: 2000| Training Loss: 3.017170093357563\n",
      "Epoch 7, Batch: 3000| Training Loss: 3.0174953127702078\n",
      "Epoch 7, Batch: 4000| Training Loss: 3.0186905769705774\n",
      "Epoch 7, Batch: 5000| Training Loss: 3.0124560492038728\n",
      "Epoch 7, Batch: 6000| Training Loss: 3.0095274131298067\n",
      "Epoch 7, Batch: 7000| Training Loss: 3.007142053638186\n",
      "Epoch 7, Batch: 8000| Training Loss: 3.0049535977989437\n",
      "Epoch 7, Batch: 9000| Training Loss: 3.003604592680931\n",
      "Epoch 7, Batch: 10000| Training Loss: 3.0019830515146255\n",
      "Epoch 7, Batch: 11000| Training Loss: 3.0001343515136023\n",
      "Epoch 7, Batch: 12000| Training Loss: 2.9979772396186988\n",
      "Epoch 7, Batch: 13000| Training Loss: 2.996847433869655\n",
      "Epoch 7, Batch: 14000| Training Loss: 2.995643303470952\n",
      "Epoch 7, Batch: 15000| Training Loss: 2.9937524714549384\n",
      "Epoch 7, Batch: 16000| Training Loss: 2.991907784126699\n",
      "Epoch 7, Batch: 17000| Training Loss: 2.9905570759562887\n",
      "Epoch 7, Batch: 18000| Training Loss: 2.989292973856131\n",
      "Epoch 7, Batch: 19000| Training Loss: 2.9873047904403585\n",
      "Epoch 7, Batch: 20000| Training Loss: 2.986717792803049\n",
      "Epoch 7, Batch: 21000| Training Loss: 2.985636543364752\n",
      "Epoch 7, Batch: 22000| Training Loss: 2.98374988898364\n",
      "Epoch 7, Batch: 23000| Training Loss: 2.9818797529158383\n",
      "Epoch 7, Batch: 24000| Training Loss: 2.980530419498682\n",
      "Epoch 7, Batch: 25000| Training Loss: 2.9784792034959793\n",
      "Epoch 7, Training Loss: 2.9774368919852794, Validation Error: 72.72542353180484, Validation Top-3 Accuracy: 47.6709383092367, Training Error: 69.64713727265497\n",
      "Epoch 8, Batch: 1000| Training Loss: 2.985103551506996\n",
      "Epoch 8, Batch: 2000| Training Loss: 2.98260239225626\n",
      "Epoch 8, Batch: 3000| Training Loss: 2.9829917999505997\n",
      "Epoch 8, Batch: 4000| Training Loss: 2.9844755600988866\n",
      "Epoch 8, Batch: 5000| Training Loss: 2.9788212438821793\n",
      "Epoch 8, Batch: 6000| Training Loss: 2.9758359957734744\n",
      "Epoch 8, Batch: 7000| Training Loss: 2.9734476071596148\n",
      "Epoch 8, Batch: 8000| Training Loss: 2.971505431905389\n",
      "Epoch 8, Batch: 9000| Training Loss: 2.970245382944743\n",
      "Epoch 8, Batch: 10000| Training Loss: 2.9688278483510016\n",
      "Epoch 8, Batch: 11000| Training Loss: 2.9670583903030914\n",
      "Epoch 8, Batch: 12000| Training Loss: 2.9651006292104722\n",
      "Epoch 8, Batch: 13000| Training Loss: 2.963996638939931\n",
      "Epoch 8, Batch: 14000| Training Loss: 2.962999806829861\n",
      "Epoch 8, Batch: 15000| Training Loss: 2.961275273609161\n",
      "Epoch 8, Batch: 16000| Training Loss: 2.959512501217425\n",
      "Epoch 8, Batch: 17000| Training Loss: 2.958209812648156\n",
      "Epoch 8, Batch: 18000| Training Loss: 2.9569436461925505\n",
      "Epoch 8, Batch: 19000| Training Loss: 2.955014316207484\n",
      "Epoch 8, Batch: 20000| Training Loss: 2.9544911190867422\n",
      "Epoch 8, Batch: 21000| Training Loss: 2.953485906294414\n",
      "Epoch 8, Batch: 22000| Training Loss: 2.9517084162452005\n",
      "Epoch 8, Batch: 23000| Training Loss: 2.9498474678734072\n",
      "Epoch 8, Batch: 24000| Training Loss: 2.9485820445170003\n",
      "Epoch 8, Batch: 25000| Training Loss: 2.946619019021988\n",
      "Epoch 8, Training Loss: 2.945615731062451, Validation Error: 72.5089355766716, Validation Top-3 Accuracy: 47.69127945938345, Training Error: 69.20300006729352\n",
      "Epoch 9, Batch: 1000| Training Loss: 2.9591165297031403\n",
      "Epoch 9, Batch: 2000| Training Loss: 2.9565978664159775\n",
      "Epoch 9, Batch: 3000| Training Loss: 2.9579790262381236\n",
      "Epoch 9, Batch: 4000| Training Loss: 2.9592695457339286\n",
      "Epoch 9, Batch: 5000| Training Loss: 2.9535207255125044\n",
      "Epoch 9, Batch: 6000| Training Loss: 2.9507935391465825\n",
      "Epoch 9, Batch: 7000| Training Loss: 2.9486976746320726\n",
      "Epoch 9, Batch: 8000| Training Loss: 2.9469273146241903\n",
      "Epoch 9, Batch: 9000| Training Loss: 2.9459409682618247\n",
      "Epoch 9, Batch: 10000| Training Loss: 2.9443347998857496\n",
      "Epoch 9, Batch: 11000| Training Loss: 2.9425533046505667\n",
      "Epoch 9, Batch: 12000| Training Loss: 2.9405323606431484\n",
      "Epoch 9, Batch: 13000| Training Loss: 2.9396069645789953\n",
      "Epoch 9, Batch: 14000| Training Loss: 2.9387191323637962\n",
      "Epoch 9, Batch: 15000| Training Loss: 2.937059528652827\n",
      "Epoch 9, Batch: 16000| Training Loss: 2.9353375568389892\n",
      "Epoch 9, Batch: 17000| Training Loss: 2.9341450306387507\n",
      "Epoch 9, Batch: 18000| Training Loss: 2.9329819265007973\n",
      "Epoch 9, Batch: 19000| Training Loss: 2.9311470370355406\n",
      "Epoch 9, Batch: 20000| Training Loss: 2.9306889545619486\n",
      "Epoch 9, Batch: 21000| Training Loss: 2.9297191989478613\n",
      "Epoch 9, Batch: 22000| Training Loss: 2.9280191683389925\n",
      "Epoch 9, Batch: 23000| Training Loss: 2.9262417292439418\n",
      "Epoch 9, Batch: 24000| Training Loss: 2.925040630112092\n",
      "Epoch 9, Batch: 25000| Training Loss: 2.923190500454903\n",
      "Epoch 9, Training Loss: 2.9221982374027746, Validation Error: 72.37381222212537, Validation Top-3 Accuracy: 47.923749746774845, Training Error: 68.90715330074696\n",
      "Epoch 10, Batch: 1000| Training Loss: 2.942651938080788\n",
      "Epoch 10, Batch: 2000| Training Loss: 2.9388877754807474\n",
      "Epoch 10, Batch: 3000| Training Loss: 2.9402778798739115\n",
      "Epoch 10, Batch: 4000| Training Loss: 2.941494235098362\n",
      "Epoch 10, Batch: 5000| Training Loss: 2.9359844781398774\n",
      "Epoch 10, Batch: 6000| Training Loss: 2.9336939415534338\n",
      "Epoch 10, Batch: 7000| Training Loss: 2.931403711421149\n",
      "Epoch 10, Batch: 8000| Training Loss: 2.9296506073325874\n",
      "Epoch 10, Batch: 9000| Training Loss: 2.92880011733373\n",
      "Epoch 10, Batch: 10000| Training Loss: 2.927413677430153\n",
      "Epoch 10, Batch: 11000| Training Loss: 2.925694975571199\n",
      "Epoch 10, Batch: 12000| Training Loss: 2.9236992925703524\n",
      "Epoch 10, Batch: 13000| Training Loss: 2.922867253386057\n",
      "Epoch 10, Batch: 14000| Training Loss: 2.9219514468227112\n",
      "Epoch 10, Batch: 15000| Training Loss: 2.920351440747579\n",
      "Epoch 10, Batch: 16000| Training Loss: 2.918701268956065\n",
      "Epoch 10, Batch: 17000| Training Loss: 2.917517299567952\n",
      "Epoch 10, Batch: 18000| Training Loss: 2.916393088698387\n",
      "Epoch 10, Batch: 19000| Training Loss: 2.914512223877405\n",
      "Epoch 10, Batch: 20000| Training Loss: 2.914086051827669\n",
      "Epoch 10, Batch: 21000| Training Loss: 2.9131151449737094\n",
      "Epoch 10, Batch: 22000| Training Loss: 2.911441717326641\n",
      "Epoch 10, Batch: 23000| Training Loss: 2.909687483549118\n",
      "Epoch 10, Batch: 24000| Training Loss: 2.9084795691172283\n",
      "Epoch 10, Batch: 25000| Training Loss: 2.9066401192712785\n",
      "Epoch 10, Training Loss: 2.905672571250429, Validation Error: 72.27791822857641, Validation Top-3 Accuracy: 48.05015546554392, Training Error: 68.66850602276968\n",
      "Epoch 11, Batch: 1000| Training Loss: 2.9272338223457335\n",
      "Epoch 11, Batch: 2000| Training Loss: 2.9249668496251107\n",
      "Epoch 11, Batch: 3000| Training Loss: 2.926251265605291\n",
      "Epoch 11, Batch: 4000| Training Loss: 2.927783680379391\n",
      "Epoch 11, Batch: 5000| Training Loss: 2.9225695748805998\n",
      "Epoch 11, Batch: 6000| Training Loss: 2.9202721897761026\n",
      "Epoch 11, Batch: 7000| Training Loss: 2.918092726945877\n",
      "Epoch 11, Batch: 8000| Training Loss: 2.916610171958804\n",
      "Epoch 11, Batch: 9000| Training Loss: 2.9158207762108908\n",
      "Epoch 11, Batch: 10000| Training Loss: 2.9144918267607687\n",
      "Epoch 11, Batch: 11000| Training Loss: 2.9128593230139126\n",
      "Epoch 11, Batch: 12000| Training Loss: 2.911004111687342\n",
      "Epoch 11, Batch: 13000| Training Loss: 2.9100704071613457\n",
      "Epoch 11, Batch: 14000| Training Loss: 2.9092596410768374\n",
      "Epoch 11, Batch: 15000| Training Loss: 2.9076943257252377\n",
      "Epoch 11, Batch: 16000| Training Loss: 2.9060568145886063\n",
      "Epoch 11, Batch: 17000| Training Loss: 2.904954256709884\n",
      "Epoch 11, Batch: 18000| Training Loss: 2.903845960398515\n",
      "Epoch 11, Batch: 19000| Training Loss: 2.902109869411117\n",
      "Epoch 11, Batch: 20000| Training Loss: 2.901758336442709\n",
      "Epoch 11, Batch: 21000| Training Loss: 2.900886386825925\n",
      "Epoch 11, Batch: 22000| Training Loss: 2.899218969150023\n",
      "Epoch 11, Batch: 23000| Training Loss: 2.897462416845819\n",
      "Epoch 11, Batch: 24000| Training Loss: 2.8962819805194933\n",
      "Epoch 11, Batch: 25000| Training Loss: 2.8944008681297304\n",
      "Epoch 11, Training Loss: 2.893461563883835, Validation Error: 72.26920059279924, Validation Top-3 Accuracy: 48.324760992525, Training Error: 68.48773116852132\n",
      "Epoch 12, Batch: 1000| Training Loss: 2.9175214400291445\n",
      "Epoch 12, Batch: 2000| Training Loss: 2.915702455699444\n",
      "Epoch 12, Batch: 3000| Training Loss: 2.9174674254258472\n",
      "Epoch 12, Batch: 4000| Training Loss: 2.9191014917492866\n",
      "Epoch 12, Batch: 5000| Training Loss: 2.9140793596982957\n",
      "Epoch 12, Batch: 6000| Training Loss: 2.9115413689812026\n",
      "Epoch 12, Batch: 7000| Training Loss: 2.909352442758424\n",
      "Epoch 12, Batch: 8000| Training Loss: 2.907803991943598\n",
      "Epoch 12, Batch: 9000| Training Loss: 2.9071117030249702\n",
      "Epoch 12, Batch: 10000| Training Loss: 2.9057951046824457\n",
      "Epoch 12, Batch: 11000| Training Loss: 2.9041041990735312\n",
      "Epoch 12, Batch: 12000| Training Loss: 2.9022029966513316\n",
      "Epoch 12, Batch: 13000| Training Loss: 2.9011787192546405\n",
      "Epoch 12, Batch: 14000| Training Loss: 2.9003167059847286\n",
      "Epoch 12, Batch: 15000| Training Loss: 2.8988964504559833\n",
      "Epoch 12, Batch: 16000| Training Loss: 2.8972342068850994\n",
      "Epoch 12, Batch: 17000| Training Loss: 2.896018249778187\n",
      "Epoch 12, Batch: 18000| Training Loss: 2.89484561182393\n",
      "Epoch 12, Batch: 19000| Training Loss: 2.89304489679713\n",
      "Epoch 12, Batch: 20000| Training Loss: 2.8927132951796053\n",
      "Epoch 12, Batch: 21000| Training Loss: 2.891872513708614\n",
      "Epoch 12, Batch: 22000| Training Loss: 2.890222723857923\n",
      "Epoch 12, Batch: 23000| Training Loss: 2.888470413612283\n",
      "Epoch 12, Batch: 24000| Training Loss: 2.88723755514125\n",
      "Epoch 12, Batch: 25000| Training Loss: 2.885444519753456\n",
      "Epoch 12, Training Loss: 2.8845409466986762, Validation Error: 72.21253596024758, Validation Top-3 Accuracy: 48.37416092859567, Training Error: 68.37314866361194\n",
      "Epoch 13, Batch: 1000| Training Loss: 2.912703545808792\n",
      "Epoch 13, Batch: 2000| Training Loss: 2.909955084979534\n",
      "Epoch 13, Batch: 3000| Training Loss: 2.911555638829867\n",
      "Epoch 13, Batch: 4000| Training Loss: 2.913081691920757\n",
      "Epoch 13, Batch: 5000| Training Loss: 2.9080309290647506\n",
      "Epoch 13, Batch: 6000| Training Loss: 2.9057465114196144\n",
      "Epoch 13, Batch: 7000| Training Loss: 2.9037076154436385\n",
      "Epoch 13, Batch: 8000| Training Loss: 2.902227616533637\n",
      "Epoch 13, Batch: 9000| Training Loss: 2.9014974092377557\n",
      "Epoch 13, Batch: 10000| Training Loss: 2.9001386999726297\n",
      "Epoch 13, Batch: 11000| Training Loss: 2.8984868774955923\n",
      "Epoch 13, Batch: 12000| Training Loss: 2.896740030914545\n",
      "Epoch 13, Batch: 13000| Training Loss: 2.8958291242947944\n",
      "Epoch 13, Batch: 14000| Training Loss: 2.8951704015476363\n",
      "Epoch 13, Batch: 15000| Training Loss: 2.8937872382799785\n",
      "Epoch 13, Batch: 16000| Training Loss: 2.892190606854856\n",
      "Epoch 13, Batch: 17000| Training Loss: 2.8910519582874636\n",
      "Epoch 13, Batch: 18000| Training Loss: 2.889973071263896\n",
      "Epoch 13, Batch: 19000| Training Loss: 2.8882898202444376\n",
      "Epoch 13, Batch: 20000| Training Loss: 2.887992945200205\n",
      "Epoch 13, Batch: 21000| Training Loss: 2.887200176000595\n",
      "Epoch 13, Batch: 22000| Training Loss: 2.885629110878164\n",
      "Epoch 13, Batch: 23000| Training Loss: 2.8839206466052842\n",
      "Epoch 13, Batch: 24000| Training Loss: 2.8827505879650515\n",
      "Epoch 13, Batch: 25000| Training Loss: 2.8810259076309204\n",
      "Epoch 13, Training Loss: 2.8801704435786855, Validation Error: 72.17621247784268, Validation Top-3 Accuracy: 48.44390201481309, Training Error: 68.3078127772034\n",
      "Epoch 14, Batch: 1000| Training Loss: 2.9070361644029616\n",
      "Epoch 14, Batch: 2000| Training Loss: 2.9045500036478042\n",
      "Epoch 14, Batch: 3000| Training Loss: 2.9061776479482653\n",
      "Epoch 14, Batch: 4000| Training Loss: 2.907652519762516\n",
      "Epoch 14, Batch: 5000| Training Loss: 2.9027121810436247\n",
      "Epoch 14, Batch: 6000| Training Loss: 2.9004349587162337\n",
      "Epoch 14, Batch: 7000| Training Loss: 2.898411779386657\n",
      "Epoch 14, Batch: 8000| Training Loss: 2.897178705081344\n",
      "Epoch 14, Batch: 9000| Training Loss: 2.8964210275411606\n",
      "Epoch 14, Batch: 10000| Training Loss: 2.895313549041748\n",
      "Epoch 14, Batch: 11000| Training Loss: 2.8937483720129187\n",
      "Epoch 14, Batch: 12000| Training Loss: 2.891885655124982\n",
      "Epoch 14, Batch: 13000| Training Loss: 2.8911438416517696\n",
      "Epoch 14, Batch: 14000| Training Loss: 2.8904253344791275\n",
      "Epoch 14, Batch: 15000| Training Loss: 2.8889762285629907\n",
      "Epoch 14, Batch: 16000| Training Loss: 2.887486680150032\n",
      "Epoch 14, Batch: 17000| Training Loss: 2.8864215368944057\n",
      "Epoch 14, Batch: 18000| Training Loss: 2.885308835380607\n",
      "Epoch 14, Batch: 19000| Training Loss: 2.8836468281808654\n",
      "Epoch 14, Batch: 20000| Training Loss: 2.883354423236847\n",
      "Epoch 14, Batch: 21000| Training Loss: 2.8826268673226947\n",
      "Epoch 14, Batch: 22000| Training Loss: 2.881123961865902\n",
      "Epoch 14, Batch: 23000| Training Loss: 2.8795367946106456\n",
      "Epoch 14, Batch: 24000| Training Loss: 2.8784785576264063\n",
      "Epoch 14, Batch: 25000| Training Loss: 2.876788261680603\n",
      "Epoch 14, Training Loss: 2.875925107636436, Validation Error: 72.16313602417691, Validation Top-3 Accuracy: 48.426466743258736, Training Error: 68.23611459474009\n",
      "Epoch 15, Batch: 1000| Training Loss: 2.902768669247627\n",
      "Epoch 15, Batch: 2000| Training Loss: 2.900293594837189\n",
      "Epoch 15, Batch: 3000| Training Loss: 2.9017615470488867\n",
      "Epoch 15, Batch: 4000| Training Loss: 2.9035180234909057\n",
      "Epoch 15, Batch: 5000| Training Loss: 2.8984339817762375\n",
      "Epoch 15, Batch: 6000| Training Loss: 2.8959592772722242\n",
      "Epoch 15, Batch: 7000| Training Loss: 2.8938503274236407\n",
      "Epoch 15, Batch: 8000| Training Loss: 2.8925002371668818\n",
      "Epoch 15, Batch: 9000| Training Loss: 2.89177711851067\n",
      "Epoch 15, Batch: 10000| Training Loss: 2.8905411552786826\n",
      "Epoch 15, Batch: 11000| Training Loss: 2.888896894617514\n",
      "Epoch 15, Batch: 12000| Training Loss: 2.8871798782845337\n",
      "Epoch 15, Batch: 13000| Training Loss: 2.886488559456972\n",
      "Epoch 15, Batch: 14000| Training Loss: 2.885865281198706\n",
      "Epoch 15, Batch: 15000| Training Loss: 2.884509125614166\n",
      "Epoch 15, Batch: 16000| Training Loss: 2.8830463977232577\n",
      "Epoch 15, Batch: 17000| Training Loss: 2.8820447498419703\n",
      "Epoch 15, Batch: 18000| Training Loss: 2.8809907164904804\n",
      "Epoch 15, Batch: 19000| Training Loss: 2.879369472208776\n",
      "Epoch 15, Batch: 20000| Training Loss: 2.8791235761106013\n",
      "Epoch 15, Batch: 21000| Training Loss: 2.878373941557748\n",
      "Epoch 15, Batch: 22000| Training Loss: 2.8768955464363097\n",
      "Epoch 15, Batch: 23000| Training Loss: 2.8752972190069115\n",
      "Epoch 15, Batch: 24000| Training Loss: 2.8743170627256234\n",
      "Epoch 15, Batch: 25000| Training Loss: 2.8726384447431563\n",
      "Epoch 15, Training Loss: 2.8717943185432975, Validation Error: 72.09194199866329, Validation Top-3 Accuracy: 48.47005492214462, Training Error: 68.17389868043533\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 37.41142857142857%\n"
     ]
    }
   ],
   "source": [
    "def is_legal_move(chess_board, move_san):\n",
    "    try:\n",
    "        chess_move = chess_board.parse_san(move_san)\n",
    "        return chess_move in chess_board.legal_moves\n",
    "    except ValueError:\n",
    "        # This handles cases where the SAN move cannot be parsed or is not legal\n",
    "        return False\n",
    "\n",
    "def load_board_state_from_san(moves):\n",
    "    board = chess.Board()\n",
    "    for index in moves:\n",
    "        try:\n",
    "            if index == 0:\n",
    "                return board\n",
    "            else:\n",
    "                move_san = vocab.get_move(index.item())\n",
    "                move = board.parse_san(move_san)\n",
    "                board.push(move)\n",
    "        except ValueError:\n",
    "            # Handle invalid moves, e.g., break the loop or log an error\n",
    "            break\n",
    "    return board\n",
    "\n",
    "val_size = int(total_size * 0.04)\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "if val_loader is not None:\n",
    "    with torch.no_grad():\n",
    "        for boards, sequences, lengths, labels in val_loader:\n",
    "            boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            outputs = model(boards, sequences, lengths)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            minus = 0\n",
    "            for idx, (sequence, label) in enumerate(zip(sequences, labels)):\n",
    "                # This tells us we're looking at games that include the opening but has developed more than the first 4 half-moves\n",
    "                if sequence[-1].item() == 0 and sequence[2].item() != 0 and sequence[3].item() != 0 and sequence[4].item() != 0:\n",
    "                    output = probabilities[idx]\n",
    "                    sorted_probs, sorted_indices = torch.sort(output, descending=True)\n",
    "                    predicted_move = sorted_indices[0]\n",
    "                    # print(predicted_move)\n",
    "                    chess_board = load_board_state_from_san(sequence)\n",
    "                    for move_idx in sorted_indices:\n",
    "                        move = vocab.get_move(move_idx.item()) # Convert index to move (e.g., 'e2e4')\n",
    "                        if is_legal_move(chess_board, move):\n",
    "                            # print(\"we found one\")\n",
    "                            predicted_move = vocab.get_id(move)\n",
    "                            break\n",
    "                    \n",
    "                    # Check if predicted move is correct\n",
    "                    correct_move = label.item() # Convert label to move\n",
    "                    # print(correct_move)\n",
    "                    if predicted_move == correct_move:\n",
    "                        val_correct += 1\n",
    "                else:\n",
    "                    minus += 1\n",
    "            val_total += (labels.size(0) - minus)\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multimodalmodel-2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634630\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131882\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # alpha can be set to a constant, or it can be a tensor of shape (num_classes,)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 100\n",
    "d_embed = 48\n",
    "NUM_EPOCHS = 15\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalTwo(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(gamma=2, alpha=1, reduction='mean')\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 5.216632953643799\n",
      "Epoch 1, Batch: 2000| Training Loss: 4.9681913522481915\n",
      "Epoch 1, Batch: 3000| Training Loss: 4.809984962622325\n",
      "Epoch 1, Batch: 4000| Training Loss: 4.693838460147381\n",
      "Epoch 1, Batch: 5000| Training Loss: 4.583828910255432\n",
      "Epoch 1, Batch: 6000| Training Loss: 4.49359255596002\n",
      "Epoch 1, Batch: 7000| Training Loss: 4.419784398453576\n",
      "Epoch 1, Batch: 8000| Training Loss: 4.352040460258722\n",
      "Epoch 1, Batch: 9000| Training Loss: 4.294694580793381\n",
      "Epoch 1, Batch: 10000| Training Loss: 4.244223247122765\n",
      "Epoch 1, Batch: 11000| Training Loss: 4.197138833761215\n",
      "Epoch 1, Batch: 12000| Training Loss: 4.153256935497125\n",
      "Epoch 1, Batch: 13000| Training Loss: 4.113717842560548\n",
      "Epoch 1, Batch: 14000| Training Loss: 4.077679372889655\n",
      "Epoch 1, Batch: 15000| Training Loss: 4.044189786974589\n",
      "Epoch 1, Batch: 16000| Training Loss: 4.012455889597535\n",
      "Epoch 1, Batch: 17000| Training Loss: 3.9834087364673616\n",
      "Epoch 1, Batch: 18000| Training Loss: 3.956665287401941\n",
      "Epoch 1, Batch: 19000| Training Loss: 3.931393717213681\n",
      "Epoch 1, Batch: 20000| Training Loss: 3.9084694637537\n",
      "Epoch 1, Batch: 21000| Training Loss: 3.8859627670674097\n",
      "Epoch 1, Batch: 22000| Training Loss: 3.8635805377851833\n",
      "Epoch 1, Batch: 23000| Training Loss: 3.8423105001967888\n",
      "Epoch 1, Batch: 24000| Training Loss: 3.822287365158399\n",
      "Epoch 1, Batch: 25000| Training Loss: 3.8030709920835495\n",
      "Epoch 1, Training Loss: 3.7927171604092496, Validation Error: 77.91968151570627, Validation Top-3 Accuracy: 40.393165373723896, Training Error: 81.12129350372868\n",
      "Epoch 2, Batch: 1000| Training Loss: 3.2819662536382674\n",
      "Epoch 2, Batch: 2000| Training Loss: 3.275852493822575\n",
      "Epoch 2, Batch: 3000| Training Loss: 3.269808694163958\n",
      "Epoch 2, Batch: 4000| Training Loss: 3.2638935178220274\n",
      "Epoch 2, Batch: 5000| Training Loss: 3.249166759586334\n",
      "Epoch 2, Batch: 6000| Training Loss: 3.240282856941223\n",
      "Epoch 2, Batch: 7000| Training Loss: 3.233263347574643\n",
      "Epoch 2, Batch: 8000| Training Loss: 3.2249943661540748\n",
      "Epoch 2, Batch: 9000| Training Loss: 3.2195510797897975\n",
      "Epoch 2, Batch: 10000| Training Loss: 3.2134720901846885\n",
      "Epoch 2, Batch: 11000| Training Loss: 3.207195402741432\n",
      "Epoch 2, Batch: 12000| Training Loss: 3.2001376625299454\n",
      "Epoch 2, Batch: 13000| Training Loss: 3.194754500535818\n",
      "Epoch 2, Batch: 14000| Training Loss: 3.1895418390376227\n",
      "Epoch 2, Batch: 15000| Training Loss: 3.1838418961524964\n",
      "Epoch 2, Batch: 16000| Training Loss: 3.177613760218024\n",
      "Epoch 2, Batch: 17000| Training Loss: 3.1721128615772023\n",
      "Epoch 2, Batch: 18000| Training Loss: 3.1673498900665176\n",
      "Epoch 2, Batch: 19000| Training Loss: 3.1619891420351833\n",
      "Epoch 2, Batch: 20000| Training Loss: 3.1582417416751385\n",
      "Epoch 2, Batch: 21000| Training Loss: 3.153596747018042\n",
      "Epoch 2, Batch: 22000| Training Loss: 3.1481343298879536\n",
      "Epoch 2, Batch: 23000| Training Loss: 3.1426628705418627\n",
      "Epoch 2, Batch: 24000| Training Loss: 3.138012427295248\n",
      "Epoch 2, Batch: 25000| Training Loss: 3.1328575599241257\n",
      "Epoch 2, Training Loss: 3.1301114987528003, Validation Error: 75.44096707639555, Validation Top-3 Accuracy: 44.029872431929796, Training Error: 75.23476260682845\n",
      "Epoch 3, Batch: 1000| Training Loss: 3.000643640637398\n",
      "Epoch 3, Batch: 2000| Training Loss: 2.9942184072732925\n",
      "Epoch 3, Batch: 3000| Training Loss: 2.990476909081141\n",
      "Epoch 3, Batch: 4000| Training Loss: 2.98824698343873\n",
      "Epoch 3, Batch: 5000| Training Loss: 2.977424277639389\n",
      "Epoch 3, Batch: 6000| Training Loss: 2.9723902878959976\n",
      "Epoch 3, Batch: 7000| Training Loss: 2.968596156716347\n",
      "Epoch 3, Batch: 8000| Training Loss: 2.963587023794651\n",
      "Epoch 3, Batch: 9000| Training Loss: 2.9616011015706594\n",
      "Epoch 3, Batch: 10000| Training Loss: 2.9581176186800002\n",
      "Epoch 3, Batch: 11000| Training Loss: 2.954791423212398\n",
      "Epoch 3, Batch: 12000| Training Loss: 2.950438773840666\n",
      "Epoch 3, Batch: 13000| Training Loss: 2.9477586478636817\n",
      "Epoch 3, Batch: 14000| Training Loss: 2.9452212199143\n",
      "Epoch 3, Batch: 15000| Training Loss: 2.942076596911748\n",
      "Epoch 3, Batch: 16000| Training Loss: 2.9382585611194374\n",
      "Epoch 3, Batch: 17000| Training Loss: 2.935024822340292\n",
      "Epoch 3, Batch: 18000| Training Loss: 2.9325994193024107\n",
      "Epoch 3, Batch: 19000| Training Loss: 2.9291543554883255\n",
      "Epoch 3, Batch: 20000| Training Loss: 2.9274938245236872\n",
      "Epoch 3, Batch: 21000| Training Loss: 2.924983433411235\n",
      "Epoch 3, Batch: 22000| Training Loss: 2.9215648407177492\n",
      "Epoch 3, Batch: 23000| Training Loss: 2.918184255288995\n",
      "Epoch 3, Batch: 24000| Training Loss: 2.915475325147311\n",
      "Epoch 3, Batch: 25000| Training Loss: 2.912181992902756\n",
      "Epoch 3, Training Loss: 2.9106184281120004, Validation Error: 74.33673321128643, Validation Top-3 Accuracy: 45.90125824646973, Training Error: 72.88829888109235\n",
      "Epoch 4, Batch: 1000| Training Loss: 2.8472345204353333\n",
      "Epoch 4, Batch: 2000| Training Loss: 2.841963389873505\n",
      "Epoch 4, Batch: 3000| Training Loss: 2.840683022260666\n",
      "Epoch 4, Batch: 4000| Training Loss: 2.840290137499571\n",
      "Epoch 4, Batch: 5000| Training Loss: 2.8310180349826815\n",
      "Epoch 4, Batch: 6000| Training Loss: 2.826858473777771\n",
      "Epoch 4, Batch: 7000| Training Loss: 2.823676180209432\n",
      "Epoch 4, Batch: 8000| Training Loss: 2.8195690371245146\n",
      "Epoch 4, Batch: 9000| Training Loss: 2.818657794793447\n",
      "Epoch 4, Batch: 10000| Training Loss: 2.8161660451292994\n",
      "Epoch 4, Batch: 11000| Training Loss: 2.813642211480574\n",
      "Epoch 4, Batch: 12000| Training Loss: 2.810279815226793\n",
      "Epoch 4, Batch: 13000| Training Loss: 2.8083488430243273\n",
      "Epoch 4, Batch: 14000| Training Loss: 2.8067419288243567\n",
      "Epoch 4, Batch: 15000| Training Loss: 2.8043741691350936\n",
      "Epoch 4, Batch: 16000| Training Loss: 2.801354578591883\n",
      "Epoch 4, Batch: 17000| Training Loss: 2.798886652287315\n",
      "Epoch 4, Batch: 18000| Training Loss: 2.7970022896462017\n",
      "Epoch 4, Batch: 19000| Training Loss: 2.7941356245906728\n",
      "Epoch 4, Batch: 20000| Training Loss: 2.7931177794218063\n",
      "Epoch 4, Batch: 21000| Training Loss: 2.7914336742162704\n",
      "Epoch 4, Batch: 22000| Training Loss: 2.788882786349817\n",
      "Epoch 4, Batch: 23000| Training Loss: 2.7861152065111243\n",
      "Epoch 4, Batch: 24000| Training Loss: 2.7840932846864064\n",
      "Epoch 4, Batch: 25000| Training Loss: 2.78158749809742\n",
      "Epoch 4, Training Loss: 2.7803334090976835, Validation Error: 73.52308720541656, Validation Top-3 Accuracy: 46.97352744688932, Training Error: 71.3624489945737\n",
      "Epoch 5, Batch: 1000| Training Loss: 2.7503232909440993\n",
      "Epoch 5, Batch: 2000| Training Loss: 2.7431851140260695\n",
      "Epoch 5, Batch: 3000| Training Loss: 2.742925958673159\n",
      "Epoch 5, Batch: 4000| Training Loss: 2.7433976736366747\n",
      "Epoch 5, Batch: 5000| Training Loss: 2.735429484319687\n",
      "Epoch 5, Batch: 6000| Training Loss: 2.7319870011011758\n",
      "Epoch 5, Batch: 7000| Training Loss: 2.729166055253574\n",
      "Epoch 5, Batch: 8000| Training Loss: 2.7258504909723995\n",
      "Epoch 5, Batch: 9000| Training Loss: 2.7255642856491935\n",
      "Epoch 5, Batch: 10000| Training Loss: 2.72330943107605\n",
      "Epoch 5, Batch: 11000| Training Loss: 2.7212300214659084\n",
      "Epoch 5, Batch: 12000| Training Loss: 2.718216303706169\n",
      "Epoch 5, Batch: 13000| Training Loss: 2.7166780581841103\n",
      "Epoch 5, Batch: 14000| Training Loss: 2.7155301693422453\n",
      "Epoch 5, Batch: 15000| Training Loss: 2.7136465473413467\n",
      "Epoch 5, Batch: 16000| Training Loss: 2.710937145344913\n",
      "Epoch 5, Batch: 17000| Training Loss: 2.708697263801799\n",
      "Epoch 5, Batch: 18000| Training Loss: 2.707256145026949\n",
      "Epoch 5, Batch: 19000| Training Loss: 2.7046667062546077\n",
      "Epoch 5, Batch: 20000| Training Loss: 2.7040714063823224\n",
      "Epoch 5, Batch: 21000| Training Loss: 2.7027382278896512\n",
      "Epoch 5, Batch: 22000| Training Loss: 2.7006037548292765\n",
      "Epoch 5, Batch: 23000| Training Loss: 2.6983664329414783\n",
      "Epoch 5, Batch: 24000| Training Loss: 2.6966489586333435\n",
      "Epoch 5, Batch: 25000| Training Loss: 2.694535411877632\n",
      "Epoch 5, Training Loss: 2.693509607515379, Validation Error: 72.97678203004679, Validation Top-3 Accuracy: 47.714526486736965, Training Error: 70.37861779118211\n",
      "Epoch 6, Batch: 1000| Training Loss: 2.685300169467926\n",
      "Epoch 6, Batch: 2000| Training Loss: 2.677582105219364\n",
      "Epoch 6, Batch: 3000| Training Loss: 2.6778926556507745\n",
      "Epoch 6, Batch: 4000| Training Loss: 2.6790279506742953\n",
      "Epoch 6, Batch: 5000| Training Loss: 2.6716868950128556\n",
      "Epoch 6, Batch: 6000| Training Loss: 2.6688768330613772\n",
      "Epoch 6, Batch: 7000| Training Loss: 2.666120795420238\n",
      "Epoch 6, Batch: 8000| Training Loss: 2.663067936077714\n",
      "Epoch 6, Batch: 9000| Training Loss: 2.66277545773983\n",
      "Epoch 6, Batch: 10000| Training Loss: 2.6609107632756235\n",
      "Epoch 6, Batch: 11000| Training Loss: 2.6589836594191465\n",
      "Epoch 6, Batch: 12000| Training Loss: 2.6563256346186\n",
      "Epoch 6, Batch: 13000| Training Loss: 2.655052075817035\n",
      "Epoch 6, Batch: 14000| Training Loss: 2.654100790696485\n",
      "Epoch 6, Batch: 15000| Training Loss: 2.6523516079505285\n",
      "Epoch 6, Batch: 16000| Training Loss: 2.649903248369694\n",
      "Epoch 6, Batch: 17000| Training Loss: 2.6478227971161115\n",
      "Epoch 6, Batch: 18000| Training Loss: 2.6466453631652724\n",
      "Epoch 6, Batch: 19000| Training Loss: 2.644197550434815\n",
      "Epoch 6, Batch: 20000| Training Loss: 2.6437614857912064\n",
      "Epoch 6, Batch: 21000| Training Loss: 2.6426062157154084\n",
      "Epoch 6, Batch: 22000| Training Loss: 2.640676648286256\n",
      "Epoch 6, Batch: 23000| Training Loss: 2.638580253445584\n",
      "Epoch 6, Batch: 24000| Training Loss: 2.637031800394257\n",
      "Epoch 6, Batch: 25000| Training Loss: 2.6350659330511093\n",
      "Epoch 6, Training Loss: 2.634147851060714, Validation Error: 72.64696480981024, Validation Top-3 Accuracy: 48.118443611079506, Training Error: 69.65557955011226\n",
      "Epoch 7, Batch: 1000| Training Loss: 2.639013529062271\n",
      "Epoch 7, Batch: 2000| Training Loss: 2.632335887491703\n",
      "Epoch 7, Batch: 3000| Training Loss: 2.6334518223603567\n",
      "Epoch 7, Batch: 4000| Training Loss: 2.6345638402104377\n",
      "Epoch 7, Batch: 5000| Training Loss: 2.6274857970952987\n",
      "Epoch 7, Batch: 6000| Training Loss: 2.624602499385675\n",
      "Epoch 7, Batch: 7000| Training Loss: 2.621914629970278\n",
      "Epoch 7, Batch: 8000| Training Loss: 2.6191388222426175\n",
      "Epoch 7, Batch: 9000| Training Loss: 2.618940722372797\n",
      "Epoch 7, Batch: 10000| Training Loss: 2.616948248231411\n",
      "Epoch 7, Batch: 11000| Training Loss: 2.6151848739277233\n",
      "Epoch 7, Batch: 12000| Training Loss: 2.6126033655603726\n",
      "Epoch 7, Batch: 13000| Training Loss: 2.6115063859316017\n",
      "Epoch 7, Batch: 14000| Training Loss: 2.6106998269472803\n",
      "Epoch 7, Batch: 15000| Training Loss: 2.6090445515473686\n",
      "Epoch 7, Batch: 16000| Training Loss: 2.6066624905765057\n",
      "Epoch 7, Batch: 17000| Training Loss: 2.604841964819852\n",
      "Epoch 7, Batch: 18000| Training Loss: 2.603607551323043\n",
      "Epoch 7, Batch: 19000| Training Loss: 2.601259569419058\n",
      "Epoch 7, Batch: 20000| Training Loss: 2.6008927312552927\n",
      "Epoch 7, Batch: 21000| Training Loss: 2.5998962810096287\n",
      "Epoch 7, Batch: 22000| Training Loss: 2.598103395012292\n",
      "Epoch 7, Batch: 23000| Training Loss: 2.5960418548376665\n",
      "Epoch 7, Batch: 24000| Training Loss: 2.5946545254141093\n",
      "Epoch 7, Batch: 25000| Training Loss: 2.59269545627594\n",
      "Epoch 7, Training Loss: 2.591847537001842, Validation Error: 72.3578298898672, Validation Top-3 Accuracy: 48.34655508179474, Training Error: 69.11967845934554\n",
      "Epoch 8, Batch: 1000| Training Loss: 2.6064755642414092\n",
      "Epoch 8, Batch: 2000| Training Loss: 2.599916649401188\n",
      "Epoch 8, Batch: 3000| Training Loss: 2.6016660457452137\n",
      "Epoch 8, Batch: 4000| Training Loss: 2.602395138710737\n",
      "Epoch 8, Batch: 5000| Training Loss: 2.5955066236257553\n",
      "Epoch 8, Batch: 6000| Training Loss: 2.592838813463847\n",
      "Epoch 8, Batch: 7000| Training Loss: 2.5901660614865167\n",
      "Epoch 8, Batch: 8000| Training Loss: 2.58733392752707\n",
      "Epoch 8, Batch: 9000| Training Loss: 2.587564178850916\n",
      "Epoch 8, Batch: 10000| Training Loss: 2.5856376228809355\n",
      "Epoch 8, Batch: 11000| Training Loss: 2.5840525516379964\n",
      "Epoch 8, Batch: 12000| Training Loss: 2.5814534273942313\n",
      "Epoch 8, Batch: 13000| Training Loss: 2.5803328416164106\n",
      "Epoch 8, Batch: 14000| Training Loss: 2.5795928243398665\n",
      "Epoch 8, Batch: 15000| Training Loss: 2.577925791533788\n",
      "Epoch 8, Batch: 16000| Training Loss: 2.575623089849949\n",
      "Epoch 8, Batch: 17000| Training Loss: 2.573853932983735\n",
      "Epoch 8, Batch: 18000| Training Loss: 2.572701970981227\n",
      "Epoch 8, Batch: 19000| Training Loss: 2.5703974293156673\n",
      "Epoch 8, Batch: 20000| Training Loss: 2.5700979127943517\n",
      "Epoch 8, Batch: 21000| Training Loss: 2.5692663190251306\n",
      "Epoch 8, Batch: 22000| Training Loss: 2.5675760295174337\n",
      "Epoch 8, Batch: 23000| Training Loss: 2.565718868058661\n",
      "Epoch 8, Batch: 24000| Training Loss: 2.5643945365498464\n",
      "Epoch 8, Batch: 25000| Training Loss: 2.5625151151800156\n",
      "Epoch 8, Training Loss: 2.561702743463178, Validation Error: 72.16023014558452, Validation Top-3 Accuracy: 48.64150175771014, Training Error: 68.7438135847256\n",
      "Epoch 9, Batch: 1000| Training Loss: 2.5845847001075746\n",
      "Epoch 9, Batch: 2000| Training Loss: 2.577542078256607\n",
      "Epoch 9, Batch: 3000| Training Loss: 2.5787126348813376\n",
      "Epoch 9, Batch: 4000| Training Loss: 2.579888223975897\n",
      "Epoch 9, Batch: 5000| Training Loss: 2.573327467107773\n",
      "Epoch 9, Batch: 6000| Training Loss: 2.570638845205307\n",
      "Epoch 9, Batch: 7000| Training Loss: 2.5675133563791004\n",
      "Epoch 9, Batch: 8000| Training Loss: 2.5651421091258526\n",
      "Epoch 9, Batch: 9000| Training Loss: 2.5650546063582103\n",
      "Epoch 9, Batch: 10000| Training Loss: 2.563202931725979\n",
      "Epoch 9, Batch: 11000| Training Loss: 2.5614651249863885\n",
      "Epoch 9, Batch: 12000| Training Loss: 2.558929222593705\n",
      "Epoch 9, Batch: 13000| Training Loss: 2.5578428015525523\n",
      "Epoch 9, Batch: 14000| Training Loss: 2.5570287166322982\n",
      "Epoch 9, Batch: 15000| Training Loss: 2.55551614716053\n",
      "Epoch 9, Batch: 16000| Training Loss: 2.553303094774485\n",
      "Epoch 9, Batch: 17000| Training Loss: 2.551567654266077\n",
      "Epoch 9, Batch: 18000| Training Loss: 2.5503882195750873\n",
      "Epoch 9, Batch: 19000| Training Loss: 2.5481156351942764\n",
      "Epoch 9, Batch: 20000| Training Loss: 2.5478197799682616\n",
      "Epoch 9, Batch: 21000| Training Loss: 2.5470049419119243\n",
      "Epoch 9, Batch: 22000| Training Loss: 2.545286584165963\n",
      "Epoch 9, Batch: 23000| Training Loss: 2.5433601272158\n",
      "Epoch 9, Batch: 24000| Training Loss: 2.5420123490641515\n",
      "Epoch 9, Batch: 25000| Training Loss: 2.540224390501976\n",
      "Epoch 9, Training Loss: 2.5394104992192426, Validation Error: 72.09920669514428, Validation Top-3 Accuracy: 48.78824862662596, Training Error: 68.41976471739783\n",
      "Epoch 10, Batch: 1000| Training Loss: 2.56681403195858\n",
      "Epoch 10, Batch: 2000| Training Loss: 2.560149531841278\n",
      "Epoch 10, Batch: 3000| Training Loss: 2.561791538874308\n",
      "Epoch 10, Batch: 4000| Training Loss: 2.5628273230195044\n",
      "Epoch 10, Batch: 5000| Training Loss: 2.5566755803823473\n",
      "Epoch 10, Batch: 6000| Training Loss: 2.5542537104884784\n",
      "Epoch 10, Batch: 7000| Training Loss: 2.551227181008884\n",
      "Epoch 10, Batch: 8000| Training Loss: 2.5485607362091542\n",
      "Epoch 10, Batch: 9000| Training Loss: 2.548568366540803\n",
      "Epoch 10, Batch: 10000| Training Loss: 2.5467330105900765\n",
      "Epoch 10, Batch: 11000| Training Loss: 2.5449853281649677\n",
      "Epoch 10, Batch: 12000| Training Loss: 2.542459654589494\n",
      "Epoch 10, Batch: 13000| Training Loss: 2.5414291294996554\n",
      "Epoch 10, Batch: 14000| Training Loss: 2.5406890376039915\n",
      "Epoch 10, Batch: 15000| Training Loss: 2.5390507150014243\n",
      "Epoch 10, Batch: 16000| Training Loss: 2.5368099968954922\n",
      "Epoch 10, Batch: 17000| Training Loss: 2.535053325737224\n",
      "Epoch 10, Batch: 18000| Training Loss: 2.5339546286066374\n",
      "Epoch 10, Batch: 19000| Training Loss: 2.531782887007061\n",
      "Epoch 10, Batch: 20000| Training Loss: 2.5315011556088924\n",
      "Epoch 10, Batch: 21000| Training Loss: 2.530646411214556\n",
      "Epoch 10, Batch: 22000| Training Loss: 2.5289936655109577\n",
      "Epoch 10, Batch: 23000| Training Loss: 2.5271431494536607\n",
      "Epoch 10, Batch: 24000| Training Loss: 2.5257807784080506\n",
      "Epoch 10, Batch: 25000| Training Loss: 2.524061273508072\n",
      "Epoch 10, Training Loss: 2.5232615483487635, Validation Error: 72.00621858018772, Validation Top-3 Accuracy: 48.94371313131895, Training Error: 68.21042070682662\n",
      "Epoch 11, Batch: 1000| Training Loss: 2.553467501282692\n",
      "Epoch 11, Batch: 2000| Training Loss: 2.5473553225398065\n",
      "Epoch 11, Batch: 3000| Training Loss: 2.5490424966812135\n",
      "Epoch 11, Batch: 4000| Training Loss: 2.5502828776538373\n",
      "Epoch 11, Batch: 5000| Training Loss: 2.544268540596962\n",
      "Epoch 11, Batch: 6000| Training Loss: 2.541955498437087\n",
      "Epoch 11, Batch: 7000| Training Loss: 2.5392436019352504\n",
      "Epoch 11, Batch: 8000| Training Loss: 2.5366732833087444\n",
      "Epoch 11, Batch: 9000| Training Loss: 2.5366269638140997\n",
      "Epoch 11, Batch: 10000| Training Loss: 2.5349003220200537\n",
      "Epoch 11, Batch: 11000| Training Loss: 2.5331794048331\n",
      "Epoch 11, Batch: 12000| Training Loss: 2.530729010651509\n",
      "Epoch 11, Batch: 13000| Training Loss: 2.5298722680256915\n",
      "Epoch 11, Batch: 14000| Training Loss: 2.5290286399722097\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744238\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1331642\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # alpha can be set to a constant, or it can be a tensor of shape (num_classes,)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Prevents nans when probability 0\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 128\n",
    "d_embed = 64\n",
    "NUM_EPOCHS = 15\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModalThree(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(gamma=2, alpha=1, reduction='mean')\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.72 # This causes the LR to be 5e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [64, 1] but got: [64, 64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[45], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay)\u001b[0m\n\u001b[1;32m     22\u001b[0m boards, sequences, lengths, labels \u001b[38;5;241m=\u001b[39m boards\u001b[38;5;241m.\u001b[39mto(device), sequences\u001b[38;5;241m.\u001b[39mto(device), lengths, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backpropogate & Optimize\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 59\u001b[0m, in \u001b[0;36mMultiModalThree.forward\u001b[0;34m(self, board, sequence, seq_lengths)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Attention mechanism\u001b[39;00m\n\u001b[1;32m     58\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mbmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_query(seq_encoding)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_keys(cnn_encoding)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m attention_applied \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Combining both encodings with attention applied\u001b[39;00m\n\u001b[1;32m     62\u001b[0m combined_encoding \u001b[38;5;241m=\u001b[39m attention_applied\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [64, 1] but got: [64, 64]."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
