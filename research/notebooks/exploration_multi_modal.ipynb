{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Games of Elo ~1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import dask.dataframe as dd \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import chess\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      moves  white_elo  \\\n",
      "game_id                                                                  \n",
      "00VRM44j  e2e4 g8f6 e4e5 f6g8 f1b5 f7f6 e5e6 c7c6 e6d7 c...       1523   \n",
      "0MG5pz3L  c2c4 g7g6 e2e4 f8g7 d2d3 d7d6 f1e2 g8f6 g1f3 e...       1538   \n",
      "0MV7dmua  e2e4 e7e5 f2f4 e5f4 g1f3 d7d5 e4d5 d8d5 b1c3 d...       1587   \n",
      "0UNsyqEb  d2d4 d7d5 c2c4 g8f6 b1c3 c8f5 c1g5 e7e6 e2e3 h...       1507   \n",
      "10mypbCS  d2d4 g7g6 c1h6 f8h6 g1f3 g8f6 e2e3 b8c6 f1b5 d...       1500   \n",
      "...                                                     ...        ...   \n",
      "yckDW8pr  d2d4 d7d5 c1f4 c8f5 e2e3 e7e6 f1d3 f5g6 g1f3 g...       1520   \n",
      "yjFqLLIM  d2d4 g8f6 c2c4 e7e6 g1f3 d7d5 e2e3 c7c6 b2b3 f...       1525   \n",
      "yzgxpBPa  d2d4 d7d5 c2c4 e7e6 b1c3 g8f6 c1g5 f8b4 d1c2 b...       1532   \n",
      "zHpwiwWK  e2e4 c7c5 g1f3 b8c6 f1b5 d7d6 e1g1 e7e5 c2c3 g...       1571   \n",
      "zI8GQITc  e2e4 e7e5 g1f3 b8c6 f1b5 a7a6 b5a4 b7b5 a4b3 c...       1512   \n",
      "\n",
      "          black_elo  white_active  \\\n",
      "game_id                             \n",
      "00VRM44j       1500          True   \n",
      "0MG5pz3L       1580          True   \n",
      "0MV7dmua       1575          True   \n",
      "0UNsyqEb       1545          True   \n",
      "10mypbCS       1500          True   \n",
      "...             ...           ...   \n",
      "yckDW8pr       1500          True   \n",
      "yjFqLLIM       1572          True   \n",
      "yzgxpBPa       1509          True   \n",
      "zHpwiwWK       1557          True   \n",
      "zI8GQITc       1548          True   \n",
      "\n",
      "                                                      board  \n",
      "game_id                                                      \n",
      "00VRM44j  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0MG5pz3L  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0MV7dmua  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0UNsyqEb  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "10mypbCS  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                     ...  \n",
      "yckDW8pr  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yjFqLLIM  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yzgxpBPa  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zHpwiwWK  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zI8GQITc  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[164321 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import CSV File (from Maia: http://csslab.cs.toronto.edu/datasets/#monthly_chess_csv)\n",
    "# The CSV has 151,072,060 rows\n",
    "data_types ={'clock': 'float32',\n",
    "       'cp': 'object',\n",
    "       'opp_clock': 'float32',\n",
    "       'opp_clock_percent': 'float32'}\n",
    "df = dd.read_csv('../data/lichess_db_standard_rated_2019-01.csv', blocksize='64e6', dtype= data_types, low_memory=False)\n",
    "\n",
    "# Filter out quick games (Bullet and HyperBullet) and take out moves that happened in the last XX seconds (this won't affect how many games we import but the # of moves we look at)\n",
    "condition_time_control = ~df['time_control'].isin(['Bullet', 'HyperBullet'])\n",
    "condition_clock = df['clock'] > 45\n",
    "# condition_plays = df['num_ply'] < 80\n",
    "filtered_df = df[condition_time_control & condition_clock]\n",
    "\n",
    "# Select Relevant Columns\n",
    "selected_columns = ['game_id','white_elo','black_elo','move','white_active','board']\n",
    "filtered_df = filtered_df[selected_columns]\n",
    "\n",
    "# Filter only games of Elo 1100-1199\n",
    "filtered_df = filtered_df[(filtered_df['white_elo'].between(1500, 1599)) & (filtered_df['black_elo'].between(1500, 1599))]\n",
    "\n",
    "# Group Same Games Together \n",
    "def aggregate_moves(group):\n",
    "    moves = ' '.join(group['move'])  # Concatenate moves into a single string\n",
    "    white_elo = group['white_elo'].iloc[0]  # Get the first white_elo\n",
    "    black_elo = group['black_elo'].iloc[0]  # Get the first black_elo\n",
    "    white_active = group['white_active'].iloc[0]  # Get the first num_ply\n",
    "    board = '*'.join(group['board'])  # Get the first num_ply\n",
    "    return pd.Series({'moves': moves, 'white_elo': white_elo, 'black_elo': black_elo, 'white_active': white_active, 'board': board})\n",
    "\n",
    "grouped_df = filtered_df.groupby('game_id',sort=True).apply(aggregate_moves, meta={'moves': 'str', 'white_elo': 'int', 'black_elo': 'int', 'white_active': 'str', 'board': 'str'}).compute()\n",
    "\n",
    "# This gives us 99,300 Games when we don't filter games with more than 80 half-moves\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.to_csv('haha_longer_1500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      moves  white_elo  \\\n",
      "game_id                                                                  \n",
      "00VRM44j  e2e4 g8f6 e4e5 f6g8 f1b5 f7f6 e5e6 c7c6 e6d7 c...       1523   \n",
      "0MG5pz3L  c2c4 g7g6 e2e4 f8g7 d2d3 d7d6 f1e2 g8f6 g1f3 e...       1538   \n",
      "0MV7dmua  e2e4 e7e5 f2f4 e5f4 g1f3 d7d5 e4d5 d8d5 b1c3 d...       1587   \n",
      "0UNsyqEb  d2d4 d7d5 c2c4 g8f6 b1c3 c8f5 c1g5 e7e6 e2e3 h...       1507   \n",
      "10mypbCS  d2d4 g7g6 c1h6 f8h6 g1f3 g8f6 e2e3 b8c6 f1b5 d...       1500   \n",
      "...                                                     ...        ...   \n",
      "yckDW8pr  d2d4 d7d5 c1f4 c8f5 e2e3 e7e6 f1d3 f5g6 g1f3 g...       1520   \n",
      "yjFqLLIM  d2d4 g8f6 c2c4 e7e6 g1f3 d7d5 e2e3 c7c6 b2b3 f...       1525   \n",
      "yzgxpBPa  d2d4 d7d5 c2c4 e7e6 b1c3 g8f6 c1g5 f8b4 d1c2 b...       1532   \n",
      "zHpwiwWK  e2e4 c7c5 g1f3 b8c6 f1b5 d7d6 e1g1 e7e5 c2c3 g...       1571   \n",
      "zI8GQITc  e2e4 e7e5 g1f3 b8c6 f1b5 a7a6 b5a4 b7b5 a4b3 c...       1512   \n",
      "\n",
      "          black_elo  white_active  \\\n",
      "game_id                             \n",
      "00VRM44j       1500          True   \n",
      "0MG5pz3L       1580          True   \n",
      "0MV7dmua       1575          True   \n",
      "0UNsyqEb       1545          True   \n",
      "10mypbCS       1500          True   \n",
      "...             ...           ...   \n",
      "yckDW8pr       1500          True   \n",
      "yjFqLLIM       1572          True   \n",
      "yzgxpBPa       1509          True   \n",
      "zHpwiwWK       1557          True   \n",
      "zI8GQITc       1548          True   \n",
      "\n",
      "                                                      board  \n",
      "game_id                                                      \n",
      "00VRM44j  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0MG5pz3L  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0MV7dmua  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0UNsyqEb  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "10mypbCS  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                     ...  \n",
      "yckDW8pr  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yjFqLLIM  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yzgxpBPa  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zHpwiwWK  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zI8GQITc  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[164321 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw data, we need to be able to make sense of chess moves. Meaning, we're transforming our entire world from chess moves into numerical tokens that will serve as indices into unique embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a mapping from each move to a unique embedding. In order to index into our matrix of \n",
    "# embeddings (matrix format so it's something we can tune), we'll also want a mapping from each move to a unique ID\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.move_to_id = {\"<UNK>\": 0}\n",
    "        self.id_to_move = {0: \"<UNK>\"}\n",
    "        self.index = 1  # Start indexing from 1\n",
    "\n",
    "    def add_move(self, move):\n",
    "        if move not in self.move_to_id:\n",
    "            self.move_to_id[move] = self.index\n",
    "            self.id_to_move[self.index] = move\n",
    "            self.index += 1\n",
    "\n",
    "    def get_id(self, move):\n",
    "        return self.move_to_id.get(move, self.move_to_id[\"<UNK>\"])\n",
    "\n",
    "    def get_move(self, id):\n",
    "        return self.id_to_move.get(id, self.id_to_move[0])\n",
    "\n",
    "# We can just use nn.Embedding later when we pass the model a sequence of indices, but this is if we ever want to pre-train and have access to the matrix we've trained\n",
    "def get_embedding_matrix(vocab, d_embed):\n",
    "    n_embed = len(vocab.move_to_id)\n",
    "    return np.random.normal(0, 1, (n_embed, d_embed))\n",
    "# embedding_matrix = get_embedding_matrix(vocab, 64)\n",
    "    \n",
    "piece_to_index = {\n",
    "    'p' : 0,\n",
    "    'r' : 1,\n",
    "    'b' : 2,\n",
    "    'n' : 3,\n",
    "    'q' : 4,\n",
    "    'k' : 5,\n",
    "}\n",
    "\n",
    "def string_to_array(string):\n",
    "    rows = string.split(\"/\")\n",
    "    ans = [[[0 for a in range(8)] for b in range(8)] for c in range(6)]\n",
    "    for row in range(8):\n",
    "        curr_row = rows[row]\n",
    "        #print(curr_row)\n",
    "        offset = 0\n",
    "        for piece in range(len(curr_row)):\n",
    "            curr_piece = curr_row[piece]\n",
    "            sign = 1 if curr_piece.lower() == curr_piece else -1 # check if the piece is capitalized\n",
    "            curr_piece = curr_piece.lower() # after storing whether or not capitalized, standardize it to lower case for easy processing\n",
    "            if curr_piece not in piece_to_index.keys():\n",
    "                offset += int(curr_piece) - 1\n",
    "            else:\n",
    "                current_board = ans[piece_to_index[curr_piece]]\n",
    "                current_board[row][offset + piece] = 1 * sign\n",
    "                ans[piece_to_index[curr_piece]] = current_board\n",
    "    return ans\n",
    "\n",
    "# Function to pad move sequences & get their sequence lengths\n",
    "def pad_sequences(sequences, max_len=None, pad_id=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = np.full((len(sequences), max_len), pad_id, dtype=int)\n",
    "    sequence_lengths = np.zeros(len(sequences), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = seq[:length]\n",
    "        sequence_lengths[i] = length\n",
    "    return padded_sequences, sequence_lengths\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, sequences, boards, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.boards = boards\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        boards, sequences, lengths, labels = self.boards[idx], self.sequences[idx], self.lengths[idx], self.labels[idx]\n",
    "        return torch.tensor(boards, dtype=torch.float32), torch.tensor(sequences, dtype=torch.long), torch.tensor(lengths, dtype=torch.long), torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_data(df, sampling_rate=1, fixed_window = True, fixed_window_size = 16, algebraic_notation=True):\n",
    "    \"\"\"\n",
    "    Input: Dataframe of training data in which each row represents a full game played between players\n",
    "    Output: List in which each item represents some game's history up until a particular move, List in the same order in which the associated label is the following move\n",
    "    \"\"\"\n",
    "    board_states = []\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "    vocab = Vocabulary()\n",
    "    chess_board = chess.Board()\n",
    "    for game_board, game_moves in zip(df['board'],df['moves']):\n",
    "        moves = game_moves.split()\n",
    "        boards = game_board.split('*')\n",
    "        # Encode the moves into SAN notation and then into corresponding indices\n",
    "        encoded_moves = []\n",
    "        for move in moves:\n",
    "            # Create a move object from the coordinate notation\n",
    "            move_obj = chess.Move.from_uci(move)\n",
    "            if move_obj not in chess_board.legal_moves:\n",
    "                break \n",
    "            else:\n",
    "                if algebraic_notation:\n",
    "                    algebraic_move = chess_board.san(move_obj)\n",
    "                    chess_board.push(move_obj)\n",
    "                    vocab.add_move(algebraic_move)\n",
    "                    encoded_move = vocab.get_id(algebraic_move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "                else:\n",
    "                    encoded_move = vocab.get_id(move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "        chess_board.reset()\n",
    "        boards = boards[:len(encoded_moves)]\n",
    "        # Now generate X,Y with sampling\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            #TODO: Figure out how to deal with black orientation 'seeing' a different board\n",
    "            if random.uniform(0, 1) <= sampling_rate and 'w' in boards[i]:\n",
    "                # Board\n",
    "                board_states.append(string_to_array(boards[i].split(' ')[0]))\n",
    "                # Sequence of Moves\n",
    "                subseq = encoded_moves[0:i+1]\n",
    "                if fixed_window and len(subseq) > fixed_window_size:\n",
    "                    subseq = subseq[-fixed_window_size:]\n",
    "                subsequences.append(subseq)\n",
    "                # Label\n",
    "                label = encoded_moves[i+1]\n",
    "                next_moves.append(label)\n",
    "\n",
    "    return subsequences, board_states, next_moves, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_sequences, trainX_boards, trainY, vocab = df_to_data(grouped_df, fixed_window=True, sampling_rate=0.5)\n",
    "trainX_sequences, trainX_seqlengths  = pad_sequences(trainX_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6431\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.id_to_move.keys()))\n",
    "print(len(trainX_sequences[140]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2142188\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX_boards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModal(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn = RNNModel(vocab,d_embed, d_hidden, 16, dropout=dropout)\n",
    "        self.cnn = ChessCNN(16)\n",
    "        self.fc = nn.Linear(16*2,d_out)\n",
    "\n",
    "    def forward(self, board, sequence, seq_lengths):\n",
    "        seq_encoding = self.rnn(sequence, seq_lengths)\n",
    "        cnn_encoding = self.cnn(board)\n",
    "        pred = self.fc(torch.cat((seq_encoding,cnn_encoding),dim=1))\n",
    "        return pred\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5, num_layers = 2, bidirectional = False, embedding_matrix = None):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(vocab.move_to_id), d_embed)\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(d_embed, d_hidden, dropout = dropout, bidirectional=bidirectional, num_layers = num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_hidden,d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = self.embeddings(x)\n",
    "        # Sort x and seq_lengths in descending order\n",
    "        # This is required for packing the sequence\n",
    "        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        x = x[perm_idx]\n",
    "        # Pack the sequence\n",
    "        packed_input = pack_padded_sequence(x, seq_lengths, batch_first=True)\n",
    "        # Pass the packed sequence through the LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        # Unpack the sequence\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True,total_length = x.size()[1])\n",
    "        _, unperm_idx = perm_idx.sort(0)\n",
    "        #unperm_idx = unperm_idx.to(self.device)\n",
    "        output = output.index_select(0, unperm_idx)\n",
    "        #This takes all the outputs across the cells\n",
    "        mean_pooled = torch.mean(output, dim=1)\n",
    "        #output = torch.cat((mean_pooled,hidden[-1]),dim=1)\n",
    "        output = self.fc(mean_pooled)\n",
    "        return output\n",
    "\n",
    "        \n",
    "class ChessCNN(nn.Module):\n",
    "    def __init__(self, d_out):\n",
    "        super(ChessCNN, self).__init__()\n",
    "        # Assuming each channel represents a different piece type (e.g., 6 channels for 6 types each)\n",
    "        self.conv1 = nn.Conv2d(6, 12, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(12)  # Batch normalization for first conv layer\n",
    "        self.conv2 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization for second conv layer\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Assuming an 8x8 chess board\n",
    "        self.fc2 = nn.Linear(128, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, followed by batch norm, then ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Apply second convolution, followed by batch norm, then ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate top-3 accuracy\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    top3 = torch.topk(y_pred, 3, dim=1).indices\n",
    "    correct = top3.eq(y_true.view(-1, 1).expand_as(top3))\n",
    "    return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "def train(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_start = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for boards, sequences, lengths, labels in train_loader:\n",
    "            count += 1\n",
    "            boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            # Forward Pass\n",
    "            output = model(boards, sequences, lengths)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        if epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                validation_loss = 0\n",
    "\n",
    "                for boards, sequences, lengths, labels in val_loader:\n",
    "                    boards, sequences, lengths, labels = boards.to(device), sequences.to(device), lengths.to(device), labels.to(device)\n",
    "                    outputs = model(boards, sequences, lengths)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    val_top3_correct += top_3_accuracy(labels, outputs) * labels.size(0)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    validation_loss += loss.item()\n",
    "\n",
    "                val_loss_values.append(validation_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values, swa_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2142188\n"
     ]
    }
   ],
   "source": [
    "dataset = MultimodalDataset(trainX_sequences, trainX_boards, trainX_seqlengths, trainY)\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2035078\n"
     ]
    }
   ],
   "source": [
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011691\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 64\n",
    "d_embed = 32\n",
    "NUM_EPOCHS = 6\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = MultiModal(vocab,d_embed,d_hidden,d_out) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.7 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 5.383534812688827\n",
      "Epoch 1, Batch: 2000| Training Loss: 5.159182703375817\n",
      "Epoch 1, Batch: 3000| Training Loss: 5.028050996859869\n",
      "Epoch 1, Batch: 4000| Training Loss: 4.930243832886219\n",
      "Epoch 1, Batch: 5000| Training Loss: 4.854028081703186\n",
      "Epoch 1, Batch: 6000| Training Loss: 4.789974905451139\n",
      "Epoch 1, Batch: 7000| Training Loss: 4.72769423862866\n",
      "Epoch 1, Batch: 8000| Training Loss: 4.675290651649236\n",
      "Epoch 1, Batch: 9000| Training Loss: 4.630269824610816\n",
      "Epoch 1, Batch: 10000| Training Loss: 4.589820999884606\n",
      "Epoch 1, Batch: 11000| Training Loss: 4.554073373881254\n",
      "Epoch 1, Batch: 12000| Training Loss: 4.520208475450675\n",
      "Epoch 1, Batch: 13000| Training Loss: 4.491328831727688\n",
      "Epoch 1, Batch: 14000| Training Loss: 4.465768239617348\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value,swa_model = train(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we filter out illegal moves in our prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_legal_move(chess_board, move_san):\n",
    "    try:\n",
    "        chess_move = chess_board.parse_san(move_san)\n",
    "        return chess_move in chess_board.legal_moves\n",
    "    except ValueError:\n",
    "        # This handles cases where the SAN move cannot be parsed or is not legal\n",
    "        return False\n",
    "\n",
    "def load_board_state_from_san(moves):\n",
    "    board = chess.Board()\n",
    "    for index in moves:\n",
    "        try:\n",
    "            if index == 0:\n",
    "                return board\n",
    "            else:\n",
    "                move_san = vocab.get_move(index.item())\n",
    "                move = board.parse_san(move_san)\n",
    "                board.push(move)\n",
    "        except ValueError:\n",
    "            # Handle invalid moves, e.g., break the loop or log an error\n",
    "            break\n",
    "    return board\n",
    "\n",
    "val_size = int(total_size * 0.04)\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "if val_loader is not None:\n",
    "    with torch.no_grad():\n",
    "        for sequences, lengths, labels in val_loader:\n",
    "            print(\"hey\")\n",
    "            sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            outputs = model(sequences, lengths)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            minus = 0\n",
    "            for idx, (sequence, label) in enumerate(zip(sequences, labels)):\n",
    "                if sequence[-1].item() == 0 and sequence[2].item() != 0 and sequence[3].item() != 0 and sequence[4].item() != 0:\n",
    "                    output = probabilities[idx]\n",
    "                    sorted_probs, sorted_indices = torch.sort(output, descending=True)\n",
    "                    predicted_move = sorted_indices[0]\n",
    "                    print(predicted_move)\n",
    "                    chess_board = load_board_state_from_san(sequence)\n",
    "                    for move_idx in sorted_indices:\n",
    "                        move = vocab.get_move(move_idx.item()) # Convert index to move (e.g., 'e2e4')\n",
    "                        if is_legal_move(chess_board, move):\n",
    "                            print(\"we found one\")\n",
    "                            predicted_move = vocab.get_id(move)\n",
    "                            break\n",
    "                    \n",
    "                    # Check if predicted move is correct\n",
    "                    correct_move = label.item() # Convert label to move\n",
    "                    print(correct_move)\n",
    "                    if predicted_move == correct_move:\n",
    "                        val_correct += 1\n",
    "                else:\n",
    "                    minus += 1\n",
    "            val_total += (labels.size(0) - minus)\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_accuracy}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
