{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "import dask.dataframe as dd \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "import pickle\n",
    "import utils\n",
    "import models\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         game_id                                              moves  \\\n",
      "0       00VRM44j  e2e4 g8f6 e4e5 f6g8 f1b5 f7f6 e5e6 c7c6 e6d7 c...   \n",
      "1       0MG5pz3L  c2c4 g7g6 e2e4 f8g7 d2d3 d7d6 f1e2 g8f6 g1f3 e...   \n",
      "2       0MV7dmua  e2e4 e7e5 f2f4 e5f4 g1f3 d7d5 e4d5 d8d5 b1c3 d...   \n",
      "3       0UNsyqEb  d2d4 d7d5 c2c4 g8f6 b1c3 c8f5 c1g5 e7e6 e2e3 h...   \n",
      "4       10mypbCS  d2d4 g7g6 c1h6 f8h6 g1f3 g8f6 e2e3 b8c6 f1b5 d...   \n",
      "...          ...                                                ...   \n",
      "466483  yT5CKw8c  g1f3 e7e6 b1c3 d7d5 d2d4 g8f6 c1g5 c7c5 e2e3 h...   \n",
      "466484  yxAYsxBu  e2e4 e7e5 d2d4 e5d4 c2c3 d4c3 f1c4 c3b2 c1b2 f...   \n",
      "466485  zLOF4boA  d2d4 c7c5 d4c5 b8c6 a2a3 g7g6 g1f3 f8g7 c2c3 e...   \n",
      "466486  zimmRKoi  e2e4 e7e5 g1f3 d7d6 f1c4 c7c6 h2h3 d8e7 e1g1 c...   \n",
      "466487  zzidHYZH  e2e4 e7e6 d2d4 d7d5 e4e5 c7c5 c2c3 d8b6 g1f3 b...   \n",
      "\n",
      "        white_elo  black_elo  white_active  \\\n",
      "0            1523       1500          True   \n",
      "1            1538       1580          True   \n",
      "2            1587       1575          True   \n",
      "3            1507       1545          True   \n",
      "4            1500       1500          True   \n",
      "...           ...        ...           ...   \n",
      "466483       1537       1598          True   \n",
      "466484       1569       1536          True   \n",
      "466485       1535       1594          True   \n",
      "466486       1587       1536          True   \n",
      "466487       1597       1587          True   \n",
      "\n",
      "                                                    board  \n",
      "0       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "1       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "3       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "4       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                   ...  \n",
      "466483  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "466484  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "466485  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "466486  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "466487  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[466488 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "grouped_df = pd.read_csv(\"haha-longer-mid.csv\")\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_sequences, trainX, trainY, vocab = df_to_data(grouped_df, fixed_window=True, sampling_rate=0.25)\n",
    "trainX_sequences, trainX_seqlengths  = pad_sequences(trainX_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainX_sequences, trainX_seqlengths, trainY = np.array(trainX), np.array(trainX_sequences), np.array(trainX_seqlengths), np.array(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step of saving our data onto our disk so that it's ready to load as we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_memmap(array, filename):\n",
    "    # Determine the dtype and shape of the array to create a compatible memmap\n",
    "    dtype = array.dtype\n",
    "    shape = array.shape\n",
    "    \n",
    "    # Create a memmap file with write mode, which will also allocate the disk space\n",
    "    memmap_array = np.memmap(filename, dtype=dtype, mode='w+', shape=shape)\n",
    "    \n",
    "    # Copy the data into the memmap array\n",
    "    memmap_array[:] = array[:]\n",
    "    \n",
    "    # Flush memory changes to disk to ensure all data is written\n",
    "    memmap_array.flush()\n",
    "\n",
    "    # Return the path for confirmation\n",
    "    return filename\n",
    "\n",
    "# Save each array as a memmap file\n",
    "filenames = [\n",
    "    save_as_memmap(trainX_sequences, './../data/jan-march/trainX_sequences.memmap'),\n",
    "    save_as_memmap(trainX, './../data/jan-march/trainX.memmap'),\n",
    "    save_as_memmap(trainY, './../data/jan-march/trainY.memmap'),\n",
    "    save_as_memmap(trainX_seqlengths, './../data/jan-march/trainX_seqlengths.memmap')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/jan-march/vocab.pkl', 'wb') as outp:\n",
    "    pickle.dump(vocab, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished processing the data, let's now load in the data (assuming we're starting from fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "\n",
    "# Assuming you know the dtype and shape, for example:\n",
    "# For trainX_sequences\n",
    "dtype_trainX_sequences = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_sequences = (3038976, 16)  # replace with the correct shape\n",
    "trainX_sequences = load_memmap('./../data/jan-march/trainX_sequences.memmap', dtype_trainX_sequences, shape_trainX_sequences)\n",
    "\n",
    "# For trainX\n",
    "dtype_trainX = np.int64  # or the correct dtype for your data\n",
    "shape_trainX = (3038976, 12, 8, 8)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/jan-march/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int64  # or the correct dtype for your data\n",
    "shape_trainY = (3038976,)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/jan-march/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "# For trainX_seqlengths\n",
    "dtype_trainX_seqlengths = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_seqlengths = (3038976,)  # replace with the correct shape\n",
    "trainX_seqlengths = load_memmap('./../data/jan-march/trainX_seqlengths.memmap', dtype_trainX_seqlengths, shape_trainX_seqlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/jan-march/vocab.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8121\n",
      "torch.Size([12, 8, 8])\n",
      "[[[0 0 0 0 0 0 0 0]\n",
      "  [1 1 0 0 1 0 1 1]\n",
      "  [0 0 1 0 0 1 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 1 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 1 0 0]\n",
      "  [0 0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [1 1 1 1 0 1 1 1]\n",
      "  [0 0 0 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, sequences, boards, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.boards = boards\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.boards[idx], dtype=torch.float32),\n",
    "            torch.from_numpy(self.sequences[idx], dtype=torch.long),\n",
    "            torch.from_numpy(self.lengths[idx], dtype=torch.long),\n",
    "            torch.from_numpy(self.labels[idx], dtype=torch.long),\n",
    "        )\n",
    "print(len(vocab.id_to_move.keys()))\n",
    "print(torch.tensor(trainX[0]).shape)\n",
    "print(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3038976\n"
     ]
    }
   ],
   "source": [
    "dataset = ChessDataset(trainX, trainY)\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(total_size * 0.04)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
