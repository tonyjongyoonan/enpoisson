{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/ubuntu/enpoisson/research/notebooks']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3278\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3275\u001b[0m _run_async \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 3278\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3280\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3281\u001b[0m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3282\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/IPython/core/compilerop.py:155\u001b[0m, in \u001b[0;36mCachingCompiler.cache\u001b[0;34m(self, transformed_code, number, raw_code)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m transformed_code\n\u001b[0;32m--> 155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_map[name] \u001b[38;5;241m=\u001b[39m number\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipykernel/compiler.py:105\u001b[0m, in \u001b[0;36mXCachingCompiler.get_code_name\u001b[0;34m(self, raw_code, code, number)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipykernel/compiler.py:91\u001b[0m, in \u001b[0;36mget_file_name\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     name \u001b[38;5;241m=\u001b[39m murmur2_x86(code, get_tmp_hash_seed())\n\u001b[0;32m---> 91\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipykernel/compiler.py:76\u001b[0m, in \u001b[0;36mget_tmp_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tmp_directory\u001b[39m():\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     tmp_dir \u001b[38;5;241m=\u001b[39m convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     pid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/tempfile.py:315\u001b[0m, in \u001b[0;36mgettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgettempdir\u001b[39m():\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns tempfile.tempdir as str.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mfsdecode(\u001b[43m_gettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/tempfile.py:308\u001b[0m, in \u001b[0;36m_gettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m         tempdir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     _once_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/tempfile.py:223\u001b[0m, in \u001b[0;36m_get_default_tempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    224\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    225\u001b[0m                         dirlist)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/ubuntu/enpoisson/research/notebooks']"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "import pickle\n",
    "import utils\n",
    "import models\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         game_id                                              moves  \\\n",
      "0       02sLP1mI  e2e4 c7c6 g1f3 d7d6 d2d4 d8c7 f1d3 b8d7 c1f4 e...   \n",
      "1       0Ckuld9I  e2e4 e7e5 f1c4 g8f6 d2d4 d7d6 d4e5 d6e5 c4f7 e...   \n",
      "2       0ETbdcMb  e2e3 b7b6 f1c4 c8b7 d1h5 g7g6 h5g4 g8f6 g4g3 f...   \n",
      "3       0XBDxqJt  e2e4 d7d6 g1f3 g8f6 b1c3 g7g6 d2d4 f8g7 c1f4 e...   \n",
      "4       0gG0BpHe  e2e4 c7c5 f1c4 e7e6 g1f3 d7d5 e4d5 e6d5 d1e2 c...   \n",
      "...          ...                                                ...   \n",
      "464727  z6BD6FH8  d2d4 d7d5 e2e4 d5e4 b1c3 c8f5 f2f3 e4f3 d1f3 f...   \n",
      "464728  z7OBa02Q  d2d4 g8f6 e2e3 g7g6 c2c4 f8g7 b1c3 e8g8 g1f3 d...   \n",
      "464729  zLfSFBtJ  e2e4 e7e5 g1f3 b8c6 d2d4 e5d4 f3d4 c6d4 d1d4 d...   \n",
      "464730  zWDYo2XR  e2e3 d7d5 d1f3 g8f6 b2b3 c7c6 c1b2 e7e6 f1e2 e...   \n",
      "464731  zjh7MyLr  e2e4 c7c5 g1f3 b8c6 f1b5 g8f6 b5c6 d7c6 b1c3 c...   \n",
      "\n",
      "        white_elo  black_elo  white_active  \\\n",
      "0            1595       1533          True   \n",
      "1            1577       1547          True   \n",
      "2            1500       1501          True   \n",
      "3            1571       1589          True   \n",
      "4            1592       1598          True   \n",
      "...           ...        ...           ...   \n",
      "464727       1581       1594          True   \n",
      "464728       1594       1577          True   \n",
      "464729       1545       1562          True   \n",
      "464730       1503       1502          True   \n",
      "464731       1538       1510          True   \n",
      "\n",
      "                                                    board  \n",
      "0       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "1       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "3       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "4       rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                   ...  \n",
      "464727  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "464728  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "464729  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "464730  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "464731  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[464732 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "grouped_df = pd.read_csv(\"../data/haha-longer-oct-dec.csv\")\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this when combining memmaps\n",
    "with open('./../data/black-data/july-sept/vocab.pkl', 'rb') as inp:\n",
    "    vocab_old = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_memmap(array, filename):\n",
    "    # Determine the dtype and shape of the array to create a compatible memmap\n",
    "    dtype = array.dtype\n",
    "    shape = array.shape\n",
    "    \n",
    "    # Create a memmap file with write mode, which will also allocate the disk space\n",
    "    memmap_array = np.memmap(filename, dtype=dtype, mode='w+', shape=shape)\n",
    "    \n",
    "    # Copy the data into the memmap array\n",
    "    memmap_array[:] = array[:]\n",
    "    \n",
    "    # Flush memory changes to disk to ensure all data is written\n",
    "    memmap_array.flush()\n",
    "\n",
    "    # Return the path for confirmation\n",
    "    return filename\n",
    "\n",
    "# If you forget the shape\n",
    "def find_working_shape(filename, dtype, max_first_dim, other_dims):\n",
    "\n",
    "    # Try decreasing sizes from the max_first_dim until we find a working shape\n",
    "    for first_dim in range(max_first_dim, 0, -1):\n",
    "        shape_to_try = (first_dim,) + other_dims\n",
    "        \n",
    "        try:\n",
    "            # Attempt to load the memmap with the current shape\n",
    "            memmap_array = np.memmap(filename, dtype=dtype, mode='r', shape=shape_to_try)\n",
    "            # If successful, return the array\n",
    "            print(f\"Successful shape: {shape_to_try}\")\n",
    "            return memmap_array\n",
    "        except ValueError as e:\n",
    "            # Catch the ValueError if the shape is not feasible, and try the next size\n",
    "            continue\n",
    "    \n",
    "    raise ValueError(\"Could not find a working shape within the given bounds.\")\n",
    "\n",
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, vocab = df_to_data_fen_only_padded(grouped_df, fixed_window=True, sampling_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2524"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 6, 4, 0, 0, 0, 0, 0, 0], [3, 31, 4, 0, 0, 0, 0, 0, 0], [3, 35, 36, 37, 4, 0, 0, 0, 0], [3, 43, 37, 4, 0, 0, 0, 0, 0], [3, 58, 4, 0, 0, 0, 0, 0, 0], [3, 26, 4, 0, 0, 0, 0, 0, 0], [3, 69, 37, 4, 0, 0, 0, 0, 0], [3, 42, 4, 0, 0, 0, 0, 0, 0], [3, 6, 4, 0, 0, 0, 0, 0, 0], [3, 77, 4, 0, 0, 0, 0, 0, 0], [3, 81, 4, 0, 0, 0, 0, 0, 0], [3, 85, 4, 0, 0, 0, 0, 0, 0], [3, 91, 37, 4, 0, 0, 0, 0, 0], [3, 92, 37, 4, 0, 0, 0, 0, 0], [3, 61, 37, 4, 0, 0, 0, 0, 0], [3, 94, 37, 4, 0, 0, 0, 0, 0], [3, 97, 37, 4, 0, 0, 0, 0, 0], [3, 98, 4, 0, 0, 0, 0, 0, 0], [3, 106, 4, 0, 0, 0, 0, 0, 0], [3, 107, 37, 4, 0, 0, 0, 0, 0], [3, 53, 4, 0, 0, 0, 0, 0, 0], [3, 52, 4, 0, 0, 0, 0, 0, 0], [3, 46, 4, 0, 0, 0, 0, 0, 0], [3, 57, 4, 0, 0, 0, 0, 0, 0], [3, 114, 4, 0, 0, 0, 0, 0, 0], [3, 54, 4, 0, 0, 0, 0, 0, 0], [3, 115, 4, 0, 0, 0, 0, 0, 0], [3, 48, 4, 0, 0, 0, 0, 0, 0], [3, 118, 37, 4, 0, 0, 0, 0, 0], [3, 64, 4, 0, 0, 0, 0, 0, 0], [3, 123, 36, 37, 4, 0, 0, 0, 0], [3, 128, 4, 0, 0, 0, 0, 0, 0], [3, 132, 36, 37, 4, 0, 0, 0, 0], [3, 56, 4, 0, 0, 0, 0, 0, 0], [3, 64, 4, 0, 0, 0, 0, 0, 0], [3, 67, 36, 4, 0, 0, 0, 0, 0], [3, 133, 36, 4, 0, 0, 0, 0, 0], [3, 138, 36, 4, 0, 0, 0, 0, 0], [3, 141, 36, 37, 4, 0, 0, 0, 0], [3, 149, 36, 37, 4, 0, 0, 0, 0], [3, 156, 4, 0, 0, 0, 0, 0, 0], [3, 75, 4, 0, 0, 0, 0, 0, 0], [3, 141, 4, 0, 0, 0, 0, 0, 0], [3, 49, 4, 0, 0, 0, 0, 0, 0], [3, 163, 4, 0, 0, 0, 0, 0, 0], [3, 163, 37, 4, 0, 0, 0, 0, 0], [3, 56, 4, 0, 0, 0, 0, 0, 0], [3, 78, 4, 0, 0, 0, 0, 0, 0], [3, 75, 4, 0, 0, 0, 0, 0, 0], [3, 164, 4, 0, 0, 0, 0, 0, 0], [3, 165, 4, 0, 0, 0, 0, 0, 0], [3, 6, 4, 0, 0, 0, 0, 0, 0], [3, 69, 37, 4, 0, 0, 0, 0, 0], [3, 166, 4, 0, 0, 0, 0, 0, 0], [3, 167, 4, 0, 0, 0, 0, 0, 0], [3, 168, 4, 0, 0, 0, 0, 0, 0], [3, 170, 37, 4, 0, 0, 0, 0, 0], [3, 118, 37, 4, 0, 0, 0, 0, 0], [3, 172, 37, 4, 0, 0, 0, 0, 0], [3, 174, 4, 0, 0, 0, 0, 0, 0], [3, 179, 37, 4, 0, 0, 0, 0, 0], [3, 90, 4, 0, 0, 0, 0, 0, 0], [3, 182, 37, 4, 0, 0, 0, 0, 0], [3, 115, 4, 0, 0, 0, 0, 0, 0], [3, 81, 4, 0, 0, 0, 0, 0, 0], [3, 184, 4, 0, 0, 0, 0, 0, 0], [3, 49, 4, 0, 0, 0, 0, 0, 0], [3, 78, 4, 0, 0, 0, 0, 0, 0], [3, 164, 4, 0, 0, 0, 0, 0, 0], [3, 32, 4, 0, 0, 0, 0, 0, 0], [3, 61, 4, 0, 0, 0, 0, 0, 0], [3, 92, 4, 0, 0, 0, 0, 0, 0], [3, 56, 4, 0, 0, 0, 0, 0, 0], [3, 189, 4, 0, 0, 0, 0, 0, 0], [3, 169, 4, 0, 0, 0, 0, 0, 0], [3, 193, 36, 4, 0, 0, 0, 0, 0], [3, 26, 4, 0, 0, 0, 0, 0, 0], [3, 78, 4, 0, 0, 0, 0, 0, 0], [3, 65, 37, 4, 0, 0, 0, 0, 0], [3, 197, 37, 4, 0, 0, 0, 0, 0], [3, 25, 4, 0, 0, 0, 0, 0, 0], [3, 110, 4, 0, 0, 0, 0, 0, 0], [3, 46, 4, 0, 0, 0, 0, 0, 0], [3, 53, 4, 0, 0, 0, 0, 0, 0], [3, 203, 4, 0, 0, 0, 0, 0, 0], [3, 205, 4, 0, 0, 0, 0, 0, 0], [3, 208, 4, 0, 0, 0, 0, 0, 0], [3, 178, 4, 0, 0, 0, 0, 0, 0], [3, 131, 4, 0, 0, 0, 0, 0, 0], [3, 213, 4, 0, 0, 0, 0, 0, 0], [3, 81, 4, 0, 0, 0, 0, 0, 0], [3, 110, 4, 0, 0, 0, 0, 0, 0], [3, 25, 4, 0, 0, 0, 0, 0, 0], [3, 164, 4, 0, 0, 0, 0, 0, 0], [3, 214, 4, 0, 0, 0, 0, 0, 0], [3, 52, 4, 0, 0, 0, 0, 0, 0], [3, 48, 4, 0, 0, 0, 0, 0, 0], [3, 220, 4, 0, 0, 0, 0, 0, 0], [3, 172, 37, 4, 0, 0, 0, 0, 0], [3, 70, 4, 0, 0, 0, 0, 0, 0], [3, 224, 4, 0, 0, 0, 0, 0, 0], [3, 226, 37, 4, 0, 0, 0, 0, 0], [3, 40, 37, 4, 0, 0, 0, 0, 0], [3, 231, 36, 37, 4, 0, 0, 0, 0], [3, 233, 4, 0, 0, 0, 0, 0, 0], [3, 237, 36, 4, 0, 0, 0, 0, 0], [3, 6, 4, 0, 0, 0, 0, 0, 0], [3, 26, 4, 0, 0, 0, 0, 0, 0], [3, 190, 4, 0, 0, 0, 0, 0, 0], [3, 242, 36, 37, 4, 0, 0, 0, 0], [3, 244, 4, 0, 0, 0, 0, 0, 0], [3, 124, 4, 0, 0, 0, 0, 0, 0], [3, 248, 4, 0, 0, 0, 0, 0, 0], [3, 114, 4, 0, 0, 0, 0, 0, 0], [3, 183, 4, 0, 0, 0, 0, 0, 0], [3, 188, 4, 0, 0, 0, 0, 0, 0], [3, 209, 37, 4, 0, 0, 0, 0, 0], [3, 49, 4, 0, 0, 0, 0, 0, 0], [3, 213, 4, 0, 0, 0, 0, 0, 0], [3, 178, 4, 0, 0, 0, 0, 0, 0], [3, 259, 4, 0, 0, 0, 0, 0, 0], [3, 152, 4, 0, 0, 0, 0, 0, 0], [3, 105, 4, 0, 0, 0, 0, 0, 0], [3, 77, 4, 0, 0, 0, 0, 0, 0], [3, 190, 4, 0, 0, 0, 0, 0, 0], [3, 245, 4, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = np.asarray(trainX, dtype=np.int32), np.asarray(trainY, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for transformer data\n",
    "filenames = [\n",
    "    save_as_memmap(trainX, './../data/transformer/jan-march/trainX.memmap'),\n",
    "    save_as_memmap(trainY, './../data/transformer/jan-march/trainY.memmap'),\n",
    "]\n",
    "with open('./../data/transformer/jan-march/vocab.pkl', 'wb') as outp:\n",
    "    pickle.dump(vocab, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for lengths so we're able to load them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6130889, 585)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trainX\n",
    "dtype_trainX = np.int32  # or the correct dtype for your data\n",
    "shape_trainX = (2161324, 585)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/transformer/jan/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int32 # or the correct dtype for your data\n",
    "shape_trainY = (2161324, 6)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/transformer/jan/trainY.memmap', dtype_trainY, shape_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  7  8  9 10 11  9  8  7 12 13 13 13 13 13 13 13 13 12 14 14 14 14\n",
      " 14 14 14 14 12 14 14 14 14 14 14 14 14 12 14 14 14 14 14 14 14 14 12 14\n",
      " 14 14 14 14 14 14 14 12 15 15 15 15 15 15 15 15 12 16 17 18 19 20 18 17\n",
      " 16  5 21  5 20 19 11 10  5 22  5 23  5 24  3 72  4  2  7  8  9 10 11  9\n",
      "  8  7 12 13 13 13 13 13 13 13 13 12 14 14 14 14 14 14 14 14 12 14 14 14\n",
      " 14 14 14 14 14 12 14 14 15 14 14 14 14 14 12 14 14 14 14 14 14 14 14 12\n",
      " 15 15 14 15 15 15 15 15 12 16 17 18 19 20 18 17 16  5  9  5 20 19 11 10\n",
      "  5 22  5 23  5 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O-O\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_word(77))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_sequences, fens, trainX, trainY, vocab = df_to_data_black(grouped_df, fixed_window=True, sampling_rate=0.25, vocab=vocab_old)\n",
    "trainX_sequences, trainX_seqlengths  = pad_sequences(trainX_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainX_sequences, fens, trainX_seqlengths, trainY = np.asarray(trainX), np.asarray(trainX_sequences), np.asarray(fens), np.asarray(trainX_seqlengths), np.asarray(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for black multi-modal\n",
    "filenames = [\n",
    "    save_as_memmap(trainX_sequences, './../data/black-data/oct-dec/trainX_sequences.memmap'),\n",
    "    save_as_memmap(trainX, './../data/black-data/oct-dec/trainX.memmap'),\n",
    "    save_as_memmap(trainY, './../data/black-data/oct-dec/trainY.memmap'),\n",
    "    save_as_memmap(trainX_seqlengths, './../data/black-data/oct-dec/trainX_seqlengths.memmap')\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(fens, columns=['fens'])\n",
    "csv_filename = './../data/black-data/oct-dec/fens.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "with open('./../data/black-data/oct-dec/vocab.pkl', 'wb') as outp:\n",
    "    pickle.dump(vocab, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.move_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2965521, 12, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished processing the data, let's now load in the data (assuming we're starting from fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dtype_trainX = np.int64\n",
    "max_first_dim = 3038976  # This is your starting point, the upper bound of the first dimension\n",
    "other_dims = (12, 8, 8)  # The other dimensions of the shape, assumed to be correct\n",
    "\n",
    "trainX_filename = './../data/apr-jun/trainX.memmap'\n",
    "\n",
    "# Try to find a working memmap shape\n",
    "dummy = find_working_shape(trainX_filename, dtype_trainX, max_first_dim, other_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for jan (black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "\n",
    "#for jan-march\n",
    "\n",
    "# Assuming you know the dtype and shape, for example:\n",
    "# For trainX_sequences\n",
    "dtype_trainX_sequences = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_sequences = (1977299, 16)  # replace with the correct shape\n",
    "trainX_sequences = load_memmap('./../data/black-data/jan/trainX_sequences.memmap', dtype_trainX_sequences, shape_trainX_sequences)\n",
    "\n",
    "# For trainX\n",
    "dtype_trainX = np.int64  # or the correct dtype for your data\n",
    "shape_trainX = (1977299, 12, 8, 8)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/black-data/jan/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int64  # or the correct dtype for your data\n",
    "shape_trainY = (1977299,)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/black-data/jan/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "# For trainX_seqlengths\n",
    "dtype_trainX_seqlengths = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_seqlengths = (1977299,)  # replace with the correct shape\n",
    "trainX_seqlengths = load_memmap('./../data/black-data/jan/trainX_seqlengths.memmap', dtype_trainX_seqlengths, shape_trainX_seqlengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for jan-march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "\n",
    "#for jan-march\n",
    "\n",
    "# Assuming you know the dtype and shape, for example:\n",
    "# For trainX_sequences\n",
    "dtype_trainX_sequences = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_sequences = (3038976, 16)  # replace with the correct shape\n",
    "trainX_sequences = load_memmap('./../data/jan-march/trainX_sequences.memmap', dtype_trainX_sequences, shape_trainX_sequences)\n",
    "\n",
    "# For trainX\n",
    "dtype_trainX = np.int64  # or the correct dtype for your data\n",
    "shape_trainX = (3038976, 12, 8, 8)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/jan-march/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int64  # or the correct dtype for your data\n",
    "shape_trainY = (3038976,)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/jan-march/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "# For trainX_seqlengths\n",
    "dtype_trainX_seqlengths = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_seqlengths = (3038976,)  # replace with the correct shape\n",
    "trainX_seqlengths = load_memmap('./../data/jan-march/trainX_seqlengths.memmap', dtype_trainX_seqlengths, shape_trainX_seqlengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for april-june"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "\n",
    "# Assuming you know the dtype and shape, for example:\n",
    "# For trainX_sequences\n",
    "dtype_trainX_sequences = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_sequences = (2780980, 16)  # replace with the correct shape\n",
    "trainX_sequences = load_memmap('./../data/apr-jun/trainX_sequences.memmap', dtype_trainX_sequences, shape_trainX_sequences)\n",
    "\n",
    "# For trainX\n",
    "dtype_trainX = np.int64  # or the correct dtype for your data\n",
    "shape_trainX = (2780980, 12, 8, 8)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/apr-jun/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int64  # or the correct dtype for your data\n",
    "shape_trainY = (2780980,)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/apr-jun/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "# For trainX_seqlengths\n",
    "dtype_trainX_seqlengths = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_seqlengths = (2780980,)  # replace with the correct shape\n",
    "trainX_seqlengths = load_memmap('./../data/apr-jun/trainX_seqlengths.memmap', dtype_trainX_seqlengths, shape_trainX_seqlengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's combine memmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5819956, 16)\n"
     ]
    }
   ],
   "source": [
    "def concatenate_memmaps(file1, shape1, file2, shape2, dtype, result_file):\n",
    "    # Calculate the new shape\n",
    "    new_shape = (shape1[0] + shape2[0],) + shape1[1:]\n",
    "    \n",
    "    # Create a new memmap for the concatenated data\n",
    "    concatenated = np.memmap(result_file, dtype=dtype, mode='w+', shape=new_shape)\n",
    "    \n",
    "    # Load the original memmaps\n",
    "    memmap1 = np.memmap(file1, dtype=dtype, mode='r', shape=shape1)\n",
    "    memmap2 = np.memmap(file2, dtype=dtype, mode='r', shape=shape2)\n",
    "    \n",
    "    # Copy data from the original memmaps to the new memmap\n",
    "    concatenated[:shape1[0]] = memmap1[:]\n",
    "    concatenated[shape1[0]:] = memmap2[:]\n",
    "    \n",
    "    # Flush changes to ensure they're written to disk\n",
    "    concatenated.flush()\n",
    "    \n",
    "    return concatenated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for trainX_sequences\n",
    "dtype = np.int64\n",
    "shape_jan_mar = (3038976, 16)\n",
    "shape_apr_jun = (2780980, 16)\n",
    "result_file_sequences = './../data/combined/trainX_sequences.memmap'\n",
    "\n",
    "concatenated_sequences = concatenate_memmaps(\n",
    "    './../data/jan-march/trainX_sequences.memmap', shape_jan_mar,\n",
    "    './../data/apr-jun/trainX_sequences.memmap', shape_apr_jun,\n",
    "    dtype, result_file_sequences)\n",
    "\n",
    "\n",
    "# trainX concatenation\n",
    "shape_jan_mar_trainX = (3038976, 12, 8, 8)\n",
    "shape_apr_jun_trainX = (2780980, 12, 8, 8)\n",
    "result_file_trainX = './../data/combined/trainX.memmap'\n",
    "\n",
    "concatenated_trainX = concatenate_memmaps(\n",
    "    './../data/jan-march/trainX.memmap', shape_jan_mar_trainX,\n",
    "    './../data/apr-jun/trainX.memmap', shape_apr_jun_trainX,\n",
    "    dtype_trainX, result_file_trainX)\n",
    "\n",
    "# trainY concatenation\n",
    "shape_jan_mar_trainY = (3038976,)\n",
    "shape_apr_jun_trainY = (2780980,)\n",
    "result_file_trainY = './../data/combined/trainY.memmap'\n",
    "\n",
    "concatenated_trainY = concatenate_memmaps(\n",
    "    './../data/jan-march/trainY.memmap', shape_jan_mar_trainY,\n",
    "    './../data/apr-jun/trainY.memmap', shape_apr_jun_trainY,\n",
    "    dtype_trainY, result_file_trainY)\n",
    "\n",
    "# trainX_seqlengths concatenation\n",
    "shape_jan_mar_seqlengths = (3038976,)\n",
    "shape_apr_jun_seqlengths = (2780980,)\n",
    "result_file_seqlengths = './../data/combined/trainX_seqlengths.memmap'\n",
    "\n",
    "concatenated_seqlengths = concatenate_memmaps(\n",
    "    './../data/jan-march/trainX_seqlengths.memmap', shape_jan_mar_seqlengths,\n",
    "    './../data/apr-jun/trainX_seqlengths.memmap', shape_apr_jun_seqlengths,\n",
    "    dtype_trainX_seqlengths, result_file_seqlengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do it for black data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_memmaps(files, shapes, dtype, result_file):\n",
    "    if len(files) != len(shapes):\n",
    "        raise ValueError(\"The number of files must match the number of shapes\")\n",
    "\n",
    "    # Calculate the total number of rows for the new shape\n",
    "    total_rows = sum(shape[0] for shape in shapes)\n",
    "    # Assume all shapes have the same dimensions beyond the first axis\n",
    "    new_shape = (total_rows,) + shapes[0][1:]\n",
    "    \n",
    "    # Create a new memmap for the concatenated data\n",
    "    concatenated = np.memmap(result_file, dtype=dtype, mode='w+', shape=new_shape)\n",
    "    \n",
    "    # Initial index for data insertion in the concatenated memmap\n",
    "    current_index = 0\n",
    "    \n",
    "    # Loop over each file and corresponding shape\n",
    "    for file, shape in zip(files, shapes):\n",
    "        # Load the original memmap\n",
    "        memmap = np.memmap(file, dtype=dtype, mode='r', shape=shape)\n",
    "        \n",
    "        # Copy data to the appropriate slice of the concatenated memmap\n",
    "        concatenated[current_index:current_index + shape[0]] = memmap[:]\n",
    "        \n",
    "        # Update the index for the next insertion point\n",
    "        current_index += shape[0]\n",
    "    \n",
    "    # Flush changes to ensure they're written to disk\n",
    "    concatenated.flush()\n",
    "    \n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainX_files = ['./../data/black-data/jan/trainX.memmap',\n",
    "                './../data/black-data/feb/trainX.memmap', \n",
    "                './../data/black-data/march/trainX.memmap',\n",
    "                './../data/black-data/apr-jun/trainX.memmap',\n",
    "                './../data/black-data/july-sept/trainX.memmap',\n",
    "                './../data/black-data/oct-dec/trainX.memmap'\n",
    "                ]\n",
    "\n",
    "trainY_files = ['./../data/black-data/jan/trainY.memmap',\n",
    "                './../data/black-data/feb/trainY.memmap',\n",
    "                './../data/black-data/march/trainY.memmap',\n",
    "                './../data/black-data/apr-jun/trainY.memmap',\n",
    "                './../data/black-data/july-sept/trainY.memmap',\n",
    "                './../data/black-data/oct-dec/trainY.memmap'\n",
    "               ]\n",
    "\n",
    "trainX_sequences_files = ['./../data/black-data/jan/trainX_sequences.memmap',\n",
    "                          './../data/black-data/feb/trainX_sequences.memmap',\n",
    "                          './../data/black-data/march/trainX_sequences.memmap',\n",
    "                          './../data/black-data/apr-jun/trainX_sequences.memmap',\n",
    "                          './../data/black-data/july-sept/trainX_sequences.memmap',\n",
    "                          './../data/black-data/oct-dec/trainX_sequences.memmap'\n",
    "                         ]\n",
    "\n",
    "trainX_seqlengths_files = ['./../data/black-data/jan/trainX_seqlengths.memmap',\n",
    "                           './../data/black-data/feb/trainX_seqlengths.memmap',\n",
    "                           './../data/black-data/march/trainX_seqlengths.memmap',\n",
    "                           './../data/black-data/apr-jun/trainX_seqlengths.memmap',\n",
    "                           './../data/black-data/july-sept/trainX_seqlengths.memmap',\n",
    "                           './../data/black-data/oct-dec/trainX_seqlengths.memmap'\n",
    "                          ]\n",
    "\n",
    "trainX_shapes = [(1071641, 12, 8, 8),\n",
    "                 (922280, 12, 8, 8),\n",
    "                 (1040912, 12, 8, 8),\n",
    "                 (2726090, 12, 8, 8),\n",
    "                 (2654080, 12, 8, 8),\n",
    "                 (2965521, 12, 8, 8)\n",
    "                 ]\n",
    "\n",
    "train_shapes = [(1071641,),\n",
    "                 (922280,),\n",
    "                 (1040912,),\n",
    "                 (2726090,),\n",
    "                 (2654080,),\n",
    "                 (2965521,)\n",
    "                 ]\n",
    "\n",
    "sequence_shapes =[(1071641,16),\n",
    "                 (922280,16),\n",
    "                 (1040912,16),\n",
    "                 (2726090,16),\n",
    "                 (2654080,16),\n",
    "                 (2965521,16)\n",
    "                 ]\n",
    "\n",
    "concatenate_memmaps(trainX_files,trainX_shapes,np.int64, './../data/black-data/combined/trainX.memmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_memmaps(trainY_files,train_shapes,np.int64, './../data/black-data/combined/trainY.memmap')\n",
    "concatenate_memmaps(trainX_sequences_files,sequence_shapes,np.int64, './../data/black-data/combined/trainX_sequences.memmap')\n",
    "concatenate_memmaps(trainX_seqlengths_files,train_shapes,np.int64, './../data/black-data/combined/trainX_seqlengths.memmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "\n",
    "# Assuming you know the dtype and shape, for example:\n",
    "# For trainX_sequences\n",
    "dtype_trainX_sequences = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_sequences = (11380524, 16)  # replace with the correct shape\n",
    "trainX_sequences = load_memmap('./../data/black-data/combined/trainX_sequences.memmap', dtype_trainX_sequences, shape_trainX_sequences)\n",
    "\n",
    "# For trainX\n",
    "dtype_trainX = np.int64  # or the correct dtype for your data\n",
    "shape_trainX = (11380524, 12, 8, 8)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/black-data/combined/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int64  # or the correct dtype for your data\n",
    "shape_trainY = (11380524,)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/black-data/combined/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "# For trainX_seqlengths\n",
    "dtype_trainX_seqlengths = np.int64  # or the correct dtype for your data\n",
    "shape_trainX_seqlengths = (11380524,)  # replace with the correct shape\n",
    "trainX_seqlengths = load_memmap('./../data/black-data/combined/trainX_seqlengths.memmap', dtype_trainX_seqlengths, shape_trainX_seqlengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
