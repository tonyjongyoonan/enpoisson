{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Games of Elo ~1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dask.dataframe as dd \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract data from the dataframe at the given index\n",
    "        # Format the data as required for your model\n",
    "        data = self.dataframe[idx]\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        return data_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      moves  white_elo  \\\n",
      "game_id                                                                  \n",
      "0TH6rnrg  e2e4 e7e5 d2d4 g8f6 d4e5 f6e4 c2c3 d8e7 g1f3 d...       1137   \n",
      "0VoR1Pz3  d2d4 e7e6 c1f4 g8f6 e2e3 d7d6 f1d3 c7c5 d4c5 d...       1197   \n",
      "0nTYGMRx  e2e4 e7e5 g1f3 f8c5 f3e5 g8f6 f1c4 e8g8 d2d3 d...       1185   \n",
      "2BP6vWMj  d2d4 g8f6 b1c3 d7d6 d4d5 e7e6 e2e4 e6d5 e4d5 b...       1138   \n",
      "32zLcujS  d2d4 d7d5 c2c4 c7c6 e2e3 g8f6 f1e2 b8d7 c4d5 c...       1131   \n",
      "...                                                     ...        ...   \n",
      "waNjx6pa  d2d4 d7d5 b1c3 e7e6 c1f4 f8d6 f4d6 d8d6 c3b5 d...       1140   \n",
      "wmoOaDoG  e2e4 e7e5 g1f3 b8c6 d2d4 e5d4 f3d4 f8c5 f1b5 c...       1187   \n",
      "xaKGPR97  e2e4 e7e5 d1h5 d7d5 h5e5 c8e6 f1b5 c7c6 b5a4 b...       1108   \n",
      "yCNWC8pw  e2e4 e7e5 g1f3 b8c6 f1b5 d7d6 e1g1 g8f6 b1c3 a...       1147   \n",
      "zEtSHtQz  e2e3 e7e5 b1c3 f8b4 a2a3 b4c3 b2c3 d7d6 c1b2 b...       1100   \n",
      "\n",
      "          black_elo  white_active  \\\n",
      "game_id                             \n",
      "0TH6rnrg       1175          True   \n",
      "0VoR1Pz3       1113          True   \n",
      "0nTYGMRx       1151          True   \n",
      "2BP6vWMj       1195          True   \n",
      "32zLcujS       1121          True   \n",
      "...             ...           ...   \n",
      "waNjx6pa       1139          True   \n",
      "wmoOaDoG       1129          True   \n",
      "xaKGPR97       1122          True   \n",
      "yCNWC8pw       1170          True   \n",
      "zEtSHtQz       1175          True   \n",
      "\n",
      "                                                      board  \n",
      "game_id                                                      \n",
      "0TH6rnrg  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0VoR1Pz3  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0nTYGMRx  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2BP6vWMj  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "32zLcujS  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                     ...  \n",
      "waNjx6pa  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "wmoOaDoG  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "xaKGPR97  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yCNWC8pw  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zEtSHtQz  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[70305 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import CSV File (from Maia: http://csslab.cs.toronto.edu/datasets/#monthly_chess_csv)\n",
    "# The CSV has 151,072,060 rows\n",
    "data_types ={'clock': 'float32',\n",
    "       'cp': 'object',\n",
    "       'opp_clock': 'float32',\n",
    "       'opp_clock_percent': 'float32'}\n",
    "df = dd.read_csv('../data/lichess_db_standard_rated_2019-01.csv', blocksize='64e6', dtype= data_types, low_memory=False)\n",
    "\n",
    "# Filter out quick games (Bullet and HyperBullet) and take out moves that happened in the last XX seconds (this won't affect how many games we import but the # of moves we look at)\n",
    "condition_time_control = ~df['time_control'].isin(['Blitz', 'Bullet', 'HyperBullet'])\n",
    "condition_clock = df['clock'] > 60\n",
    "condition_plays = df['num_ply'] < 80\n",
    "filtered_df = df[condition_time_control & condition_clock & condition_plays]\n",
    "\n",
    "# Select Relevant Columns\n",
    "selected_columns = ['game_id','white_elo','black_elo','move','white_active','board']\n",
    "filtered_df = filtered_df[selected_columns]\n",
    "\n",
    "# Filter only games of Elo 1100-1199\n",
    "filtered_df = filtered_df[(filtered_df['white_elo'].between(1100, 1199)) & (filtered_df['black_elo'].between(1100, 1199))]\n",
    "\n",
    "# Group Same Games Together \n",
    "def aggregate_moves(group):\n",
    "    moves = ' '.join(group['move'])  # Concatenate moves into a single string\n",
    "    white_elo = group['white_elo'].iloc[0]  # Get the first white_elo\n",
    "    black_elo = group['black_elo'].iloc[0]  # Get the first black_elo\n",
    "    white_active = group['white_active'].iloc[0]  # Get the first num_ply\n",
    "    board = group['board'].iloc[0]  # Get the first num_ply\n",
    "    return pd.Series({'moves': moves, 'white_elo': white_elo, 'black_elo': black_elo, 'white_active': white_active, 'board': board})\n",
    "\n",
    "grouped_df = filtered_df.groupby('game_id').apply(aggregate_moves, meta={'moves': 'str', 'white_elo': 'int', 'black_elo': 'int', 'white_active': 'str', 'board': 'str'}).compute()\n",
    "\n",
    "# This gives us 99,300 Games when we don't filter games with more than 80 half-moves\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      moves  white_elo  \\\n",
      "game_id                                                                  \n",
      "0TH6rnrg  e2e4 e7e5 d2d4 g8f6 d4e5 f6e4 c2c3 d8e7 g1f3 d...       1137   \n",
      "0VoR1Pz3  d2d4 e7e6 c1f4 g8f6 e2e3 d7d6 f1d3 c7c5 d4c5 d...       1197   \n",
      "0nTYGMRx  e2e4 e7e5 g1f3 f8c5 f3e5 g8f6 f1c4 e8g8 d2d3 d...       1185   \n",
      "2BP6vWMj  d2d4 g8f6 b1c3 d7d6 d4d5 e7e6 e2e4 e6d5 e4d5 b...       1138   \n",
      "32zLcujS  d2d4 d7d5 c2c4 c7c6 e2e3 g8f6 f1e2 b8d7 c4d5 c...       1131   \n",
      "...                                                     ...        ...   \n",
      "waNjx6pa  d2d4 d7d5 b1c3 e7e6 c1f4 f8d6 f4d6 d8d6 c3b5 d...       1140   \n",
      "wmoOaDoG  e2e4 e7e5 g1f3 b8c6 d2d4 e5d4 f3d4 f8c5 f1b5 c...       1187   \n",
      "xaKGPR97  e2e4 e7e5 d1h5 d7d5 h5e5 c8e6 f1b5 c7c6 b5a4 b...       1108   \n",
      "yCNWC8pw  e2e4 e7e5 g1f3 b8c6 f1b5 d7d6 e1g1 g8f6 b1c3 a...       1147   \n",
      "zEtSHtQz  e2e3 e7e5 b1c3 f8b4 a2a3 b4c3 b2c3 d7d6 c1b2 b...       1100   \n",
      "\n",
      "          black_elo  white_active  \\\n",
      "game_id                             \n",
      "0TH6rnrg       1175          True   \n",
      "0VoR1Pz3       1113          True   \n",
      "0nTYGMRx       1151          True   \n",
      "2BP6vWMj       1195          True   \n",
      "32zLcujS       1121          True   \n",
      "...             ...           ...   \n",
      "waNjx6pa       1139          True   \n",
      "wmoOaDoG       1129          True   \n",
      "xaKGPR97       1122          True   \n",
      "yCNWC8pw       1170          True   \n",
      "zEtSHtQz       1175          True   \n",
      "\n",
      "                                                      board  \n",
      "game_id                                                      \n",
      "0TH6rnrg  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0VoR1Pz3  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0nTYGMRx  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2BP6vWMj  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "32zLcujS  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                     ...  \n",
      "waNjx6pa  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "wmoOaDoG  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "xaKGPR97  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yCNWC8pw  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zEtSHtQz  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[70305 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw data, we need to be able to make sense of chess moves. Meaning, we're transforming our entire world from chess moves into numerical tokens that will serve as indices into unique embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a mapping from each move to a unique embedding. In order to index into our matrix of \n",
    "# embeddings (matrix format so it's something we can tune), we'll also want a mapping from each move to a unique ID\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.move_to_id = {\"<UNK>\": 0}\n",
    "        self.id_to_move = {0: \"<UNK>\"}\n",
    "        self.index = 1  # Start indexing from 1\n",
    "\n",
    "    def add_move(self, move):\n",
    "        if move not in self.move_to_id:\n",
    "            self.move_to_id[move] = self.index\n",
    "            self.id_to_move[self.index] = move\n",
    "            self.index += 1\n",
    "\n",
    "    def get_id(self, move):\n",
    "        return self.move_to_id.get(move, self.move_to_id[\"<UNK>\"])\n",
    "\n",
    "    def get_move(self, id):\n",
    "        return self.id_to_move.get(id, self.id_to_move[0])\n",
    "    \n",
    "vocab = Vocabulary()\n",
    "for i,game in enumerate(grouped_df['moves']):\n",
    "    moves = game.split()\n",
    "    for move in moves: \n",
    "        vocab.add_move(move)\n",
    "\n",
    "# We can just use nn.Embedding later when we pass the model a sequence of indices, but this is if we ever want to pre-train and have access to the matrix we've trained\n",
    "def get_embedding_matrix(vocab, d_embed):\n",
    "    n_embed = len(vocab.move_to_id)\n",
    "    return np.random.normal(0, 1, (n_embed, d_embed))\n",
    "# embedding_matrix = get_embedding_matrix(vocab, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's turn our data into sequences of indices instead of chess moves\n",
    "\n",
    "# Function to convert games to a list of lists in which each list represents the move sequence of a game\n",
    "def df_to_list_of_games(df, vocab_map):\n",
    "    sequences = []\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        seq = [vocab_map.get_id(move) for move in moves]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "def df_to_subsequences_and_labels(df, vocab_map):\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        encoded_moves = [vocab_map.get_id(move) for move in moves]\n",
    "\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            subseq = encoded_moves[0:i+1]\n",
    "            label = encoded_moves[i+1]\n",
    "            subsequences.append(subseq)\n",
    "            next_moves.append(label)\n",
    "\n",
    "    return subsequences, next_moves\n",
    "\n",
    "# Function to pad move sequences & get their sequence lengths\n",
    "def pad_sequences(sequences, max_len=None, pad_id=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = np.full((len(sequences), max_len), pad_id, dtype=int)\n",
    "    sequence_lengths = np.zeros(len(sequences), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = seq[:length]\n",
    "        sequence_lengths[i] = length\n",
    "    return padded_sequences, sequence_lengths\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.lengths[idx], self.labels[idx]\n",
    "\n",
    "trainX, trainY = df_to_subsequences_and_labels(grouped_df, vocab)\n",
    "trainX, trainX_seqlengths  = pad_sequences(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.id_to_move.keys()))\n",
    "print(len(trainX[140]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Note: The input to the Embedding module is a list of indices, and the output is the corresponding word embeddings.\"\"\"\n",
    "# Bi-LSTM Model for PyTorch\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5, num_layers = 2, bidirectional = False, embedding_matrix = None):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(vocab.move_to_id), d_embed, padding_idx=0)\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(d_embed, d_hidden, dropout = dropout, bidirectional=bidirectional, num_layers = num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * d_hidden,d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = self.embeddings(x)\n",
    "        # Sort x and seq_lengths in descending order\n",
    "        # This is required for packing the sequence\n",
    "        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        x = x[perm_idx]\n",
    "        # Pack the sequence\n",
    "        packed_input = pack_padded_sequence(x, seq_lengths, batch_first=True)\n",
    "        # Pass the packed sequence through the LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        # Unpack the sequence\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True,total_length = x.size()[1])\n",
    "        _, unperm_idx = perm_idx.sort(0)\n",
    "        #unperm_idx = unperm_idx.to(self.device)\n",
    "        output = output.index_select(0, unperm_idx)\n",
    "        #This takes all the outputs across the cells\n",
    "        mean_pooled = torch.mean(output, dim=1)\n",
    "        output = torch.cat((mean_pooled,hidden[-1]),dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, vocab):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_start = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for sequences, lengths, labels in train_loader:\n",
    "            count += 1\n",
    "            sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            # Forward Pass\n",
    "            output = model(sequences, lengths)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        if epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                for sequences, lengths, labels in val_loader:\n",
    "                    sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "                    outputs = model(sequences, lengths)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    validation_loss += loss.item()\n",
    "            val_loss_values.append(validation_loss / len(val_loader))\n",
    "            val_error.append(100-100*val_correct/val_total)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * 0.5\n",
    "    return train_error,train_loss_values, val_error, val_loss_values, swa_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SequenceDataset(trainX, trainX_seqlengths, trainY)\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "\n",
    "# 0.2 gives us ~1,000,000 training samples (when we didn't filter games with more than 40 moves & filtered out moves with less than 30 seconds left)\n",
    "train_size = int(0.4 * total_size)\n",
    "val_size = int(train_size/10)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size+val_size))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1288999"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984142\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 128\n",
    "d_embed = 128\n",
    "NUM_EPOCHS = 5\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = RNNModel(vocab,d_embed,d_hidden,d_out,num_layers=2) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=0\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 6.182356120109558\n",
      "Epoch 1, Batch: 2000| Training Loss: 6.177685140609741\n",
      "Epoch 1, Batch: 3000| Training Loss: 6.1761104799906414\n",
      "Epoch 1, Batch: 4000| Training Loss: 6.172615287899971\n",
      "Epoch 1, Batch: 5000| Training Loss: 6.174867381381988\n",
      "Epoch 1, Batch: 6000| Training Loss: 6.174857710599899\n",
      "Epoch 1, Batch: 7000| Training Loss: 6.174077647686005\n",
      "Epoch 1, Batch: 8000| Training Loss: 6.172386170566082\n",
      "Epoch 1, Batch: 9000| Training Loss: 6.167981874889797\n",
      "Epoch 1, Batch: 10000| Training Loss: 6.166811403226853\n",
      "Epoch 1, Batch: 11000| Training Loss: 6.165029389554804\n",
      "Epoch 1, Batch: 12000| Training Loss: 6.164951903780302\n",
      "Epoch 1, Batch: 13000| Training Loss: 6.165426612083729\n",
      "Epoch 1, Batch: 14000| Training Loss: 6.164278510774885\n",
      "Epoch 1, Batch: 15000| Training Loss: 6.165609864234924\n",
      "Epoch 1, Batch: 16000| Training Loss: 6.164390026688576\n",
      "Epoch 1, Batch: 17000| Training Loss: 6.163982957026538\n",
      "Epoch 1, Batch: 18000| Training Loss: 6.162472054004669\n",
      "Epoch 1, Batch: 19000| Training Loss: 6.16075740859383\n",
      "Epoch 1, Batch: 20000| Training Loss: 6.1596701373815534\n",
      "Epoch 1, Training Loss: 6.159543181831743, Validation Error: 97.27926516109513, Training Error: 97.46415629492343\n",
      "Epoch 2, Batch: 1000| Training Loss: 6.132572294712067\n",
      "Epoch 2, Batch: 2000| Training Loss: 6.1257810161113735\n",
      "Epoch 2, Batch: 3000| Training Loss: 6.123581082344055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value,swa_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, vocab)\u001b[0m\n\u001b[1;32m     17\u001b[0m sequences, lengths, labels \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mto(device), lengths\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Backpropogate & Optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[0;34m(self, x, seq_lengths)\u001b[0m\n\u001b[1;32m     21\u001b[0m packed_input \u001b[38;5;241m=\u001b[39m pack_padded_sequence(x, seq_lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Pass the packed sequence through the LSTM\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m packed_output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Unpack the sequence\u001b[39;00m\n\u001b[1;32m     26\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(packed_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,total_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:882\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    879\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value,swa_model = train_rnn(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, vocab)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
