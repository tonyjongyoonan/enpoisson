{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Games of Elo ~1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dask.dataframe as dd \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract data from the dataframe at the given index\n",
    "        # Format the data as required for your model\n",
    "        data = self.dataframe[idx]\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        return data_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      moves  white_elo  \\\n",
      "game_id                                                                  \n",
      "0TH6rnrg  e2e4 e7e5 d2d4 g8f6 d4e5 f6e4 c2c3 d8e7 g1f3 d...       1137   \n",
      "0VoR1Pz3  d2d4 e7e6 c1f4 g8f6 e2e3 d7d6 f1d3 c7c5 d4c5 d...       1197   \n",
      "0nTYGMRx  e2e4 e7e5 g1f3 f8c5 f3e5 g8f6 f1c4 e8g8 d2d3 d...       1185   \n",
      "21RePraY  e2e4 b7b6 g1f3 c8b7 b1c3 g8f6 d2d3 e7e6 e4e5 f...       1109   \n",
      "2BP6vWMj  d2d4 g8f6 b1c3 d7d6 d4d5 e7e6 e2e4 e6d5 e4d5 b...       1138   \n",
      "...                                                     ...        ...   \n",
      "yCNWC8pw  e2e4 e7e5 g1f3 b8c6 f1b5 d7d6 e1g1 g8f6 b1c3 a...       1147   \n",
      "yFUMU63B  e2e4 c7c5 d1h5 e7e6 f1c4 a7a6 g1f3 b7b5 c4b3 g...       1157   \n",
      "z8aPkN9f  e2e4 e7e6 d2d4 d7d5 e4e5 c7c5 g1f3 b8c6 d4c5 f...       1193   \n",
      "zEtSHtQz  e2e3 e7e5 b1c3 f8b4 a2a3 b4c3 b2c3 d7d6 c1b2 b...       1100   \n",
      "zlGwUezu  d2d4 d7d5 c2c4 g7g6 b1c3 e7e6 g1f3 g8f6 e2e3 f...       1194   \n",
      "\n",
      "          black_elo  white_active  \\\n",
      "game_id                             \n",
      "0TH6rnrg       1175          True   \n",
      "0VoR1Pz3       1113          True   \n",
      "0nTYGMRx       1151          True   \n",
      "21RePraY       1116          True   \n",
      "2BP6vWMj       1195          True   \n",
      "...             ...           ...   \n",
      "yCNWC8pw       1170          True   \n",
      "yFUMU63B       1178          True   \n",
      "z8aPkN9f       1140          True   \n",
      "zEtSHtQz       1175          True   \n",
      "zlGwUezu       1189          True   \n",
      "\n",
      "                                                      board  \n",
      "game_id                                                      \n",
      "0TH6rnrg  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0VoR1Pz3  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0nTYGMRx  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "21RePraY  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2BP6vWMj  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                     ...  \n",
      "yCNWC8pw  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yFUMU63B  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "z8aPkN9f  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zEtSHtQz  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zlGwUezu  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[99300 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import CSV File (from Maia: http://csslab.cs.toronto.edu/datasets/#monthly_chess_csv)\n",
    "# The CSV has 151,072,060 rows\n",
    "data_types ={'clock': 'float32',\n",
    "       'cp': 'object',\n",
    "       'opp_clock': 'float32',\n",
    "       'opp_clock_percent': 'float32'}\n",
    "df = dd.read_csv('../data/lichess_db_standard_rated_2019-01.csv', blocksize='64e6', dtype= data_types, low_memory=False)\n",
    "\n",
    "# Filter out quick games (Bullet and HyperBullet)\n",
    "condition_time_control = ~df['time_control'].isin(['Blitz', 'Bullet', 'HyperBullet'])\n",
    "condition_clock = df['clock'] > 30\n",
    "filtered_df = df[condition_time_control & condition_clock]\n",
    "\n",
    "# Select Relevant Columns\n",
    "selected_columns = ['game_id','white_elo','black_elo','move','white_active','board']\n",
    "filtered_df = filtered_df[selected_columns]\n",
    "\n",
    "# Filter only games of Elo 1100-1199\n",
    "filtered_df = filtered_df[(filtered_df['white_elo'].between(1100, 1199)) & (filtered_df['black_elo'].between(1100, 1199))]\n",
    "\n",
    "# Group Same Games Together \n",
    "def aggregate_moves(group):\n",
    "    moves = ' '.join(group['move'])  # Concatenate moves into a single string\n",
    "    white_elo = group['white_elo'].iloc[0]  # Get the first white_elo\n",
    "    black_elo = group['black_elo'].iloc[0]  # Get the first black_elo\n",
    "    white_active = group['white_active'].iloc[0]  # Get the first num_ply\n",
    "    board = group['board'].iloc[0]  # Get the first num_ply\n",
    "    return pd.Series({'moves': moves, 'white_elo': white_elo, 'black_elo': black_elo, 'white_active': white_active, 'board': board})\n",
    "\n",
    "grouped_df = filtered_df.groupby('game_id').apply(aggregate_moves, meta={'moves': 'str', 'white_elo': 'int', 'black_elo': 'int', 'white_active': 'str', 'board': 'str'}).compute()\n",
    "\n",
    "# This gives us 99,300 Games \n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      moves  white_elo  \\\n",
      "game_id                                                                  \n",
      "0TH6rnrg  e2e4 e7e5 d2d4 g8f6 d4e5 f6e4 c2c3 d8e7 g1f3 d...       1137   \n",
      "0VoR1Pz3  d2d4 e7e6 c1f4 g8f6 e2e3 d7d6 f1d3 c7c5 d4c5 d...       1197   \n",
      "0nTYGMRx  e2e4 e7e5 g1f3 f8c5 f3e5 g8f6 f1c4 e8g8 d2d3 d...       1185   \n",
      "21RePraY  e2e4 b7b6 g1f3 c8b7 b1c3 g8f6 d2d3 e7e6 e4e5 f...       1109   \n",
      "2BP6vWMj  d2d4 g8f6 b1c3 d7d6 d4d5 e7e6 e2e4 e6d5 e4d5 b...       1138   \n",
      "...                                                     ...        ...   \n",
      "yCNWC8pw  e2e4 e7e5 g1f3 b8c6 f1b5 d7d6 e1g1 g8f6 b1c3 a...       1147   \n",
      "yFUMU63B  e2e4 c7c5 d1h5 e7e6 f1c4 a7a6 g1f3 b7b5 c4b3 g...       1157   \n",
      "z8aPkN9f  e2e4 e7e6 d2d4 d7d5 e4e5 c7c5 g1f3 b8c6 d4c5 f...       1193   \n",
      "zEtSHtQz  e2e3 e7e5 b1c3 f8b4 a2a3 b4c3 b2c3 d7d6 c1b2 b...       1100   \n",
      "zlGwUezu  d2d4 d7d5 c2c4 g7g6 b1c3 e7e6 g1f3 g8f6 e2e3 f...       1194   \n",
      "\n",
      "          black_elo  white_active  \\\n",
      "game_id                             \n",
      "0TH6rnrg       1175          True   \n",
      "0VoR1Pz3       1113          True   \n",
      "0nTYGMRx       1151          True   \n",
      "21RePraY       1116          True   \n",
      "2BP6vWMj       1195          True   \n",
      "...             ...           ...   \n",
      "yCNWC8pw       1170          True   \n",
      "yFUMU63B       1178          True   \n",
      "z8aPkN9f       1140          True   \n",
      "zEtSHtQz       1175          True   \n",
      "zlGwUezu       1189          True   \n",
      "\n",
      "                                                      board  \n",
      "game_id                                                      \n",
      "0TH6rnrg  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0VoR1Pz3  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "0nTYGMRx  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "21RePraY  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "2BP6vWMj  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "...                                                     ...  \n",
      "yCNWC8pw  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "yFUMU63B  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "z8aPkN9f  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zEtSHtQz  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "zlGwUezu  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
      "\n",
      "[99300 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our raw data, we need to be able to make sense of chess moves. Meaning, we're transforming our entire world from chess moves into numerical tokens that will serve as indices into unique embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a mapping from each move to a unique embedding. In order to index into our matrix of \n",
    "# embeddings (matrix format so it's something we can tune), we'll also want a mapping from each move to a unique ID\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.move_to_id = {\"<UNK>\": 0}\n",
    "        self.id_to_move = {0: \"<UNK>\"}\n",
    "        self.index = 1  # Start indexing from 1\n",
    "\n",
    "    def add_move(self, move):\n",
    "        if move not in self.move_to_id:\n",
    "            self.move_to_id[move] = self.index\n",
    "            self.id_to_move[self.index] = move\n",
    "            self.index += 1\n",
    "\n",
    "    def get_id(self, move):\n",
    "        return self.move_to_id.get(move, self.move_to_id[\"<UNK>\"])\n",
    "\n",
    "    def get_move(self, id):\n",
    "        return self.id_to_move.get(id, self.id_to_move[0])\n",
    "    \n",
    "vocab = Vocabulary()\n",
    "for i,game in enumerate(grouped_df['moves']):\n",
    "    moves = game.split()\n",
    "    for move in moves: \n",
    "        vocab.add_move(move)\n",
    "\n",
    "# We can just use nn.Embedding later when we pass the model a sequence of indices, but this is if we ever want to pre-train and have access to the matrix we've trained\n",
    "def get_embedding_matrix(vocab, d_embed):\n",
    "    n_embed = len(vocab.move_to_id)\n",
    "    return np.random.normal(0, 1, (n_embed, d_embed))\n",
    "# embedding_matrix = get_embedding_matrix(vocab, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's turn our data into sequences of indices instead of chess moves\n",
    "\n",
    "# Function to convert games to a list of lists in which each list represents the move sequence of a game\n",
    "def df_to_list_of_games(df, vocab_map):\n",
    "    sequences = []\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        seq = [vocab_map.get_id(move) for move in moves]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "def df_to_subsequences_and_labels(df, vocab_map):\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        encoded_moves = [vocab_map.get_id(move) for move in moves]\n",
    "\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            subseq = encoded_moves[0:i+1]\n",
    "            label = encoded_moves[i+1]\n",
    "            subsequences.append(subseq)\n",
    "            next_moves.append(label)\n",
    "\n",
    "    return subsequences, next_moves\n",
    "\n",
    "# Function to pad move sequences & get their sequence lengths\n",
    "def pad_sequences(sequences, max_len=None, pad_id=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = np.full((len(sequences), max_len), pad_id, dtype=int)\n",
    "    sequence_lengths = np.zeros(len(sequences), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = seq[:length]\n",
    "        sequence_lengths[i] = length\n",
    "    return padded_sequences, sequence_lengths\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, lengths, labels):\n",
    "        self.sequences = sequences\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.lengths[idx], self.labels[idx]\n",
    "\n",
    "trainX, trainY = df_to_subsequences_and_labels(grouped_df, vocab)\n",
    "trainX, trainX_seqlengths  = pad_sequences(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904\n",
      "[  1  76   9  77  78   4  55  41  79  59  60  80  81  44  82  70  83  69\n",
      "  15  84  20  85  68  86   7  21  22  87  88  89  38  45  90  67  68  91\n",
      "  92  93  94  95  96  97  98  99  17 100 101 102   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.id_to_move.keys()))\n",
    "print(trainX[140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Note: The input to the Embedding module is a list of indices, and the output is the corresponding word embeddings.\"\"\"\n",
    "# Bi-LSTM Model for PyTorch\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab, d_embed, d_hidden, d_out, dropout = 0.5, num_layers = 2, bidirectional = False, embedding_matrix = None):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(vocab.move_to_id), d_embed, padding_idx=0)\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(d_embed, d_hidden, dropout = dropout, bidirectional=bidirectional, num_layers = num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2 * d_hidden,d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = self.embeddings(x)\n",
    "        # Sort x and seq_lengths in descending order\n",
    "        # This is required for packing the sequence\n",
    "        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        x = x[perm_idx]\n",
    "        # Pack the sequence\n",
    "        packed_input = pack_padded_sequence(x, seq_lengths, batch_first=True)\n",
    "        # Pass the packed sequence through the LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        # Unpack the sequence\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True,total_length = x.size()[1])\n",
    "        _, unperm_idx = perm_idx.sort(0)\n",
    "        #unperm_idx = unperm_idx.to(self.device)\n",
    "        output = output.index_select(0, unperm_idx)\n",
    "        #This takes all the outputs across the cells\n",
    "        mean_pooled = torch.mean(output, dim=1)\n",
    "        output = torch.cat((mean_pooled, hidden[-1]),dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, vocab):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_start = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for sequences, lengths, labels in train_loader:\n",
    "            count += 1\n",
    "            sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "            # Forward Pass\n",
    "            output = model(sequences, lengths)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        if epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                for sequences, lengths, labels in val_loader:\n",
    "                    sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "                    outputs = model(sequences, lengths)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    validation_loss += loss.item()\n",
    "            val_loss_values.append(validation_loss / len(val_loader))\n",
    "            val_error.append(100-100*val_correct/val_total)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * 0.5\n",
    "    return train_error,train_loss_values, val_error, val_loss_values, swa_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SequenceDataset(trainX, trainX_seqlengths, trainY)\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "\n",
    "# This gives us ~1,000,000 training samples\n",
    "train_size = int(0.2 * total_size)\n",
    "val_size = int(train_size/10)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size+val_size))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734704\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "d_hidden = 100\n",
    "d_embed = 64\n",
    "NUM_EPOCHS = 3\n",
    "d_out = len(vocab.id_to_move.keys())\n",
    "model = RNNModel(vocab,d_embed,d_hidden,d_out,num_layers=2) \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-3\n",
    "weight_decay=1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 6.54957947731018\n",
      "Epoch 1, Batch: 2000| Training Loss: 6.476903906106949\n",
      "Epoch 1, Batch: 3000| Training Loss: 6.442900150934856\n",
      "Epoch 1, Batch: 4000| Training Loss: 6.416354013562202\n",
      "Epoch 1, Batch: 5000| Training Loss: 6.402802619743347\n",
      "Epoch 1, Batch: 6000| Training Loss: 6.390775051116943\n",
      "Epoch 1, Batch: 7000| Training Loss: 6.383608567510333\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_error,train_loss_values, val_error, val_loss_value,swa_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[71], line 43\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, vocab)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequences, lengths, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m---> 43\u001b[0m         sequences, lengths, labels \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(sequences, lengths)\n\u001b[1;32m     45\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value,swa_model = train_rnn(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, vocab)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('validation_error_model_rnn.png')  # This will save the plot as an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
