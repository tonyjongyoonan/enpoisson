{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with transformers\n",
    "Transformers remain as a promising replacement of RNNs due to their parallelizability. However, RNNs are unique in their hidden state which tends to be uniquely useful for games. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlee0/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import numpy as np\n",
    "import utils\n",
    "import models\n",
    "\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.read_csv('../data/haha-longer-mid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def df_to_data_with_special_tokens(df, fixed_window=False, fixed_window_size=16, sampling_rate=1, algebraic_notation=True):\n",
    "    \"\"\"\n",
    "    Input: Dataframe of training data in which each row represents a full game played between players\n",
    "    Output: List in which each item represents some game's history up until a particular move, List in the same order in which the associated label is the following move\n",
    "    \"\"\"\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "    vocab = VocabularyWithCLS()\n",
    "    board = chess.Board()\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        # Turn the game into a list of moves\n",
    "        encoded_moves = [1]\n",
    "        for move in moves:\n",
    "            # Create a move object from the coordinate notation\n",
    "            move_obj = chess.Move.from_uci(move)\n",
    "            if move_obj not in board.legal_moves:\n",
    "                break \n",
    "            else:\n",
    "                if algebraic_notation:\n",
    "                    algebraic_move = board.san(move_obj)\n",
    "                    board.push(move_obj)\n",
    "                    vocab.add_move(algebraic_move)\n",
    "                    encoded_move = vocab.get_id(algebraic_move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "                else:\n",
    "                    encoded_move = vocab.get_id(move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "        board.reset()\n",
    "        # Turn the list of moves into subsequences\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            if random.uniform(0, 1) <= sampling_rate:\n",
    "                subseq = encoded_moves[0:i+1]\n",
    "                if fixed_window and len(subseq) > fixed_window_size:\n",
    "                    subseq = subseq[-fixed_window_size:]\n",
    "                label = encoded_moves[i+1]\n",
    "                subsequences.append(subseq)\n",
    "                next_moves.append(label)\n",
    "\n",
    "    return subsequences, next_moves, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, vocab = df_to_data_with_special_tokens(grouped_df, fixed_window=True, sampling_rate=0.0875)\n",
    "trainX, trainX_seqlengths  = pad_sequences(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trainX\n",
    "dtype_trainX = np.int32  # or the correct dtype for your data\n",
    "shape_trainX = (2161482, 750)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/transformer/jan/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int32 # or the correct dtype for your data\n",
    "shape_trainY = (2161482, 7)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/transformer/jan/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "with open('./../data/transformer/jan/vocab.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[:1,:4])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences, self.labels = sequences, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084538\n"
     ]
    }
   ],
   "source": [
    "dataset = TransformerDataset(trainX, trainY)\n",
    "total_size = len(dataset)\n",
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.97 * total_size)\n",
    "val_size = int(total_size * 0.02)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)\n",
    "# Reload the data with particular batch size\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class ChessTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformerDecoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model =d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Only decoder is needed for autoregressive models\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                       nhead=nhead, \n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True,),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # Memory is optional and could be used for incorporating encoder states in a hybrid model\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device)\n",
    "\n",
    "        # Embedding and Positional Encoding for tgt\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "        # Autoregressive decoding using the Transformer Decoder\n",
    "        output = self.transformer_decoder(tgt_emb, memory=None,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          tgt_is_causal = True,\n",
    "                                          tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\" Generate a square mask for the sequence to mask out subsequent positions. \"\"\"\n",
    "        sz = tgt.size(1)\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=tgt.device, dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class ChessTransformer(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers,\n",
    "                                          batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        # Create source padding mask\n",
    "        src_padding_mask = self.create_padding_mask(src).to(src.device)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        # Embedding and Positional Encoding for src\n",
    "\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "    \n",
    "        # Transformer\n",
    "        output = self.transformer(src_emb, src_emb, \n",
    "                                  src_key_padding_mask=src_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                  tgt_is_causal = True,\n",
    "                                  src_is_causal = True, \n",
    "                                  src_mask = self.square_subsequent_mask(src),\n",
    "                                  tgt_mask = self.square_subsequent_mask(tgt))\n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with `True`.\n",
    "        This mask ensures that for any position `i` in `tgt`, the decoder's self-attention mechanism\n",
    "        can only attend to positions at or before `i`.\n",
    "        \n",
    "        Args:\n",
    "            tgt (Tensor): The target input tensor of shape [batch_size, tgt_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A mask of shape [tgt_len, tgt_len] where `True` indicates that attention is not allowed.\n",
    "        \"\"\"\n",
    "        # tgt_len could be derived from the second dimension of tgt\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Generate an upper triangular matrix with `True` in the upper triangle\n",
    "        mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "    def generate_sequence(self, src, src_length, start_symbol_id, sep_token_id, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate a sequence autoregressively using the trained transformer model.\n",
    "\n",
    "        Args:\n",
    "        - src (Tensor): The input source sequence tensor.\n",
    "        - src_length (Tensor): The length of the source sequence.\n",
    "        - start_symbol_id (int): The ID of the start symbol to begin generation.\n",
    "        - sep_token_id (int): The ID of the SEP token for sequence termination.\n",
    "        - max_length (int): Maximum length of the generated sequence to prevent infinite loops.\n",
    "\n",
    "        Returns:\n",
    "        - The generated sequence tensor.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure the model is in eval mode\n",
    "\n",
    "        # Initialize the target sequence with the start symbol\n",
    "        tgt = torch.tensor([start_symbol_id], dtype=torch.long).to(src.device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Assuming src_length is a tensor with the length of src. Adjust as needed.\n",
    "\n",
    "            # Perform a forward pass to get logits for the next token\n",
    "            logits = self.forward(src, src_length, tgt, src)\n",
    "            # Get the last token logits and apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            # get most likely token from probs\n",
    "            next_token = torch.max(probs, 1)\n",
    "            \n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt = torch.cat((tgt, next_token), dim=1)\n",
    "            \n",
    "            # Check if the <SEP> token is generated\n",
    "            if next_token.item() == sep_token_id:\n",
    "                break\n",
    "\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate top-3 accuracy\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    top3 = torch.topk(y_pred, 3, dim=1).indices\n",
    "    correct = top3.eq(y_true.view(-1, 1).expand_as(top3))\n",
    "    return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "def train_decoder(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            tgt_labels = torch.cat([sequences[:,1:],labels.unsqueeze(1)],dim=1).to(device)\n",
    "            logits = model(sequences, sequences)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), tgt_labels.view(-1))\n",
    "\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, dim=2)\n",
    "            train_correct += (predicted == tgt_labels).sum().item()\n",
    "            train_total += tgt_labels.numel()\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                val_loss = 0\n",
    "\n",
    "                for sequences, labels in val_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    tgt_labels = torch.cat([sequences[:,1:],labels.unsqueeze(1)],dim=1).to(device)\n",
    "                    logits = model(sequences, sequences)\n",
    "                    loss = criterion(logits.view(-1, model.vocab_size), tgt_labels.view(-1))\n",
    "\n",
    "                    # For logging purposes\n",
    "                    val_loss += loss.item()\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted = torch.max(logits, dim=2)\n",
    "                    val_correct += (predicted == tgt_labels).sum().item()\n",
    "                    val_total += tgt_labels.numel()\n",
    "\n",
    "                    val_loss_values.append(val_loss / len(val_loader))\n",
    "                    val_accuracy = 100 * val_correct / val_total\n",
    "                    val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                    val_error.append(100 - val_accuracy)\n",
    "                    val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values\n",
    "\n",
    "def train_transformer(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            count += 1\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            logits = model(sequences, labels)\n",
    "            print(logits)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), labels.contiguous().view(-1))\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # _, predicted = torch.max(output.data, 1)\n",
    "            # train_total += labels.size(0)\n",
    "            # train_correct += (predicted == labels).sum().item()\n",
    "            # Get the predicted class indices for each position in each sequence\n",
    "            _, predicted = torch.max(logits.data, dim=2)  # Shape: (batch_size, seq_length)\n",
    "            correct_predictions = predicted == labels  # Shape: (batch_size, seq_length)\n",
    "            correct_sequences = correct_predictions.all(dim=1)  # Shape: (batch_size)\n",
    "            train_correct += correct_sequences.sum().item()\n",
    "            train_total += labels.size(0) \n",
    "            break\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        # if val_loader is not None:\n",
    "        #     with torch.no_grad():\n",
    "        #         val_correct = 0\n",
    "        #         val_total = 0\n",
    "        #         val_top3_correct = 0\n",
    "        #         validation_loss = 0\n",
    "\n",
    "        #         for sequences, lengths, labels in val_loader:\n",
    "        #             sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "        #             outputs = model.generate(sequences, lengths)\n",
    "        #             _, predicted = torch.max(outputs.data, 1)\n",
    "        #             val_total += labels.size(0)\n",
    "        #             val_correct += (predicted == labels).sum().item()\n",
    "        #             val_top3_correct += top_3_accuracy(labels, outputs) * labels.size(0)\n",
    "        #             loss = criterion(outputs, labels)\n",
    "        #             validation_loss += loss.item()\n",
    "\n",
    "        #         val_loss_values.append(validation_loss / len(val_loader))\n",
    "        #         val_accuracy = 100 * val_correct / val_total\n",
    "        #         val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "        #         val_error.append(100 - val_accuracy)\n",
    "        #         val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4592570\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "# batch_size = 1\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 128\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.65 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 6.008774438381195\n",
      "Epoch 1, Batch: 2000| Training Loss: 5.91263079738617\n",
      "Epoch 1, Batch: 3000| Training Loss: 5.866261313597361\n",
      "Epoch 1, Batch: 4000| Training Loss: 5.8419452521801\n",
      "Epoch 1, Batch: 5000| Training Loss: 5.8258527850151065\n",
      "Epoch 1, Batch: 6000| Training Loss: 5.8119971399307255\n",
      "Epoch 1, Batch: 7000| Training Loss: 5.803614270176206\n",
      "Epoch 1, Batch: 8000| Training Loss: 5.792056399792433\n",
      "Epoch 1, Batch: 9000| Training Loss: 5.780102597051196\n",
      "Epoch 1, Batch: 10000| Training Loss: 5.768266700720787\n",
      "Epoch 1, Batch: 11000| Training Loss: 5.76097933754054\n",
      "Epoch 1, Batch: 12000| Training Loss: 5.752273487309615\n",
      "Epoch 1, Batch: 13000| Training Loss: 5.743259393471938\n",
      "Epoch 1, Batch: 14000| Training Loss: 5.737206680689539\n",
      "Epoch 1, Batch: 15000| Training Loss: 5.730790600506465\n",
      "Epoch 1, Batch: 16000| Training Loss: 5.727749186083674\n",
      "Epoch 1, Batch: 17000| Training Loss: 5.721433475382188\n",
      "Epoch 1, Batch: 18000| Training Loss: 5.71827676361137\n",
      "Epoch 1, Batch: 19000| Training Loss: 5.7130007714472315\n",
      "Epoch 1, Batch: 20000| Training Loss: 5.70726757388115\n",
      "Epoch 1, Batch: 21000| Training Loss: 5.703593109040034\n",
      "Epoch 1, Batch: 22000| Training Loss: 5.699584164857864\n",
      "Epoch 1, Batch: 23000| Training Loss: 5.696349255188652\n",
      "Epoch 1, Batch: 24000| Training Loss: 5.693179104199012\n",
      "Epoch 1, Batch: 25000| Training Loss: 5.6896405223178865\n",
      "Epoch 1, Batch: 26000| Training Loss: 5.685960506668458\n",
      "Epoch 1, Batch: 27000| Training Loss: 5.682003706781953\n",
      "Epoch 1, Batch: 28000| Training Loss: 5.678016588841166\n",
      "Epoch 1, Batch: 29000| Training Loss: 5.674834115538104\n",
      "Epoch 1, Batch: 30000| Training Loss: 5.671432315222423\n",
      "Epoch 1, Batch: 31000| Training Loss: 5.667404052957412\n",
      "Epoch 1, Batch: 32000| Training Loss: 5.664689053051173\n",
      "Epoch 1, Batch: 33000| Training Loss: 5.662114857030637\n",
      "Epoch 1, Batch: 34000| Training Loss: 5.658983875723446\n",
      "Epoch 1, Batch: 35000| Training Loss: 5.655998677294595\n",
      "Epoch 1, Batch: 36000| Training Loss: 5.653676500035657\n",
      "Epoch 1, Batch: 37000| Training Loss: 5.649870466187194\n",
      "Epoch 1, Batch: 38000| Training Loss: 5.647873892752748\n",
      "Epoch 1, Batch: 39000| Training Loss: 5.645477216359897\n",
      "Epoch 1, Batch: 40000| Training Loss: 5.642982973366975\n",
      "Epoch 1, Batch: 41000| Training Loss: 5.639946414081062\n",
      "Epoch 1, Batch: 42000| Training Loss: 5.636959567927179\n",
      "Epoch 1, Batch: 43000| Training Loss: 5.634717334142951\n",
      "Epoch 1, Batch: 44000| Training Loss: 5.632480907445604\n",
      "Epoch 1, Batch: 45000| Training Loss: 5.630129598373837\n",
      "Epoch 1, Batch: 46000| Training Loss: 5.627864774444829\n",
      "Epoch 1, Batch: 47000| Training Loss: 5.625784907767113\n",
      "Epoch 1, Batch: 48000| Training Loss: 5.624041764194767\n",
      "Epoch 1, Batch: 49000| Training Loss: 5.622344399213791\n",
      "Epoch 1, Batch: 50000| Training Loss: 5.619769647116661\n",
      "Epoch 1, Batch: 51000| Training Loss: 5.618617273307314\n",
      "Epoch 1, Batch: 52000| Training Loss: 5.616301735460758\n",
      "Epoch 1, Batch: 53000| Training Loss: 5.614405789577736\n",
      "Epoch 1, Batch: 54000| Training Loss: 5.61268503164362\n",
      "Epoch 1, Batch: 55000| Training Loss: 5.610696725637262\n",
      "Epoch 1, Batch: 56000| Training Loss: 5.60893064405237\n",
      "Epoch 1, Batch: 57000| Training Loss: 5.607078356868342\n",
      "Epoch 1, Batch: 58000| Training Loss: 5.604914912022394\n",
      "Epoch 1, Batch: 59000| Training Loss: 5.603003518157086\n",
      "Epoch 1, Batch: 60000| Training Loss: 5.601873631858826\n",
      "Epoch 1, Batch: 61000| Training Loss: 5.6003122381460475\n",
      "Epoch 1, Batch: 62000| Training Loss: 5.598605803993441\n",
      "Epoch 1, Batch: 63000| Training Loss: 5.596604401894978\n",
      "Epoch 1, Batch: 64000| Training Loss: 5.595641095884145\n",
      "Epoch 1, Batch: 65000| Training Loss: 5.59411146132029\n",
      "Epoch 1, Batch: 66000| Training Loss: 5.592651679035389\n",
      "Epoch 1, Batch: 67000| Training Loss: 5.590538653501824\n",
      "Epoch 1, Batch: 68000| Training Loss: 5.589167606027687\n",
      "Epoch 1, Batch: 69000| Training Loss: 5.587705112495284\n",
      "Epoch 1, Batch: 70000| Training Loss: 5.58595506600993\n",
      "Epoch 1, Batch: 71000| Training Loss: 5.584322660570414\n",
      "Epoch 1, Batch: 72000| Training Loss: 5.582905050949918\n",
      "Epoch 1, Batch: 73000| Training Loss: 5.581746441436141\n",
      "Epoch 1, Batch: 74000| Training Loss: 5.580237703181602\n",
      "Epoch 1, Batch: 75000| Training Loss: 5.578527818867365\n",
      "Epoch 1, Batch: 76000| Training Loss: 5.576697447739149\n",
      "Epoch 1, Batch: 77000| Training Loss: 5.5752845089466545\n",
      "Epoch 1, Batch: 78000| Training Loss: 5.574053594457798\n",
      "Epoch 1, Batch: 79000| Training Loss: 5.572607085255128\n",
      "Epoch 1, Batch: 80000| Training Loss: 5.571736692783237\n",
      "Epoch 1, Batch: 81000| Training Loss: 5.570797452058321\n",
      "Epoch 1, Batch: 82000| Training Loss: 5.569251043043486\n",
      "Epoch 1, Batch: 83000| Training Loss: 5.5680200476387895\n",
      "Epoch 1, Batch: 84000| Training Loss: 5.566840319996788\n",
      "Epoch 1, Batch: 85000| Training Loss: 5.565443617352317\n",
      "Epoch 1, Batch: 86000| Training Loss: 5.564276213321575\n",
      "Epoch 1, Batch: 87000| Training Loss: 5.563453269876283\n",
      "Epoch 1, Batch: 88000| Training Loss: 5.56208020610701\n",
      "Epoch 1, Batch: 89000| Training Loss: 5.560952081173993\n",
      "Epoch 1, Batch: 90000| Training Loss: 5.559890655676524\n",
      "Epoch 1, Batch: 91000| Training Loss: 5.558940672552192\n",
      "Epoch 1, Batch: 92000| Training Loss: 5.557895122439965\n",
      "Epoch 1, Batch: 93000| Training Loss: 5.556610720416551\n",
      "Epoch 1, Batch: 94000| Training Loss: 5.555592907705206\n",
      "Epoch 1, Batch: 95000| Training Loss: 5.5546432635658665\n",
      "Epoch 1, Batch: 96000| Training Loss: 5.553336047542592\n",
      "Epoch 1, Batch: 97000| Training Loss: 5.552582059277702\n",
      "Epoch 1, Batch: 98000| Training Loss: 5.551582426438526\n",
      "Epoch 1, Batch: 99000| Training Loss: 5.55066894276696\n",
      "Epoch 1, Batch: 100000| Training Loss: 5.549648764014244\n",
      "Epoch 1, Batch: 101000| Training Loss: 5.548399833150429\n",
      "Epoch 1, Batch: 102000| Training Loss: 5.547052166534405\n",
      "Epoch 1, Batch: 103000| Training Loss: 5.546055053678531\n",
      "Epoch 1, Batch: 104000| Training Loss: 5.544890321146983\n",
      "Epoch 1, Batch: 105000| Training Loss: 5.543664395250593\n",
      "Epoch 1, Batch: 106000| Training Loss: 5.542499282917887\n",
      "Epoch 1, Batch: 107000| Training Loss: 5.541733732123241\n",
      "Epoch 1, Batch: 108000| Training Loss: 5.540577498656732\n",
      "Epoch 1, Batch: 109000| Training Loss: 5.539751216096615\n",
      "Epoch 1, Batch: 110000| Training Loss: 5.538884900914539\n",
      "Epoch 1, Batch: 111000| Training Loss: 5.538019680066152\n",
      "Epoch 1, Batch: 112000| Training Loss: 5.536765255845019\n",
      "Epoch 1, Batch: 113000| Training Loss: 5.535652971436492\n",
      "Epoch 1, Batch: 114000| Training Loss: 5.5346574525059316\n",
      "Epoch 1, Batch: 115000| Training Loss: 5.5336387693550275\n",
      "Epoch 1, Batch: 116000| Training Loss: 5.5326206564964915\n",
      "Epoch 1, Batch: 117000| Training Loss: 5.531750691813281\n",
      "Epoch 1, Batch: 118000| Training Loss: 5.530884790644808\n",
      "Epoch 1, Batch: 119000| Training Loss: 5.530220191528818\n",
      "Epoch 1, Batch: 120000| Training Loss: 5.529246772748232\n",
      "Epoch 1, Batch: 121000| Training Loss: 5.528184888071265\n",
      "Epoch 1, Batch: 122000| Training Loss: 5.5271411200273235\n",
      "Epoch 1, Batch: 123000| Training Loss: 5.5259353446534005\n",
      "Epoch 1, Batch: 124000| Training Loss: 5.5247056907703795\n",
      "Epoch 1, Batch: 125000| Training Loss: 5.5237404660282134\n",
      "Epoch 1, Batch: 126000| Training Loss: 5.522722205269904\n",
      "Epoch 1, Batch: 127000| Training Loss: 5.521634854954997\n",
      "Epoch 1, Batch: 128000| Training Loss: 5.520777756407857\n",
      "Epoch 1, Batch: 129000| Training Loss: 5.520047230543092\n",
      "Epoch 1, Batch: 130000| Training Loss: 5.519149347525376\n",
      "Epoch 1, Batch: 131000| Training Loss: 5.5182287504527405\n",
      "Epoch 1, Batch: 132000| Training Loss: 5.5171499914870115\n",
      "Epoch 1, Batch: 133000| Training Loss: 5.516220927112981\n",
      "Epoch 1, Batch: 134000| Training Loss: 5.515637725241149\n",
      "Epoch 1, Batch: 135000| Training Loss: 5.514796431377199\n",
      "Epoch 1, Batch: 136000| Training Loss: 5.514002275777214\n",
      "Epoch 1, Batch: 137000| Training Loss: 5.5134179051688115\n",
      "Epoch 1, Batch: 138000| Training Loss: 5.512458113008651\n",
      "Epoch 1, Batch: 139000| Training Loss: 5.511532760877404\n",
      "Epoch 1, Batch: 140000| Training Loss: 5.510619598732676\n",
      "Epoch 1, Batch: 141000| Training Loss: 5.5097981730420535\n",
      "Epoch 1, Batch: 142000| Training Loss: 5.508992236050082\n",
      "Epoch 1, Batch: 143000| Training Loss: 5.508232056397659\n",
      "Epoch 1, Batch: 144000| Training Loss: 5.507302372864551\n",
      "Epoch 1, Batch: 145000| Training Loss: 5.506287463027033\n",
      "Epoch 1, Batch: 146000| Training Loss: 5.5051884556551505\n",
      "Epoch 1, Batch: 147000| Training Loss: 5.504272822318434\n",
      "Epoch 1, Batch: 148000| Training Loss: 5.503408850934054\n",
      "Epoch 1, Batch: 149000| Training Loss: 5.502580202323478\n",
      "Epoch 1, Batch: 150000| Training Loss: 5.501881263039907\n",
      "Epoch 1, Batch: 151000| Training Loss: 5.501259561747116\n",
      "Epoch 1, Batch: 152000| Training Loss: 5.500361903824304\n",
      "Epoch 1, Batch: 153000| Training Loss: 5.499836449380014\n",
      "Epoch 1, Batch: 154000| Training Loss: 5.499038364843889\n",
      "Epoch 1, Batch: 155000| Training Loss: 5.498274972263459\n",
      "Epoch 1, Batch: 156000| Training Loss: 5.49745355996566\n",
      "Epoch 1, Batch: 157000| Training Loss: 5.496781304588743\n",
      "Epoch 1, Batch: 158000| Training Loss: 5.4957979455658155\n",
      "Epoch 1, Batch: 159000| Training Loss: 5.495063048448203\n",
      "Epoch 1, Batch: 160000| Training Loss: 5.494269871212542\n",
      "Epoch 1, Batch: 161000| Training Loss: 5.493472762600976\n",
      "Epoch 1, Batch: 162000| Training Loss: 5.492464874642867\n",
      "Epoch 1, Batch: 163000| Training Loss: 5.4916990410638\n",
      "Epoch 1, Batch: 164000| Training Loss: 5.490926459572664\n",
      "Epoch 1, Batch: 165000| Training Loss: 5.490262041154052\n",
      "Epoch 1, Batch: 166000| Training Loss: 5.489543336193246\n",
      "Epoch 1, Batch: 167000| Training Loss: 5.488588521906002\n",
      "Epoch 1, Batch: 168000| Training Loss: 5.487881499614034\n",
      "Epoch 1, Batch: 169000| Training Loss: 5.487333842256366\n",
      "Epoch 1, Batch: 170000| Training Loss: 5.4867387431481305\n",
      "Epoch 1, Batch: 171000| Training Loss: 5.485963712265617\n",
      "Epoch 1, Batch: 172000| Training Loss: 5.485297711982284\n",
      "Epoch 1, Batch: 173000| Training Loss: 5.484672850477902\n",
      "Epoch 1, Batch: 174000| Training Loss: 5.4839916338084755\n",
      "Epoch 1, Batch: 175000| Training Loss: 5.483337750546592\n",
      "Epoch 1, Batch: 176000| Training Loss: 5.482430921528827\n",
      "Epoch 1, Batch: 177000| Training Loss: 5.481580831067037\n",
      "Epoch 1, Batch: 178000| Training Loss: 5.480839280782121\n",
      "Epoch 1, Batch: 179000| Training Loss: 5.479973958389719\n",
      "Epoch 1, Batch: 180000| Training Loss: 5.479170806895362\n",
      "Epoch 1, Batch: 181000| Training Loss: 5.47842696648408\n",
      "Epoch 1, Batch: 182000| Training Loss: 5.477701613415729\n",
      "Epoch 1, Batch: 183000| Training Loss: 5.47706718179437\n",
      "Epoch 1, Batch: 184000| Training Loss: 5.476261704621107\n",
      "Epoch 1, Batch: 185000| Training Loss: 5.475420886667354\n",
      "Epoch 1, Batch: 186000| Training Loss: 5.474559620846984\n",
      "Epoch 1, Batch: 187000| Training Loss: 5.473848331089326\n",
      "Epoch 1, Batch: 188000| Training Loss: 5.473170066366804\n",
      "Epoch 1, Batch: 189000| Training Loss: 5.472305197247753\n",
      "Epoch 1, Batch: 190000| Training Loss: 5.471510187176654\n",
      "Epoch 1, Batch: 191000| Training Loss: 5.470639244109548\n",
      "Epoch 1, Batch: 192000| Training Loss: 5.469811508235832\n",
      "Epoch 1, Batch: 193000| Training Loss: 5.469028546118365\n",
      "Epoch 1, Batch: 194000| Training Loss: 5.468376847128278\n",
      "Epoch 1, Batch: 195000| Training Loss: 5.4677496383666995\n",
      "Epoch 1, Batch: 196000| Training Loss: 5.467057378666741\n",
      "Epoch 1, Batch: 197000| Training Loss: 5.466296103309253\n",
      "Epoch 1, Batch: 198000| Training Loss: 5.465458470526368\n",
      "Epoch 1, Batch: 199000| Training Loss: 5.464802086384452\n",
      "Epoch 1, Batch: 200000| Training Loss: 5.463882579417229\n",
      "Epoch 1, Batch: 201000| Training Loss: 5.463232977067653\n",
      "Epoch 1, Batch: 202000| Training Loss: 5.46260658316329\n",
      "Epoch 1, Batch: 203000| Training Loss: 5.461755514917702\n",
      "Epoch 1, Batch: 204000| Training Loss: 5.461013325873544\n",
      "Epoch 1, Batch: 205000| Training Loss: 5.460303576799719\n",
      "Epoch 1, Batch: 206000| Training Loss: 5.45959553183051\n",
      "Epoch 1, Batch: 207000| Training Loss: 5.458817910351039\n",
      "Epoch 1, Batch: 208000| Training Loss: 5.4579068801827155\n",
      "Epoch 1, Batch: 209000| Training Loss: 5.45714976853503\n",
      "Epoch 1, Batch: 210000| Training Loss: 5.45639992568947\n",
      "Epoch 1, Batch: 211000| Training Loss: 5.455722467918531\n",
      "Epoch 1, Batch: 212000| Training Loss: 5.4550401839152824\n",
      "Epoch 1, Batch: 213000| Training Loss: 5.454302108878821\n",
      "Epoch 1, Batch: 214000| Training Loss: 5.453571264421829\n",
      "Epoch 1, Batch: 215000| Training Loss: 5.452708533657429\n",
      "Epoch 1, Batch: 216000| Training Loss: 5.45204466387188\n",
      "Epoch 1, Batch: 217000| Training Loss: 5.451311780613139\n",
      "Epoch 1, Batch: 218000| Training Loss: 5.450544447426402\n",
      "Epoch 1, Batch: 219000| Training Loss: 5.44993431896053\n",
      "Epoch 1, Batch: 220000| Training Loss: 5.449272636231509\n",
      "Epoch 1, Batch: 221000| Training Loss: 5.448463586396222\n",
      "Epoch 1, Batch: 222000| Training Loss: 5.447656012081885\n",
      "Epoch 1, Batch: 223000| Training Loss: 5.446834056065222\n",
      "Epoch 1, Batch: 224000| Training Loss: 5.446190279192158\n",
      "Epoch 1, Batch: 225000| Training Loss: 5.44542964586046\n",
      "Epoch 1, Batch: 226000| Training Loss: 5.444661946837881\n",
      "Epoch 1, Batch: 227000| Training Loss: 5.444029162696805\n",
      "Epoch 1, Batch: 228000| Training Loss: 5.4432401289584345\n",
      "Epoch 1, Batch: 229000| Training Loss: 5.442580273988465\n",
      "Epoch 1, Batch: 230000| Training Loss: 5.441890141567977\n",
      "Epoch 1, Batch: 231000| Training Loss: 5.441307095966298\n",
      "Epoch 1, Batch: 232000| Training Loss: 5.4407871612115155\n",
      "Epoch 1, Batch: 233000| Training Loss: 5.440206890319038\n",
      "Epoch 1, Batch: 234000| Training Loss: 5.439438145215695\n",
      "Epoch 1, Batch: 235000| Training Loss: 5.438712433314831\n",
      "Epoch 1, Batch: 236000| Training Loss: 5.437841684037346\n",
      "Epoch 1, Batch: 237000| Training Loss: 5.43706651231307\n",
      "Epoch 1, Batch: 238000| Training Loss: 5.4364047235250474\n",
      "Epoch 1, Batch: 239000| Training Loss: 5.435665669415286\n",
      "Epoch 1, Batch: 240000| Training Loss: 5.434994874520103\n",
      "Epoch 1, Batch: 241000| Training Loss: 5.434280779770301\n",
      "Epoch 1, Batch: 242000| Training Loss: 5.433698690854813\n",
      "Epoch 1, Batch: 243000| Training Loss: 5.433107981634729\n",
      "Epoch 1, Batch: 244000| Training Loss: 5.432350568175316\n",
      "Epoch 1, Batch: 245000| Training Loss: 5.431503779229339\n",
      "Epoch 1, Batch: 246000| Training Loss: 5.430934566590844\n",
      "Epoch 1, Batch: 247000| Training Loss: 5.430058185572566\n",
      "Epoch 1, Batch: 248000| Training Loss: 5.429313557381591\n",
      "Epoch 1, Batch: 249000| Training Loss: 5.4285170061760635\n",
      "Epoch 1, Batch: 250000| Training Loss: 5.427877990228653\n",
      "Epoch 1, Batch: 251000| Training Loss: 5.427047372962374\n",
      "Epoch 1, Batch: 252000| Training Loss: 5.42639193221974\n",
      "Epoch 1, Batch: 253000| Training Loss: 5.425616923483935\n",
      "Epoch 1, Batch: 254000| Training Loss: 5.424897937504325\n",
      "Epoch 1, Batch: 255000| Training Loss: 5.424226792197134\n",
      "Epoch 1, Batch: 256000| Training Loss: 5.423610958198085\n",
      "Epoch 1, Batch: 257000| Training Loss: 5.42292382034068\n",
      "Epoch 1, Batch: 258000| Training Loss: 5.422231103506199\n",
      "Epoch 1, Batch: 259000| Training Loss: 5.42162927170794\n",
      "Epoch 1, Batch: 260000| Training Loss: 5.420936626084951\n",
      "Epoch 1, Training Loss: 5.420494415744595, Validation Error: 95.14614355514193, Validation Top-3 Accuracy: 0.0, Training Error: 94.22236186147722\n",
      "Epoch 2, Batch: 1000| Training Loss: 5.221507576942444\n",
      "Epoch 2, Batch: 2000| Training Loss: 5.208217735886574\n",
      "Epoch 2, Batch: 3000| Training Loss: 5.2062036333084105\n",
      "Epoch 2, Batch: 4000| Training Loss: 5.202741811037064\n",
      "Epoch 2, Batch: 5000| Training Loss: 5.203331919050217\n",
      "Epoch 2, Batch: 6000| Training Loss: 5.204704580704371\n",
      "Epoch 2, Batch: 7000| Training Loss: 5.210613161597933\n",
      "Epoch 2, Batch: 8000| Training Loss: 5.21144926789403\n",
      "Epoch 2, Batch: 9000| Training Loss: 5.211666222148471\n",
      "Epoch 2, Batch: 10000| Training Loss: 5.2115158746957775\n",
      "Epoch 2, Batch: 11000| Training Loss: 5.212687180085616\n",
      "Epoch 2, Batch: 12000| Training Loss: 5.211602446118991\n",
      "Epoch 2, Batch: 13000| Training Loss: 5.209579712849397\n",
      "Epoch 2, Batch: 14000| Training Loss: 5.210907219001225\n",
      "Epoch 2, Batch: 15000| Training Loss: 5.2110888256867725\n",
      "Epoch 2, Batch: 16000| Training Loss: 5.213134609192609\n",
      "Epoch 2, Batch: 17000| Training Loss: 5.211602370570688\n",
      "Epoch 2, Batch: 18000| Training Loss: 5.213008246421814\n",
      "Epoch 2, Batch: 19000| Training Loss: 5.211568888049377\n",
      "Epoch 2, Batch: 20000| Training Loss: 5.209026491904258\n",
      "Epoch 2, Batch: 21000| Training Loss: 5.208220659028916\n",
      "Epoch 2, Batch: 22000| Training Loss: 5.2076640825813465\n",
      "Epoch 2, Batch: 23000| Training Loss: 5.207536639493445\n",
      "Epoch 2, Batch: 24000| Training Loss: 5.207525861144066\n",
      "Epoch 2, Batch: 25000| Training Loss: 5.207216598787308\n",
      "Epoch 2, Batch: 26000| Training Loss: 5.206268027727421\n",
      "Epoch 2, Batch: 27000| Training Loss: 5.205019030306074\n",
      "Epoch 2, Batch: 28000| Training Loss: 5.203549645764487\n",
      "Epoch 2, Batch: 29000| Training Loss: 5.203383795540908\n",
      "Epoch 2, Batch: 30000| Training Loss: 5.203062026921908\n",
      "Epoch 2, Batch: 31000| Training Loss: 5.20174240890626\n",
      "Epoch 2, Batch: 32000| Training Loss: 5.201539737328887\n",
      "Epoch 2, Batch: 33000| Training Loss: 5.201089748613762\n",
      "Epoch 2, Batch: 34000| Training Loss: 5.200316231776687\n",
      "Epoch 2, Batch: 35000| Training Loss: 5.199944867031915\n",
      "Epoch 2, Batch: 36000| Training Loss: 5.200100269748105\n",
      "Epoch 2, Batch: 37000| Training Loss: 5.198670774981783\n",
      "Epoch 2, Batch: 38000| Training Loss: 5.198932344254694\n",
      "Epoch 2, Batch: 39000| Training Loss: 5.198718692590028\n",
      "Epoch 2, Batch: 40000| Training Loss: 5.198159928947687\n",
      "Epoch 2, Batch: 41000| Training Loss: 5.197151049689548\n",
      "Epoch 2, Batch: 42000| Training Loss: 5.196021990123249\n",
      "Epoch 2, Batch: 43000| Training Loss: 5.195648484745691\n",
      "Epoch 2, Batch: 44000| Training Loss: 5.195278266993436\n",
      "Epoch 2, Batch: 45000| Training Loss: 5.194371059566074\n",
      "Epoch 2, Batch: 46000| Training Loss: 5.1938055591842405\n",
      "Epoch 2, Batch: 47000| Training Loss: 5.1934313519001005\n",
      "Epoch 2, Batch: 48000| Training Loss: 5.1933489395081995\n",
      "Epoch 2, Batch: 49000| Training Loss: 5.193317837213983\n",
      "Epoch 2, Batch: 50000| Training Loss: 5.19234974981308\n",
      "Epoch 2, Batch: 51000| Training Loss: 5.192837508949579\n",
      "Epoch 2, Batch: 52000| Training Loss: 5.192310510314428\n",
      "Epoch 2, Batch: 53000| Training Loss: 5.191983189231944\n",
      "Epoch 2, Batch: 54000| Training Loss: 5.191886173954717\n",
      "Epoch 2, Batch: 55000| Training Loss: 5.191514461473985\n",
      "Epoch 2, Batch: 56000| Training Loss: 5.1913564015712055\n",
      "Epoch 2, Batch: 57000| Training Loss: 5.191047711489493\n",
      "Epoch 2, Batch: 58000| Training Loss: 5.190396454276709\n",
      "Epoch 2, Batch: 59000| Training Loss: 5.190156029660823\n",
      "Epoch 2, Batch: 60000| Training Loss: 5.190392763980229\n",
      "Epoch 2, Batch: 61000| Training Loss: 5.190080442459857\n",
      "Epoch 2, Batch: 62000| Training Loss: 5.18967668457185\n",
      "Epoch 2, Batch: 63000| Training Loss: 5.189082384794477\n",
      "Epoch 2, Batch: 64000| Training Loss: 5.189557053044438\n",
      "Epoch 2, Batch: 65000| Training Loss: 5.189339194158407\n",
      "Epoch 2, Batch: 66000| Training Loss: 5.189150555791277\n",
      "Epoch 2, Batch: 67000| Training Loss: 5.188177014112473\n",
      "Epoch 2, Batch: 68000| Training Loss: 5.187885043102153\n",
      "Epoch 2, Batch: 69000| Training Loss: 5.187460353015125\n",
      "Epoch 2, Batch: 70000| Training Loss: 5.186704008139883\n",
      "Epoch 2, Batch: 71000| Training Loss: 5.186117442762348\n",
      "Epoch 2, Batch: 72000| Training Loss: 5.185949860711893\n",
      "Epoch 2, Batch: 73000| Training Loss: 5.185838440055717\n",
      "Epoch 2, Batch: 74000| Training Loss: 5.185452650543806\n",
      "Epoch 2, Batch: 75000| Training Loss: 5.184989438346227\n",
      "Epoch 2, Batch: 76000| Training Loss: 5.1841760619502315\n",
      "Epoch 2, Batch: 77000| Training Loss: 5.183797195106358\n",
      "Epoch 2, Batch: 78000| Training Loss: 5.183529454998481\n",
      "Epoch 2, Batch: 79000| Training Loss: 5.183076385253592\n",
      "Epoch 2, Batch: 80000| Training Loss: 5.183461086517572\n",
      "Epoch 2, Batch: 81000| Training Loss: 5.183475893933096\n",
      "Epoch 2, Batch: 82000| Training Loss: 5.182874030222253\n",
      "Epoch 2, Batch: 83000| Training Loss: 5.182589976800493\n",
      "Epoch 2, Batch: 84000| Training Loss: 5.182372674665281\n",
      "Epoch 2, Batch: 85000| Training Loss: 5.181856784443294\n",
      "Epoch 2, Batch: 86000| Training Loss: 5.181733969529008\n",
      "Epoch 2, Batch: 87000| Training Loss: 5.181826741565233\n",
      "Epoch 2, Batch: 88000| Training Loss: 5.181330334434455\n",
      "Epoch 2, Batch: 89000| Training Loss: 5.180971500384674\n",
      "Epoch 2, Batch: 90000| Training Loss: 5.180619368535942\n",
      "Epoch 2, Batch: 91000| Training Loss: 5.1806592393982545\n",
      "Epoch 2, Batch: 92000| Training Loss: 5.180582459248926\n",
      "Epoch 2, Batch: 93000| Training Loss: 5.180198066446089\n",
      "Epoch 2, Batch: 94000| Training Loss: 5.180041675959496\n",
      "Epoch 2, Batch: 95000| Training Loss: 5.179910745273139\n",
      "Epoch 2, Batch: 96000| Training Loss: 5.179369947665681\n",
      "Epoch 2, Batch: 97000| Training Loss: 5.1793251348660165\n",
      "Epoch 2, Batch: 98000| Training Loss: 5.179072041213512\n",
      "Epoch 2, Batch: 99000| Training Loss: 5.178910828854098\n",
      "Epoch 2, Batch: 100000| Training Loss: 5.178632622042894\n",
      "Epoch 2, Batch: 101000| Training Loss: 5.178061966399155\n",
      "Epoch 2, Batch: 102000| Training Loss: 5.1774200860974835\n",
      "Epoch 2, Batch: 103000| Training Loss: 5.177125942423506\n",
      "Epoch 2, Batch: 104000| Training Loss: 5.176587965207604\n",
      "Epoch 2, Batch: 105000| Training Loss: 5.175935939120111\n",
      "Epoch 2, Batch: 106000| Training Loss: 5.1754525925719514\n",
      "Epoch 2, Batch: 107000| Training Loss: 5.175434820860346\n",
      "Epoch 2, Batch: 108000| Training Loss: 5.174925794514241\n",
      "Epoch 2, Batch: 109000| Training Loss: 5.174724915962701\n",
      "Epoch 2, Batch: 110000| Training Loss: 5.174465928635814\n",
      "Epoch 2, Batch: 111000| Training Loss: 5.174249198364782\n",
      "Epoch 2, Batch: 112000| Training Loss: 5.173596106185445\n",
      "Epoch 2, Batch: 113000| Training Loss: 5.173078138067659\n",
      "Epoch 2, Batch: 114000| Training Loss: 5.172762126597396\n",
      "Epoch 2, Batch: 115000| Training Loss: 5.172300769050225\n",
      "Epoch 2, Batch: 116000| Training Loss: 5.1718561601422985\n",
      "Epoch 2, Batch: 117000| Training Loss: 5.17145443882392\n",
      "Epoch 2, Batch: 118000| Training Loss: 5.17106570295257\n",
      "Epoch 2, Batch: 119000| Training Loss: 5.170915944531184\n",
      "Epoch 2, Batch: 120000| Training Loss: 5.17040367026031\n",
      "Epoch 2, Batch: 121000| Training Loss: 5.169867985604223\n",
      "Epoch 2, Batch: 122000| Training Loss: 5.16929401195147\n",
      "Epoch 2, Batch: 123000| Training Loss: 5.16862184458341\n",
      "Epoch 2, Batch: 124000| Training Loss: 5.167834081471928\n",
      "Epoch 2, Batch: 125000| Training Loss: 5.16737668438816\n",
      "Epoch 2, Batch: 126000| Training Loss: 5.166935451682598\n",
      "Epoch 2, Batch: 127000| Training Loss: 5.166544797387648\n",
      "Epoch 2, Batch: 128000| Training Loss: 5.166412316908128\n",
      "Epoch 2, Batch: 129000| Training Loss: 5.1662651457259825\n",
      "Epoch 2, Batch: 130000| Training Loss: 5.165872406223187\n",
      "Epoch 2, Batch: 131000| Training Loss: 5.165483346295721\n",
      "Epoch 2, Batch: 132000| Training Loss: 5.16492391577363\n",
      "Epoch 2, Batch: 133000| Training Loss: 5.164586657442545\n",
      "Epoch 2, Batch: 134000| Training Loss: 5.164486406410808\n",
      "Epoch 2, Batch: 135000| Training Loss: 5.164090718146607\n",
      "Epoch 2, Batch: 136000| Training Loss: 5.163749144954717\n",
      "Epoch 2, Batch: 137000| Training Loss: 5.163622627823022\n",
      "Epoch 2, Batch: 138000| Training Loss: 5.163125173481478\n",
      "Epoch 2, Batch: 139000| Training Loss: 5.16267525872481\n",
      "Epoch 2, Batch: 140000| Training Loss: 5.162279858319248\n",
      "Epoch 2, Batch: 141000| Training Loss: 5.161952447181898\n",
      "Epoch 2, Batch: 142000| Training Loss: 5.161631661770209\n",
      "Epoch 2, Batch: 143000| Training Loss: 5.161317519957369\n",
      "Epoch 2, Batch: 144000| Training Loss: 5.160847786819769\n",
      "Epoch 2, Batch: 145000| Training Loss: 5.160325567170669\n",
      "Epoch 2, Batch: 146000| Training Loss: 5.159788568111315\n",
      "Epoch 2, Batch: 147000| Training Loss: 5.1594329019371346\n",
      "Epoch 2, Batch: 148000| Training Loss: 5.158983435629187\n",
      "Epoch 2, Batch: 149000| Training Loss: 5.158604100843404\n",
      "Epoch 2, Batch: 150000| Training Loss: 5.158343702227275\n",
      "Epoch 2, Batch: 151000| Training Loss: 5.158205947407034\n",
      "Epoch 2, Batch: 152000| Training Loss: 5.157745626397823\n",
      "Epoch 2, Batch: 153000| Training Loss: 5.1576891873475\n",
      "Epoch 2, Batch: 154000| Training Loss: 5.157364523037687\n",
      "Epoch 2, Batch: 155000| Training Loss: 5.157052299800996\n",
      "Epoch 2, Batch: 156000| Training Loss: 5.156657748657923\n",
      "Epoch 2, Batch: 157000| Training Loss: 5.156360243753263\n",
      "Epoch 2, Batch: 158000| Training Loss: 5.155748381059381\n",
      "Epoch 2, Batch: 159000| Training Loss: 5.155449246132149\n",
      "Epoch 2, Batch: 160000| Training Loss: 5.15510833786875\n",
      "Epoch 2, Batch: 161000| Training Loss: 5.154803075135865\n",
      "Epoch 2, Batch: 162000| Training Loss: 5.154214646649949\n",
      "Epoch 2, Batch: 163000| Training Loss: 5.1538363836674606\n",
      "Epoch 2, Batch: 164000| Training Loss: 5.153515291831842\n",
      "Epoch 2, Batch: 165000| Training Loss: 5.1532379807602275\n",
      "Epoch 2, Batch: 166000| Training Loss: 5.152886424241296\n",
      "Epoch 2, Batch: 167000| Training Loss: 5.152304818797254\n",
      "Epoch 2, Batch: 168000| Training Loss: 5.152053330593166\n",
      "Epoch 2, Batch: 169000| Training Loss: 5.151978073743673\n",
      "Epoch 2, Batch: 170000| Training Loss: 5.151834203478869\n",
      "Epoch 2, Batch: 171000| Training Loss: 5.151491231214234\n",
      "Epoch 2, Batch: 172000| Training Loss: 5.151300199408864\n",
      "Epoch 2, Batch: 173000| Training Loss: 5.15114406439817\n",
      "Epoch 2, Batch: 174000| Training Loss: 5.150964333301988\n",
      "Epoch 2, Batch: 175000| Training Loss: 5.150748790260724\n",
      "Epoch 2, Batch: 176000| Training Loss: 5.150282830690118\n",
      "Epoch 2, Batch: 177000| Training Loss: 5.149896207280752\n",
      "Epoch 2, Batch: 178000| Training Loss: 5.149586690367608\n",
      "Epoch 2, Batch: 179000| Training Loss: 5.149221555489402\n",
      "Epoch 2, Batch: 180000| Training Loss: 5.1488554249796605\n",
      "Epoch 2, Batch: 181000| Training Loss: 5.148532347545439\n",
      "Epoch 2, Batch: 182000| Training Loss: 5.148252319281573\n",
      "Epoch 2, Batch: 183000| Training Loss: 5.148076910082108\n",
      "Epoch 2, Batch: 184000| Training Loss: 5.147735375289684\n",
      "Epoch 2, Batch: 185000| Training Loss: 5.147337430165909\n",
      "Epoch 2, Batch: 186000| Training Loss: 5.14693674325238\n",
      "Epoch 2, Batch: 187000| Training Loss: 5.146643838295325\n",
      "Epoch 2, Batch: 188000| Training Loss: 5.146404712904641\n",
      "Epoch 2, Batch: 189000| Training Loss: 5.145963636559153\n",
      "Epoch 2, Batch: 190000| Training Loss: 5.145621919406715\n",
      "Epoch 2, Batch: 191000| Training Loss: 5.145212371718197\n",
      "Epoch 2, Batch: 192000| Training Loss: 5.144837123418227\n",
      "Epoch 2, Batch: 193000| Training Loss: 5.144494465161482\n",
      "Epoch 2, Batch: 194000| Training Loss: 5.1442951515378414\n",
      "Epoch 2, Batch: 195000| Training Loss: 5.144138933876844\n",
      "Epoch 2, Batch: 196000| Training Loss: 5.14389930185189\n",
      "Epoch 2, Batch: 197000| Training Loss: 5.143563590145958\n",
      "Epoch 2, Batch: 198000| Training Loss: 5.14313398479151\n",
      "Epoch 2, Batch: 199000| Training Loss: 5.142879232927183\n",
      "Epoch 2, Batch: 200000| Training Loss: 5.142369782742858\n",
      "Epoch 2, Batch: 201000| Training Loss: 5.142160550553407\n",
      "Epoch 2, Batch: 202000| Training Loss: 5.141955510151859\n",
      "Epoch 2, Batch: 203000| Training Loss: 5.141534594041961\n",
      "Epoch 2, Batch: 204000| Training Loss: 5.1412138644883445\n",
      "Epoch 2, Batch: 205000| Training Loss: 5.14093544145619\n",
      "Epoch 2, Batch: 206000| Training Loss: 5.14068628777173\n",
      "Epoch 2, Batch: 207000| Training Loss: 5.140356833074979\n",
      "Epoch 2, Batch: 208000| Training Loss: 5.139903727715405\n",
      "Epoch 2, Batch: 209000| Training Loss: 5.139578729652902\n",
      "Epoch 2, Batch: 210000| Training Loss: 5.139269667068549\n",
      "Epoch 2, Batch: 211000| Training Loss: 5.139042796437209\n",
      "Epoch 2, Batch: 212000| Training Loss: 5.138809065299776\n",
      "Epoch 2, Batch: 213000| Training Loss: 5.138468447558197\n",
      "Epoch 2, Batch: 214000| Training Loss: 5.138177937501502\n",
      "Epoch 2, Batch: 215000| Training Loss: 5.13768145909254\n",
      "Epoch 2, Batch: 216000| Training Loss: 5.137366914679055\n",
      "Epoch 2, Batch: 217000| Training Loss: 5.137050227945851\n",
      "Epoch 2, Batch: 218000| Training Loss: 5.1366885982046435\n",
      "Epoch 2, Batch: 219000| Training Loss: 5.136514283024013\n",
      "Epoch 2, Batch: 220000| Training Loss: 5.136284292082895\n",
      "Epoch 2, Batch: 221000| Training Loss: 5.135892824893084\n",
      "Epoch 2, Batch: 222000| Training Loss: 5.135507255652466\n",
      "Epoch 2, Batch: 223000| Training Loss: 5.135152527453654\n",
      "Epoch 2, Batch: 224000| Training Loss: 5.134958293763122\n",
      "Epoch 2, Batch: 225000| Training Loss: 5.134709170269966\n",
      "Epoch 2, Batch: 226000| Training Loss: 5.134357154457443\n",
      "Epoch 2, Batch: 227000| Training Loss: 5.134148499838056\n",
      "Epoch 2, Batch: 228000| Training Loss: 5.133759410244854\n",
      "Epoch 2, Batch: 229000| Training Loss: 5.1335259435577685\n",
      "Epoch 2, Batch: 230000| Training Loss: 5.133262130527393\n",
      "Epoch 2, Batch: 231000| Training Loss: 5.133114103743008\n",
      "Epoch 2, Batch: 232000| Training Loss: 5.133010402771419\n",
      "Epoch 2, Batch: 233000| Training Loss: 5.132828102583026\n",
      "Epoch 2, Batch: 234000| Training Loss: 5.132428303909608\n",
      "Epoch 2, Batch: 235000| Training Loss: 5.132130348655518\n",
      "Epoch 2, Batch: 236000| Training Loss: 5.131695116190082\n",
      "Epoch 2, Batch: 237000| Training Loss: 5.131378849768438\n",
      "Epoch 2, Batch: 238000| Training Loss: 5.131182433008647\n",
      "Epoch 2, Batch: 239000| Training Loss: 5.1308895772077046\n",
      "Epoch 2, Batch: 240000| Training Loss: 5.130676330499351\n",
      "Epoch 2, Batch: 241000| Training Loss: 5.1304407256317335\n",
      "Epoch 2, Batch: 242000| Training Loss: 5.130282199562581\n",
      "Epoch 2, Batch: 243000| Training Loss: 5.130110335398603\n",
      "Epoch 2, Batch: 244000| Training Loss: 5.129806129799026\n",
      "Epoch 2, Batch: 245000| Training Loss: 5.129450046209899\n",
      "Epoch 2, Batch: 246000| Training Loss: 5.129343612463979\n",
      "Epoch 2, Batch: 247000| Training Loss: 5.1289563068248\n",
      "Epoch 2, Batch: 248000| Training Loss: 5.1286693378364845\n",
      "Epoch 2, Batch: 249000| Training Loss: 5.128335970138929\n",
      "Epoch 2, Batch: 250000| Training Loss: 5.128157585168362\n",
      "Epoch 2, Batch: 251000| Training Loss: 5.127760109691031\n",
      "Epoch 2, Batch: 252000| Training Loss: 5.127541651844505\n",
      "Epoch 2, Batch: 253000| Training Loss: 5.12720063445455\n",
      "Epoch 2, Batch: 254000| Training Loss: 5.126968624570708\n",
      "Epoch 2, Batch: 255000| Training Loss: 5.126710463899257\n",
      "Epoch 2, Batch: 256000| Training Loss: 5.126534927814733\n",
      "Epoch 2, Batch: 257000| Training Loss: 5.126275454284152\n",
      "Epoch 2, Batch: 258000| Training Loss: 5.12598928065032\n",
      "Epoch 2, Batch: 259000| Training Loss: 5.125796976537318\n",
      "Epoch 2, Batch: 260000| Training Loss: 5.1254627052687685\n",
      "Epoch 2, Training Loss: 5.125223002947437, Validation Error: 94.03356212191717, Validation Top-3 Accuracy: 0.0, Training Error: 92.34904208990193\n",
      "Epoch 3, Batch: 1000| Training Loss: 5.029629647493363\n",
      "Epoch 3, Batch: 2000| Training Loss: 5.014862880706787\n",
      "Epoch 3, Batch: 3000| Training Loss: 5.012406409184138\n",
      "Epoch 3, Batch: 4000| Training Loss: 5.008575411617756\n",
      "Epoch 3, Batch: 5000| Training Loss: 5.0132717092990875\n",
      "Epoch 3, Batch: 6000| Training Loss: 5.016599321305752\n",
      "Epoch 3, Batch: 7000| Training Loss: 5.022922219702176\n",
      "Epoch 3, Batch: 8000| Training Loss: 5.024416752383113\n",
      "Epoch 3, Batch: 9000| Training Loss: 5.025372951494323\n",
      "Epoch 3, Batch: 10000| Training Loss: 5.025706767904758\n",
      "Epoch 3, Batch: 11000| Training Loss: 5.02738268467513\n",
      "Epoch 3, Batch: 12000| Training Loss: 5.027317465911309\n",
      "Epoch 3, Batch: 13000| Training Loss: 5.025552734787648\n",
      "Epoch 3, Batch: 14000| Training Loss: 5.026437862472875\n",
      "Epoch 3, Batch: 15000| Training Loss: 5.026711581985156\n",
      "Epoch 3, Batch: 16000| Training Loss: 5.028956543795765\n",
      "Epoch 3, Batch: 17000| Training Loss: 5.027673351869864\n",
      "Epoch 3, Batch: 18000| Training Loss: 5.029692707690928\n",
      "Epoch 3, Batch: 19000| Training Loss: 5.0284007088949805\n",
      "Epoch 3, Batch: 20000| Training Loss: 5.026086138421297\n",
      "Epoch 3, Batch: 21000| Training Loss: 5.025163033400263\n",
      "Epoch 3, Batch: 22000| Training Loss: 5.025052746236324\n",
      "Epoch 3, Batch: 23000| Training Loss: 5.025201363910799\n",
      "Epoch 3, Batch: 24000| Training Loss: 5.025382518167297\n",
      "Epoch 3, Batch: 25000| Training Loss: 5.025161917643547\n",
      "Epoch 3, Batch: 26000| Training Loss: 5.024383073765498\n",
      "Epoch 3, Batch: 27000| Training Loss: 5.023696683684985\n",
      "Epoch 3, Batch: 28000| Training Loss: 5.022653160985027\n",
      "Epoch 3, Batch: 29000| Training Loss: 5.023153734852528\n",
      "Epoch 3, Batch: 30000| Training Loss: 5.023274757365385\n",
      "Epoch 3, Batch: 31000| Training Loss: 5.0223753872648365\n",
      "Epoch 3, Batch: 32000| Training Loss: 5.022731395799666\n",
      "Epoch 3, Batch: 33000| Training Loss: 5.022686989050923\n",
      "Epoch 3, Batch: 34000| Training Loss: 5.021970952107626\n",
      "Epoch 3, Batch: 35000| Training Loss: 5.021489188824381\n",
      "Epoch 3, Batch: 36000| Training Loss: 5.0216215178337364\n",
      "Epoch 3, Batch: 37000| Training Loss: 5.020491767680323\n",
      "Epoch 3, Batch: 38000| Training Loss: 5.021288513971003\n",
      "Epoch 3, Batch: 39000| Training Loss: 5.021387996756114\n",
      "Epoch 3, Batch: 40000| Training Loss: 5.02130261259973\n",
      "Epoch 3, Batch: 41000| Training Loss: 5.0208155131427255\n",
      "Epoch 3, Batch: 42000| Training Loss: 5.020109754985287\n",
      "Epoch 3, Batch: 43000| Training Loss: 5.020081697987956\n",
      "Epoch 3, Batch: 44000| Training Loss: 5.020022493798624\n",
      "Epoch 3, Batch: 45000| Training Loss: 5.019578827457957\n",
      "Epoch 3, Batch: 46000| Training Loss: 5.019378249844779\n",
      "Epoch 3, Batch: 47000| Training Loss: 5.019388335200066\n",
      "Epoch 3, Batch: 48000| Training Loss: 5.0196736569876474\n",
      "Epoch 3, Batch: 49000| Training Loss: 5.019803863333196\n",
      "Epoch 3, Batch: 50000| Training Loss: 5.018982890717983\n",
      "Epoch 3, Batch: 51000| Training Loss: 5.019516682356012\n",
      "Epoch 3, Batch: 52000| Training Loss: 5.019148605266443\n",
      "Epoch 3, Batch: 53000| Training Loss: 5.018926886434825\n",
      "Epoch 3, Batch: 54000| Training Loss: 5.018877719678261\n",
      "Epoch 3, Batch: 55000| Training Loss: 5.018543900002133\n",
      "Epoch 3, Batch: 56000| Training Loss: 5.018463885117854\n",
      "Epoch 3, Batch: 57000| Training Loss: 5.018158238500879\n",
      "Epoch 3, Batch: 58000| Training Loss: 5.0176249542174665\n",
      "Epoch 3, Batch: 59000| Training Loss: 5.017493975338289\n",
      "Epoch 3, Batch: 60000| Training Loss: 5.017887820440531\n",
      "Epoch 3, Batch: 61000| Training Loss: 5.017895336833156\n",
      "Epoch 3, Batch: 62000| Training Loss: 5.017538896585664\n",
      "Epoch 3, Batch: 63000| Training Loss: 5.0171233420580155\n",
      "Epoch 3, Batch: 64000| Training Loss: 5.017718190060928\n",
      "Epoch 3, Batch: 65000| Training Loss: 5.0176285373852805\n",
      "Epoch 3, Batch: 66000| Training Loss: 5.017587408399943\n",
      "Epoch 3, Batch: 67000| Training Loss: 5.016731476785532\n",
      "Epoch 3, Batch: 68000| Training Loss: 5.016693672500989\n",
      "Epoch 3, Batch: 69000| Training Loss: 5.0164269300491915\n",
      "Epoch 3, Batch: 70000| Training Loss: 5.015959426888398\n",
      "Epoch 3, Batch: 71000| Training Loss: 5.015551261517364\n",
      "Epoch 3, Batch: 72000| Training Loss: 5.01540071320037\n",
      "Epoch 3, Batch: 73000| Training Loss: 5.015445902747651\n",
      "Epoch 3, Batch: 74000| Training Loss: 5.015066619133627\n",
      "Epoch 3, Batch: 75000| Training Loss: 5.014703146271706\n",
      "Epoch 3, Batch: 76000| Training Loss: 5.014068665778951\n",
      "Epoch 3, Batch: 77000| Training Loss: 5.013763279978331\n",
      "Epoch 3, Batch: 78000| Training Loss: 5.013586924068439\n",
      "Epoch 3, Batch: 79000| Training Loss: 5.013276191954371\n",
      "Epoch 3, Batch: 80000| Training Loss: 5.013769039492309\n",
      "Epoch 3, Batch: 81000| Training Loss: 5.013875327782866\n",
      "Epoch 3, Batch: 82000| Training Loss: 5.013389716437677\n",
      "Epoch 3, Batch: 83000| Training Loss: 5.01319064761788\n",
      "Epoch 3, Batch: 84000| Training Loss: 5.013074687763339\n",
      "Epoch 3, Batch: 85000| Training Loss: 5.01271788918972\n",
      "Epoch 3, Batch: 86000| Training Loss: 5.012753008400285\n",
      "Epoch 3, Batch: 87000| Training Loss: 5.013082621789526\n",
      "Epoch 3, Batch: 88000| Training Loss: 5.012676433970983\n",
      "Epoch 3, Batch: 89000| Training Loss: 5.012479434408498\n",
      "Epoch 3, Batch: 90000| Training Loss: 5.012281610529953\n",
      "Epoch 3, Batch: 91000| Training Loss: 5.012376091067607\n",
      "Epoch 3, Batch: 92000| Training Loss: 5.012373132338991\n",
      "Epoch 3, Batch: 93000| Training Loss: 5.012091318905995\n",
      "Epoch 3, Batch: 94000| Training Loss: 5.0120221138114625\n",
      "Epoch 3, Batch: 95000| Training Loss: 5.01201439466602\n",
      "Epoch 3, Batch: 96000| Training Loss: 5.01156706195573\n",
      "Epoch 3, Batch: 97000| Training Loss: 5.011646868686086\n",
      "Epoch 3, Batch: 98000| Training Loss: 5.011629364512404\n",
      "Epoch 3, Batch: 99000| Training Loss: 5.011618548588319\n",
      "Epoch 3, Batch: 100000| Training Loss: 5.011427541382313\n",
      "Epoch 3, Batch: 101000| Training Loss: 5.010905452282122\n",
      "Epoch 3, Batch: 102000| Training Loss: 5.010360026897168\n",
      "Epoch 3, Batch: 103000| Training Loss: 5.010172001729891\n",
      "Epoch 3, Batch: 104000| Training Loss: 5.009792554069024\n",
      "Epoch 3, Batch: 105000| Training Loss: 5.009236859267099\n",
      "Epoch 3, Batch: 106000| Training Loss: 5.008897573149429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[159], line 32\u001b[0m, in \u001b[0;36mtrain_decoder\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Clip it\u001b[39;00m\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# For logging purposes\u001b[39;00m\n\u001b[1;32m     35\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/cis400/enpoisson/.venv/lib/python3.9/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_decoder(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('transformer-decoder-4-22.png')  # This will save the plot as an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
