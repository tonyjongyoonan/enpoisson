{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with transformers\n",
    "Transformers remain as a promising replacement of RNNs due to their parallelizability. However, RNNs are unique in their hidden state which tends to be uniquely useful for games. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import numpy as np\n",
    "import utils\n",
    "import data_processing_utils\n",
    "import models\n",
    "\n",
    "importlib.reload(data_processing_utils)\n",
    "from data_processing_utils import *\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.read_csv('../data/haha-longer-mid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def df_to_data_with_special_tokens(df, fixed_window=False, fixed_window_size=16, sampling_rate=1, algebraic_notation=True, old_vocab = None):\n",
    "    \"\"\"\n",
    "    Input: Dataframe of training data in which each row represents a full game played between players\n",
    "    Output: List in which each item represents some game's history up until a particular move, List in the same order in which the associated label is the following move\n",
    "    \"\"\"\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "    vocab = old_vocab \n",
    "    if vocab is None:\n",
    "        vocab = VocabularyWithCLS()\n",
    "    board = chess.Board()\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        # Turn the game into a list of moves\n",
    "        encoded_moves = [1]\n",
    "        for move in moves:\n",
    "            # Create a move object from the coordinate notation\n",
    "            move_obj = chess.Move.from_uci(move)\n",
    "            if move_obj not in board.legal_moves:\n",
    "                break \n",
    "            else:\n",
    "                if algebraic_notation:\n",
    "                    algebraic_move = board.san(move_obj)\n",
    "                    board.push(move_obj)\n",
    "                    vocab.add_move(algebraic_move)\n",
    "                    encoded_move = vocab.get_id(algebraic_move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "                else:\n",
    "                    encoded_move = vocab.get_id(move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "        board.reset()\n",
    "        # Turn the list of moves into subsequences\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            if random.uniform(0, 1) <= sampling_rate:\n",
    "                subseq = encoded_moves[0:i+1]\n",
    "                if fixed_window and len(subseq) > fixed_window_size:\n",
    "                    subseq = subseq[-fixed_window_size:]\n",
    "                label = encoded_moves[i+1]\n",
    "                subsequences.append(subseq)\n",
    "                next_moves.append(label)\n",
    "\n",
    "    return subsequences, next_moves, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/full_vocab_no_checkmate.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)\n",
    "    \n",
    "# with open('./../data/full_vocab.pkl', 'rb') as inp:\n",
    "#     vocab = pickle.load(inp)\n",
    "\n",
    "\n",
    "trainX, trainY, vocab = df_to_sequential_data(grouped_df, fixed_window=True, sampling_rate=0.5, old_vocab=vocab)\n",
    "trainX, trainX_seqlengths  = pad_sequences(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trainX\n",
    "dtype_trainX = np.int32  # or the correct dtype for your data\n",
    "shape_trainX = (2161482, 750)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/transformer/jan/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int32 # or the correct dtype for your data\n",
    "shape_trainY = (2161482, 7)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/transformer/jan/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "with open('./../data/transformer/jan/vocab.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[:1,:4])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences, self.labels = sequences, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11891042\n"
     ]
    }
   ],
   "source": [
    "dataset = TransformerDataset(trainX, trainY)\n",
    "total_size = len(dataset)\n",
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.97 * total_size)\n",
    "val_size = int(total_size * 0.02)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a long enough `pe` to be sliced according to any input `x` up to `max_len`\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is assumed to be of shape [batch_size, seq_length, d_model]\n",
    "        # Adjust `pe` to match the dimensions of `x`\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ChessTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformerDecoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model =d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Only decoder is needed for autoregressive models\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                       nhead=nhead, \n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True,),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # Memory is optional and could be used for incorporating encoder states in a hybrid model\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device)\n",
    "\n",
    "        # Embedding and Positional Encoding for tgt\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb).to(tgt.device)\n",
    "\n",
    "        # Autoregressive decoding using the Transformer Decoder\n",
    "        output = self.transformer_decoder(tgt_emb, memory=None,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          tgt_is_causal = True,\n",
    "                                          tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\" Generate a square mask for the sequence to mask out subsequent positions. \"\"\"\n",
    "        sz = tgt.size(1)\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=tgt.device, dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class ChessTransformerTwo(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=16, dropout=0.1):\n",
    "        super(ChessTransformerTwo, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        self.pos_encoder_two = PositionalEncoding(d_model, dropout, 1)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers, \n",
    "                                          batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Create source padding mask\n",
    "        src_padding_mask = self.create_padding_mask(src).to(src.device, non_blocking=True)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device, non_blocking=True)\n",
    "        # Embedding and Positional Encoding for src\n",
    "\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb).to(tgt.device, non_blocking=True)\n",
    "        \n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        tgt_emb = self.pos_encoder_two(tgt_emb).to(tgt.device, non_blocking=True)\n",
    "    \n",
    "        # Transformer\n",
    "        output = self.transformer(src_emb, tgt_emb, \n",
    "                                  src_key_padding_mask=src_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_padding_mask)\n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with `True`.\n",
    "        This mask ensures that for any position `i` in `tgt`, the decoder's self-attention mechanism\n",
    "        can only attend to positions at or before `i`.\n",
    "        \n",
    "        Args:\n",
    "            tgt (Tensor): The target input tensor of shape [batch_size, tgt_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A mask of shape [tgt_len, tgt_len] where `True` indicates that attention is not allowed.\n",
    "        \"\"\"\n",
    "        # tgt_len could be derived from the second dimension of tgt\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Generate an upper triangular matrix with `True` in the upper triangle\n",
    "        mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "class ChessTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformerDecoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model =d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Only decoder is needed for autoregressive models\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                       nhead=nhead, \n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True,),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # Memory is optional and could be used for incorporating encoder states in a hybrid model\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device)\n",
    "\n",
    "        # Embedding and Positional Encoding for tgt\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb).to(tgt.device)\n",
    "\n",
    "        # Autoregressive decoding using the Transformer Decoder\n",
    "        output = self.transformer_decoder(tgt_emb, memory=None,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          tgt_is_causal = True,\n",
    "                                          tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\" Generate a square mask for the sequence to mask out subsequent positions. \"\"\"\n",
    "        sz = tgt.size(1)\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=tgt.device, dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class ChessTransformer(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers, \n",
    "                                          batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        # Create source padding mask\n",
    "        src_padding_mask = self.create_padding_mask(src).to(src.device)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        # Embedding and Positional Encoding for src\n",
    "\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb).to(tgt.device)\n",
    "        \n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        tgt_emb = self.pos_encoder(tgt_emb).to(tgt.device)\n",
    "    \n",
    "        # Transformer\n",
    "        output = self.transformer(src_emb, tgt_emb, \n",
    "                                  src_key_padding_mask=src_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                  tgt_is_causal = True,\n",
    "                                  src_is_causal = True, \n",
    "                                  src_mask = self.square_subsequent_mask(src).to(src.device),\n",
    "                                  tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device))\n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with `True`.\n",
    "        This mask ensures that for any position `i` in `tgt`, the decoder's self-attention mechanism\n",
    "        can only attend to positions at or before `i`.\n",
    "        \n",
    "        Args:\n",
    "            tgt (Tensor): The target input tensor of shape [batch_size, tgt_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A mask of shape [tgt_len, tgt_len] where `True` indicates that attention is not allowed.\n",
    "        \"\"\"\n",
    "        # tgt_len could be derived from the second dimension of tgt\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Generate an upper triangular matrix with `True` in the upper triangle\n",
    "        mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "    def generate_sequence(self, src, src_length, start_symbol_id, sep_token_id, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate a sequence autoregressively using the trained transformer model.\n",
    "\n",
    "        Args:\n",
    "        - src (Tensor): The input source sequence tensor.\n",
    "        - src_length (Tensor): The length of the source sequence.\n",
    "        - start_symbol_id (int): The ID of the start symbol to begin generation.\n",
    "        - sep_token_id (int): The ID of the SEP token for sequence termination.\n",
    "        - max_length (int): Maximum length of the generated sequence to prevent infinite loops.\n",
    "\n",
    "        Returns:\n",
    "        - The generated sequence tensor.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure the model is in eval mode\n",
    "\n",
    "        # Initialize the target sequence with the start symbol\n",
    "        tgt = torch.tensor([start_symbol_id], dtype=torch.long).to(src.device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Assuming src_length is a tensor with the length of src. Adjust as needed.\n",
    "\n",
    "            # Perform a forward pass to get logits for the next token\n",
    "            logits = self.forward(src, src_length, tgt, src)\n",
    "            # Get the last token logits and apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            # get most likely token from probs\n",
    "            next_token = torch.max(probs, 1)\n",
    "            \n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt = torch.cat((tgt, next_token), dim=1)\n",
    "            \n",
    "            # Check if the <SEP> token is generated\n",
    "            if next_token.item() == sep_token_id:\n",
    "                break\n",
    "\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate top-3 accuracy\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    top3 = torch.topk(y_pred, 3, dim=1).indices\n",
    "    correct = top3.eq(y_true.view(-1, 1).expand_as(top3))\n",
    "    return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "def train_last_token(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "\n",
    "            # Check if your data contains zeros as padding, and modify 'greater(0)' if different padding\n",
    "            non_zero_mask = (sequences != 0) \n",
    "            last_non_zero_indices = non_zero_mask.long().argmax(dim=1)  # Gets the index of the first zero after last non-zero\n",
    "            \n",
    "            # Correction for sequences entirely non-zero or wrongly identified first zero indices\n",
    "            last_non_zero_indices = torch.where(\n",
    "                non_zero_mask.any(dim=1),\n",
    "                non_zero_mask.sum(dim=1) - 1,  # Last non-zero index\n",
    "                torch.tensor(0).to(device)  # Default to 0 if no non-zero found (edge case)\n",
    "            )\n",
    "\n",
    "            # Gather the last non-zero tokens for each sequence in the batch\n",
    "            last_non_zero_tokens = sequences[torch.arange(sequences.size(0)), last_non_zero_indices]\n",
    "\n",
    "            # Forward Pass using the last non-zero token\n",
    "\n",
    "\n",
    "            logits = model(sequences, last_non_zero_tokens.unsqueeze(1))\n",
    "            loss = criterion(logits.view(-1, model.vocab_size),labels)\n",
    "\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, dim=2)\n",
    "            predicted_last = predicted[:, -1]\n",
    "            train_correct += (predicted_last == labels).sum().item()\n",
    "            train_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "            count += 1\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count} | Training Accuracy: {train_correct/train_total}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                val_loss = 0\n",
    "\n",
    "                for sequences, labels in val_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    # Check if your data contains zeros as padding, and modify 'greater(0)' if different padding\n",
    "                    non_zero_mask = (sequences != 0) \n",
    "                    last_non_zero_indices = non_zero_mask.long().argmax(dim=1)  # Gets the index of the first zero after last non-zero\n",
    "                    \n",
    "                    # Correction for sequences entirely non-zero or wrongly identified first zero indices\n",
    "                    last_non_zero_indices = torch.where(\n",
    "                        non_zero_mask.any(dim=1),\n",
    "                        non_zero_mask.sum(dim=1) - 1,  # Last non-zero index\n",
    "                        torch.tensor(0).to(device)  # Default to 0 if no non-zero found (edge case)\n",
    "                    )\n",
    "\n",
    "                    # Gather the last non-zero tokens for each sequence in the batch\n",
    "                    last_non_zero_tokens = sequences[torch.arange(sequences.size(0)), last_non_zero_indices]\n",
    "\n",
    "                    # Forward Pass using the last non-zero token\n",
    "\n",
    "\n",
    "                    logits = model(sequences, last_non_zero_tokens.unsqueeze(1))\n",
    "                    loss = criterion(logits.view(-1, model.vocab_size),labels)\n",
    "\n",
    "                    # For logging purposes\n",
    "                    val_loss += loss.item()\n",
    "                     # Calculate accuracy\n",
    "                    _, predicted = torch.max(logits, dim=2)\n",
    "                    predicted_last = predicted[:, -1]\n",
    "                    val_correct += (predicted_last == labels).sum().item()\n",
    "                    val_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "                    count += 1\n",
    "                val_loss_values.append(val_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values\n",
    "\n",
    "def train_decoder(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            tgt_labels = torch.cat([sequences[:,1:],labels.unsqueeze(1)],dim=1).to(device)\n",
    "            logits = model(sequences, sequences)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), tgt_labels.view(-1))\n",
    "\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, dim=2)\n",
    "            # Selecting only the last token from each sequence in the batch\n",
    "            predicted_last = predicted[:, -1]\n",
    "            tgt_labels_last = tgt_labels[:, -1]\n",
    "\n",
    "            # Updating the correct count based on the last token predictions\n",
    "            train_correct += (predicted_last == tgt_labels_last).sum().item()\n",
    "            # Updating the total count to reflect that only one token per sequence is considered\n",
    "            train_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "            # train_correct += (predicted == tgt_labels).sum().item()\n",
    "            # train_total += tgt_labels.numel()\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count} | Training Accuracy: {train_correct/train_total}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                val_loss = 0\n",
    "\n",
    "                for sequences, labels in val_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    tgt_labels = torch.cat([sequences[:,1:],labels.unsqueeze(1)],dim=1).to(device)\n",
    "                    logits = model(sequences, sequences)\n",
    "                    loss = criterion(logits.view(-1, model.vocab_size), tgt_labels.view(-1))\n",
    "\n",
    "                    # For logging purposes\n",
    "                    val_loss += loss.item()\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted = torch.max(logits, dim=2)\n",
    "                    # Selecting only the last token from each sequence in the batch\n",
    "                    predicted_last = predicted[:, -1]\n",
    "                    tgt_labels_last = tgt_labels[:, -1]\n",
    "\n",
    "                    # Updating the correct count based on the last token predictions\n",
    "                    val_correct += (predicted_last == tgt_labels_last).sum().item()\n",
    "                    # Updating the total count to reflect that only one token per sequence is considered\n",
    "                    val_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "\n",
    "                val_loss_values.append(val_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values\n",
    "\n",
    "def train_transformer(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            count += 1\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            logits = model(sequences, labels)\n",
    "            print(logits)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), labels.contiguous().view(-1))\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # _, predicted = torch.max(output.data, 1)\n",
    "            # train_total += labels.size(0)\n",
    "            # train_correct += (predicted == labels).sum().item()\n",
    "            # Get the predicted class indices for each position in each sequence\n",
    "            _, predicted = torch.max(logits.data, dim=2)  # Shape: (batch_size, seq_length)\n",
    "            correct_predictions = predicted == labels  # Shape: (batch_size, seq_length)\n",
    "            correct_sequences = correct_predictions.all(dim=1)  # Shape: (batch_size)\n",
    "            train_correct += correct_sequences.sum().item()\n",
    "            train_total += labels.size(0) \n",
    "            break\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        # if val_loader is not None:\n",
    "        #     with torch.no_grad():\n",
    "        #         val_correct = 0\n",
    "        #         val_total = 0\n",
    "        #         val_top3_correct = 0\n",
    "        #         validation_loss = 0\n",
    "\n",
    "        #         for sequences, lengths, labels in val_loader:\n",
    "        #             sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "        #             outputs = model.generate(sequences, lengths)\n",
    "        #             _, predicted = torch.max(outputs.data, 1)\n",
    "        #             val_total += labels.size(0)\n",
    "        #             val_correct += (predicted == labels).sum().item()\n",
    "        #             val_top3_correct += top_3_accuracy(labels, outputs) * labels.size(0)\n",
    "        #             loss = criterion(outputs, labels)\n",
    "        #             validation_loss += loss.item()\n",
    "\n",
    "        #         val_loss_values.append(validation_loss / len(val_loader))\n",
    "        #         val_accuracy = 100 * val_correct / val_total\n",
    "        #         val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "        #         val_error.append(100 - val_accuracy)\n",
    "        #         val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207314\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 128\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.65 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 4.277270253896713 | Training Accuracy: 0.06821875\n",
      "Epoch 1, Batch: 2000| Training Loss: 3.3310693102478983 | Training Accuracy: 0.071109375\n",
      "Epoch 1, Batch: 3000| Training Loss: 2.7611966275374096 | Training Accuracy: 0.072140625\n",
      "Epoch 1, Batch: 4000| Training Loss: 2.406634035587311 | Training Accuracy: 0.07366796875\n",
      "Epoch 1, Batch: 5000| Training Loss: 2.1669605624198915 | Training Accuracy: 0.075484375\n",
      "Epoch 1, Batch: 6000| Training Loss: 1.9921583703259627 | Training Accuracy: 0.07734114583333333\n",
      "Epoch 1, Batch: 7000| Training Loss: 1.8581724826863835 | Training Accuracy: 0.07928571428571428\n",
      "Epoch 1, Batch: 8000| Training Loss: 1.7539799907431006 | Training Accuracy: 0.0808203125\n",
      "Epoch 1, Batch: 9000| Training Loss: 1.6695201332966487 | Training Accuracy: 0.08231770833333334\n",
      "Epoch 1, Batch: 10000| Training Loss: 1.5988527409672737 | Training Accuracy: 0.0835015625\n",
      "Epoch 1, Batch: 11000| Training Loss: 1.5384822490432046 | Training Accuracy: 0.08468039772727273\n",
      "Epoch 1, Batch: 12000| Training Loss: 1.4869812156309685 | Training Accuracy: 0.08593489583333333\n",
      "Epoch 1, Batch: 13000| Training Loss: 1.441819060435662 | Training Accuracy: 0.08707331730769231\n",
      "Epoch 1, Batch: 14000| Training Loss: 1.4017801074768816 | Training Accuracy: 0.08806473214285715\n",
      "Epoch 1, Batch: 15000| Training Loss: 1.3669130097587903 | Training Accuracy: 0.08905\n",
      "Epoch 1, Batch: 16000| Training Loss: 1.3354366956166923 | Training Accuracy: 0.09008203125\n",
      "Epoch 1, Batch: 17000| Training Loss: 1.3071811314400505 | Training Accuracy: 0.09099264705882353\n",
      "Epoch 1, Batch: 18000| Training Loss: 1.2814611663884588 | Training Accuracy: 0.09181944444444444\n",
      "Epoch 1, Batch: 19000| Training Loss: 1.2580664449710595 | Training Accuracy: 0.09247697368421053\n",
      "Epoch 1, Batch: 20000| Training Loss: 1.2361173732191324 | Training Accuracy: 0.09329609375\n",
      "Epoch 1, Batch: 21000| Training Loss: 1.216264807692596 | Training Accuracy: 0.09394345238095238\n",
      "Epoch 1, Batch: 22000| Training Loss: 1.1978358969254927 | Training Accuracy: 0.09464630681818181\n",
      "Epoch 1, Batch: 23000| Training Loss: 1.1807987880240316 | Training Accuracy: 0.09527445652173913\n",
      "Epoch 1, Batch: 24000| Training Loss: 1.164851968700687 | Training Accuracy: 0.09588736979166666\n",
      "Epoch 1, Batch: 25000| Training Loss: 1.1498702676606178 | Training Accuracy: 0.09658\n",
      "Epoch 1, Batch: 26000| Training Loss: 1.135807240236264 | Training Accuracy: 0.09717307692307692\n",
      "Epoch 1, Batch: 27000| Training Loss: 1.1224758436172098 | Training Accuracy: 0.09772280092592593\n",
      "Epoch 1, Batch: 28000| Training Loss: 1.1100279576969998 | Training Accuracy: 0.09823772321428571\n",
      "Epoch 1, Batch: 29000| Training Loss: 1.0982175959410339 | Training Accuracy: 0.09884213362068965\n",
      "Epoch 1, Batch: 30000| Training Loss: 1.0867453599393369 | Training Accuracy: 0.0993921875\n",
      "Epoch 1, Batch: 31000| Training Loss: 1.0762404553582592 | Training Accuracy: 0.0999758064516129\n",
      "Epoch 1, Batch: 32000| Training Loss: 1.0661412358041853 | Training Accuracy: 0.1004453125\n",
      "Epoch 1, Training Loss: 1.061050172405421, Validation Error: 85.85875061164573, Validation Top-3 Accuracy: 0.0, Training Error: 89.92611133263833\n",
      "Epoch 2, Batch: 1000| Training Loss: 0.6711575121879577 | Training Accuracy: 0.125109375\n",
      "Epoch 2, Batch: 2000| Training Loss: 0.6644032163918019 | Training Accuracy: 0.1235625\n",
      "Epoch 2, Batch: 3000| Training Loss: 0.6603363276521365 | Training Accuracy: 0.1233125\n",
      "Epoch 2, Batch: 4000| Training Loss: 0.6570277464985848 | Training Accuracy: 0.123578125\n",
      "Epoch 2, Batch: 5000| Training Loss: 0.6538746861815452 | Training Accuracy: 0.124078125\n",
      "Epoch 2, Batch: 6000| Training Loss: 0.651663731748859 | Training Accuracy: 0.1243203125\n",
      "Epoch 2, Batch: 7000| Training Loss: 0.6494722363267627 | Training Accuracy: 0.12511160714285716\n",
      "Epoch 2, Batch: 8000| Training Loss: 0.6483781324438751 | Training Accuracy: 0.12532421875\n",
      "Epoch 2, Batch: 9000| Training Loss: 0.646468456092808 | Training Accuracy: 0.12556423611111112\n",
      "Epoch 2, Batch: 10000| Training Loss: 0.6450688520371914 | Training Accuracy: 0.1259125\n",
      "Epoch 2, Batch: 11000| Training Loss: 0.6436825778728181 | Training Accuracy: 0.12587073863636364\n",
      "Epoch 2, Batch: 12000| Training Loss: 0.6423990042085449 | Training Accuracy: 0.12604036458333334\n",
      "Epoch 2, Batch: 13000| Training Loss: 0.6410449204949232 | Training Accuracy: 0.12621274038461539\n",
      "Epoch 2, Batch: 14000| Training Loss: 0.6398537410880838 | Training Accuracy: 0.12640290178571428\n",
      "Epoch 2, Batch: 15000| Training Loss: 0.6385808268547059 | Training Accuracy: 0.12664270833333333\n",
      "Epoch 2, Batch: 16000| Training Loss: 0.6372981672883034 | Training Accuracy: 0.12692578125\n",
      "Epoch 2, Batch: 17000| Training Loss: 0.6360604495038005 | Training Accuracy: 0.12711856617647058\n",
      "Epoch 2, Batch: 18000| Training Loss: 0.6348251039667262 | Training Accuracy: 0.1273263888888889\n",
      "Epoch 2, Batch: 19000| Training Loss: 0.6338025029944746 | Training Accuracy: 0.12739226973684212\n",
      "Epoch 2, Batch: 20000| Training Loss: 0.6326680403605104 | Training Accuracy: 0.12759765625\n",
      "Epoch 2, Batch: 21000| Training Loss: 0.6316152188096728 | Training Accuracy: 0.12777232142857142\n",
      "Epoch 2, Batch: 22000| Training Loss: 0.630477886107835 | Training Accuracy: 0.12803480113636365\n",
      "Epoch 2, Batch: 23000| Training Loss: 0.629568841472916 | Training Accuracy: 0.12821671195652173\n",
      "Epoch 2, Batch: 24000| Training Loss: 0.6284723163594802 | Training Accuracy: 0.128474609375\n",
      "Epoch 2, Batch: 25000| Training Loss: 0.6275226095092297 | Training Accuracy: 0.12852875\n",
      "Epoch 2, Batch: 26000| Training Loss: 0.6266797275600525 | Training Accuracy: 0.12857211538461538\n",
      "Epoch 2, Batch: 27000| Training Loss: 0.6256377971492432 | Training Accuracy: 0.12874768518518517\n",
      "Epoch 2, Batch: 28000| Training Loss: 0.6247817378225071 | Training Accuracy: 0.12896205357142856\n",
      "Epoch 2, Batch: 29000| Training Loss: 0.6239902548913298 | Training Accuracy: 0.12909213362068966\n",
      "Epoch 2, Batch: 30000| Training Loss: 0.6230426460792621 | Training Accuracy: 0.1292515625\n",
      "Epoch 2, Batch: 31000| Training Loss: 0.6220799236528335 | Training Accuracy: 0.12939818548387097\n",
      "Epoch 2, Batch: 32000| Training Loss: 0.6211614200938493 | Training Accuracy: 0.129517578125\n",
      "Epoch 2, Training Loss: 0.6207129528947369, Validation Error: 84.4793438497565, Validation Top-3 Accuracy: 0.0, Training Error: 87.03671853606276\n",
      "Epoch 3, Batch: 1000| Training Loss: 0.5521195216178894 | Training Accuracy: 0.139609375\n",
      "Epoch 3, Batch: 2000| Training Loss: 0.5478138211071492 | Training Accuracy: 0.139859375\n",
      "Epoch 3, Batch: 3000| Training Loss: 0.5459079184631507 | Training Accuracy: 0.140203125\n",
      "Epoch 3, Batch: 4000| Training Loss: 0.5440611904636026 | Training Accuracy: 0.14113671875\n",
      "Epoch 3, Batch: 5000| Training Loss: 0.5430519543707371 | Training Accuracy: 0.141515625\n",
      "Epoch 3, Batch: 6000| Training Loss: 0.5422872601995865 | Training Accuracy: 0.1415\n",
      "Epoch 3, Batch: 7000| Training Loss: 0.5410353049210139 | Training Accuracy: 0.14142410714285714\n",
      "Epoch 3, Batch: 8000| Training Loss: 0.5398365555964411 | Training Accuracy: 0.14148828125\n",
      "Epoch 3, Batch: 9000| Training Loss: 0.5390784113142225 | Training Accuracy: 0.14157465277777778\n",
      "Epoch 3, Batch: 10000| Training Loss: 0.5384658810526133 | Training Accuracy: 0.1419375\n",
      "Epoch 3, Batch: 11000| Training Loss: 0.5378261609294198 | Training Accuracy: 0.14206818181818182\n",
      "Epoch 3, Batch: 12000| Training Loss: 0.5372282589251797 | Training Accuracy: 0.14202864583333333\n",
      "Epoch 3, Batch: 13000| Training Loss: 0.5365548460506476 | Training Accuracy: 0.1419639423076923\n",
      "Epoch 3, Batch: 14000| Training Loss: 0.5359789961172001 | Training Accuracy: 0.14210714285714285\n",
      "Epoch 3, Batch: 15000| Training Loss: 0.535406116215388 | Training Accuracy: 0.1420625\n",
      "Epoch 3, Batch: 16000| Training Loss: 0.5348981884922832 | Training Accuracy: 0.1421728515625\n",
      "Epoch 3, Batch: 17000| Training Loss: 0.53452745602762 | Training Accuracy: 0.1422702205882353\n",
      "Epoch 3, Batch: 18000| Training Loss: 0.5339745546198553 | Training Accuracy: 0.1422152777777778\n",
      "Epoch 3, Batch: 19000| Training Loss: 0.5334619117583099 | Training Accuracy: 0.14241611842105265\n",
      "Epoch 3, Batch: 20000| Training Loss: 0.5330731067016721 | Training Accuracy: 0.14258984375\n",
      "Epoch 3, Batch: 21000| Training Loss: 0.532583053147509 | Training Accuracy: 0.14265997023809524\n",
      "Epoch 3, Batch: 22000| Training Loss: 0.5320687713826244 | Training Accuracy: 0.14275852272727274\n",
      "Epoch 3, Batch: 23000| Training Loss: 0.5316264822586723 | Training Accuracy: 0.1428695652173913\n",
      "Epoch 3, Batch: 24000| Training Loss: 0.5311584800941249 | Training Accuracy: 0.14293359375\n",
      "Epoch 3, Batch: 25000| Training Loss: 0.5308024905908107 | Training Accuracy: 0.142983125\n",
      "Epoch 3, Batch: 26000| Training Loss: 0.5303762592627452 | Training Accuracy: 0.14296454326923078\n",
      "Epoch 3, Batch: 27000| Training Loss: 0.5299710534270163 | Training Accuracy: 0.1429369212962963\n",
      "Epoch 3, Batch: 28000| Training Loss: 0.5295507034723248 | Training Accuracy: 0.14294866071428572\n",
      "Epoch 3, Batch: 29000| Training Loss: 0.5291636438102558 | Training Accuracy: 0.14306788793103448\n",
      "Epoch 3, Batch: 30000| Training Loss: 0.5287673182676236 | Training Accuracy: 0.143240625\n",
      "Epoch 3, Batch: 31000| Training Loss: 0.5283836900582236 | Training Accuracy: 0.14333568548387096\n",
      "Epoch 3, Batch: 32000| Training Loss: 0.5280057687032967 | Training Accuracy: 0.1434140625\n",
      "Epoch 3, Training Loss: 0.5278094835600471, Validation Error: 82.87158934687886, Validation Top-3 Accuracy: 0.0, Training Error: 85.6540203986529\n",
      "Epoch 4, Batch: 1000| Training Loss: 0.49459830272197725 | Training Accuracy: 0.1494375\n",
      "Epoch 4, Batch: 2000| Training Loss: 0.49147701229155066 | Training Accuracy: 0.151515625\n",
      "Epoch 4, Batch: 3000| Training Loss: 0.48944265352686245 | Training Accuracy: 0.15161979166666667\n",
      "Epoch 4, Batch: 4000| Training Loss: 0.48830185513198376 | Training Accuracy: 0.15156640625\n",
      "Epoch 4, Batch: 5000| Training Loss: 0.4868272895336151 | Training Accuracy: 0.151409375\n",
      "Epoch 4, Batch: 6000| Training Loss: 0.4862123900602261 | Training Accuracy: 0.15089322916666667\n",
      "Epoch 4, Batch: 7000| Training Loss: 0.48586709325228417 | Training Accuracy: 0.15121428571428572\n",
      "Epoch 4, Batch: 8000| Training Loss: 0.4852582731060684 | Training Accuracy: 0.1514296875\n",
      "Epoch 4, Batch: 9000| Training Loss: 0.4844727109571298 | Training Accuracy: 0.15150520833333334\n",
      "Epoch 4, Batch: 10000| Training Loss: 0.4840474545687437 | Training Accuracy: 0.15164375\n",
      "Epoch 4, Batch: 11000| Training Loss: 0.4836522693363103 | Training Accuracy: 0.15158522727272727\n",
      "Epoch 4, Batch: 12000| Training Loss: 0.4833195228825013 | Training Accuracy: 0.15162239583333334\n",
      "Epoch 4, Batch: 13000| Training Loss: 0.48292500425302065 | Training Accuracy: 0.1517235576923077\n",
      "Epoch 4, Batch: 14000| Training Loss: 0.4824795344918966 | Training Accuracy: 0.15173325892857142\n",
      "Epoch 4, Batch: 15000| Training Loss: 0.48225703591505686 | Training Accuracy: 0.151625\n",
      "Epoch 4, Batch: 16000| Training Loss: 0.48201838686317205 | Training Accuracy: 0.1517119140625\n",
      "Epoch 4, Batch: 17000| Training Loss: 0.48176296530926926 | Training Accuracy: 0.15174816176470587\n",
      "Epoch 4, Batch: 18000| Training Loss: 0.4814439157065418 | Training Accuracy: 0.15186545138888888\n",
      "Epoch 4, Batch: 19000| Training Loss: 0.4812255045301036 | Training Accuracy: 0.15194654605263158\n",
      "Epoch 4, Batch: 20000| Training Loss: 0.4810264293447137 | Training Accuracy: 0.151946875\n",
      "Epoch 4, Batch: 21000| Training Loss: 0.4807619699026857 | Training Accuracy: 0.1519940476190476\n",
      "Epoch 4, Batch: 22000| Training Loss: 0.4804759004983035 | Training Accuracy: 0.152109375\n",
      "Epoch 4, Batch: 23000| Training Loss: 0.48020480858113457 | Training Accuracy: 0.1521297554347826\n",
      "Epoch 4, Batch: 24000| Training Loss: 0.47996317293991647 | Training Accuracy: 0.15227669270833333\n",
      "Epoch 4, Batch: 25000| Training Loss: 0.4797390858626366 | Training Accuracy: 0.152305\n",
      "Epoch 4, Batch: 26000| Training Loss: 0.47946288880247334 | Training Accuracy: 0.15241225961538463\n",
      "Epoch 4, Batch: 27000| Training Loss: 0.47924024129465775 | Training Accuracy: 0.15252025462962962\n",
      "Epoch 4, Batch: 28000| Training Loss: 0.47903610607875247 | Training Accuracy: 0.15261774553571428\n",
      "Epoch 4, Batch: 29000| Training Loss: 0.47876586949619754 | Training Accuracy: 0.1526875\n",
      "Epoch 4, Batch: 30000| Training Loss: 0.47850796821415426 | Training Accuracy: 0.15274739583333333\n",
      "Epoch 4, Batch: 31000| Training Loss: 0.47824333665640123 | Training Accuracy: 0.15274899193548386\n",
      "Epoch 4, Batch: 32000| Training Loss: 0.4779832048462704 | Training Accuracy: 0.152833984375\n",
      "Epoch 4, Training Loss: 0.4778744530520555, Validation Error: 81.75781158981289, Validation Top-3 Accuracy: 0.0, Training Error: 84.71052264942277\n",
      "Epoch 5, Batch: 1000| Training Loss: 0.45869899761676786 | Training Accuracy: 0.155953125\n",
      "Epoch 5, Batch: 2000| Training Loss: 0.45565291939675806 | Training Accuracy: 0.1558828125\n",
      "Epoch 5, Batch: 3000| Training Loss: 0.45453603101770085 | Training Accuracy: 0.15690625\n",
      "Epoch 5, Batch: 4000| Training Loss: 0.4541607052385807 | Training Accuracy: 0.156703125\n",
      "Epoch 5, Batch: 5000| Training Loss: 0.4536404236972332 | Training Accuracy: 0.156953125\n",
      "Epoch 5, Batch: 6000| Training Loss: 0.4532230011870464 | Training Accuracy: 0.15716927083333335\n",
      "Epoch 5, Batch: 7000| Training Loss: 0.45265644044109754 | Training Accuracy: 0.15739508928571427\n",
      "Epoch 5, Batch: 8000| Training Loss: 0.45195246053114535 | Training Accuracy: 0.157560546875\n",
      "Epoch 5, Batch: 9000| Training Loss: 0.45137569953335654 | Training Accuracy: 0.15770833333333334\n",
      "Epoch 5, Batch: 10000| Training Loss: 0.45131273227632046 | Training Accuracy: 0.1577203125\n",
      "Epoch 5, Batch: 11000| Training Loss: 0.45092997777462007 | Training Accuracy: 0.1578096590909091\n",
      "Epoch 5, Batch: 12000| Training Loss: 0.4506516810854276 | Training Accuracy: 0.15791536458333333\n",
      "Epoch 5, Batch: 13000| Training Loss: 0.4505171445012093 | Training Accuracy: 0.15803485576923076\n",
      "Epoch 5, Batch: 14000| Training Loss: 0.45019282679685524 | Training Accuracy: 0.15822879464285713\n",
      "Epoch 5, Batch: 15000| Training Loss: 0.4500500827928384 | Training Accuracy: 0.15842291666666666\n",
      "Epoch 5, Batch: 16000| Training Loss: 0.4497157782446593 | Training Accuracy: 0.15866015625\n",
      "Epoch 5, Batch: 17000| Training Loss: 0.44965102347380975 | Training Accuracy: 0.15871323529411765\n",
      "Epoch 5, Batch: 18000| Training Loss: 0.44923505255745516 | Training Accuracy: 0.15880208333333334\n",
      "Epoch 5, Batch: 19000| Training Loss: 0.4489847584915789 | Training Accuracy: 0.15892845394736843\n",
      "Epoch 5, Batch: 20000| Training Loss: 0.44883309031277896 | Training Accuracy: 0.158928125\n",
      "Epoch 5, Batch: 21000| Training Loss: 0.44869594980847266 | Training Accuracy: 0.15888020833333333\n",
      "Epoch 5, Batch: 22000| Training Loss: 0.4484122320210392 | Training Accuracy: 0.158953125\n",
      "Epoch 5, Batch: 23000| Training Loss: 0.44828841744557674 | Training Accuracy: 0.1589891304347826\n",
      "Epoch 5, Batch: 24000| Training Loss: 0.44811088067417343 | Training Accuracy: 0.15902994791666666\n",
      "Epoch 5, Batch: 25000| Training Loss: 0.4479936969590187 | Training Accuracy: 0.159086875\n",
      "Epoch 5, Batch: 26000| Training Loss: 0.447838217157584 | Training Accuracy: 0.15920673076923078\n",
      "Epoch 5, Batch: 27000| Training Loss: 0.4477657610818192 | Training Accuracy: 0.15926967592592592\n",
      "Epoch 5, Batch: 28000| Training Loss: 0.44759894141554835 | Training Accuracy: 0.15923939732142858\n",
      "Epoch 5, Batch: 29000| Training Loss: 0.4474891034621617 | Training Accuracy: 0.15914924568965516\n",
      "Epoch 5, Batch: 30000| Training Loss: 0.4473615257769823 | Training Accuracy: 0.15921041666666666\n",
      "Epoch 5, Batch: 31000| Training Loss: 0.44717546733156327 | Training Accuracy: 0.15932157258064517\n",
      "Epoch 5, Batch: 32000| Training Loss: 0.44705896330997347 | Training Accuracy: 0.1593916015625\n",
      "Epoch 5, Training Loss: 0.4469470358498934, Validation Error: 81.18694223734184, Validation Top-3 Accuracy: 0.0, Training Error: 84.05311528649874\n",
      "Epoch 6, Batch: 1000| Training Loss: 0.4352319273352623 | Training Accuracy: 0.16065625\n",
      "Epoch 6, Batch: 2000| Training Loss: 0.4333916566669941 | Training Accuracy: 0.1614453125\n",
      "Epoch 6, Batch: 3000| Training Loss: 0.43248614061872165 | Training Accuracy: 0.16247916666666667\n",
      "Epoch 6, Batch: 4000| Training Loss: 0.43162360978126524 | Training Accuracy: 0.16348828125\n",
      "Epoch 6, Batch: 5000| Training Loss: 0.4309717469573021 | Training Accuracy: 0.16395\n",
      "Epoch 6, Batch: 6000| Training Loss: 0.43071966718137267 | Training Accuracy: 0.1637265625\n",
      "Epoch 6, Batch: 7000| Training Loss: 0.4303593663743564 | Training Accuracy: 0.1639486607142857\n",
      "Epoch 6, Batch: 8000| Training Loss: 0.43017050956562164 | Training Accuracy: 0.16375390625\n",
      "Epoch 6, Batch: 9000| Training Loss: 0.4301757184300158 | Training Accuracy: 0.1637829861111111\n",
      "Epoch 6, Batch: 10000| Training Loss: 0.42997571325302125 | Training Accuracy: 0.1638078125\n",
      "Epoch 6, Batch: 11000| Training Loss: 0.4296303317926147 | Training Accuracy: 0.16371590909090908\n",
      "Epoch 6, Batch: 12000| Training Loss: 0.4295967272147536 | Training Accuracy: 0.16374088541666668\n",
      "Epoch 6, Batch: 13000| Training Loss: 0.42926159909826056 | Training Accuracy: 0.1638545673076923\n",
      "Epoch 6, Batch: 14000| Training Loss: 0.42915410078423366 | Training Accuracy: 0.1638794642857143\n",
      "Epoch 6, Batch: 15000| Training Loss: 0.42893869659701983 | Training Accuracy: 0.164009375\n",
      "Epoch 6, Batch: 16000| Training Loss: 0.4287779968138784 | Training Accuracy: 0.1640712890625\n",
      "Epoch 6, Batch: 17000| Training Loss: 0.428699518759461 | Training Accuracy: 0.16413786764705882\n",
      "Epoch 6, Batch: 18000| Training Loss: 0.42863897065818307 | Training Accuracy: 0.16398871527777778\n",
      "Epoch 6, Batch: 19000| Training Loss: 0.42844890563111554 | Training Accuracy: 0.16414473684210526\n",
      "Epoch 6, Batch: 20000| Training Loss: 0.42829816463440656 | Training Accuracy: 0.1641328125\n",
      "Epoch 6, Batch: 21000| Training Loss: 0.42817502906918525 | Training Accuracy: 0.16411160714285714\n",
      "Epoch 6, Batch: 22000| Training Loss: 0.42803868731043554 | Training Accuracy: 0.16420951704545456\n",
      "Epoch 6, Batch: 23000| Training Loss: 0.42792287849084193 | Training Accuracy: 0.16425\n",
      "Epoch 6, Batch: 24000| Training Loss: 0.4278801880280177 | Training Accuracy: 0.16427864583333332\n",
      "Epoch 6, Batch: 25000| Training Loss: 0.42779499895095824 | Training Accuracy: 0.16423125\n",
      "Epoch 6, Batch: 26000| Training Loss: 0.4277355244457722 | Training Accuracy: 0.16428725961538462\n",
      "Epoch 6, Batch: 27000| Training Loss: 0.42762912454097357 | Training Accuracy: 0.1642795138888889\n",
      "Epoch 6, Batch: 28000| Training Loss: 0.4275450540472354 | Training Accuracy: 0.1643286830357143\n",
      "Epoch 6, Batch: 29000| Training Loss: 0.42739319565686684 | Training Accuracy: 0.16443696120689655\n",
      "Epoch 6, Batch: 30000| Training Loss: 0.4272943704942862 | Training Accuracy: 0.16449375\n",
      "Epoch 6, Batch: 31000| Training Loss: 0.42724125705322913 | Training Accuracy: 0.16446723790322582\n",
      "Epoch 6, Batch: 32000| Training Loss: 0.42714149523712697 | Training Accuracy: 0.16449853515625\n",
      "Epoch 6, Training Loss: 0.427062805498743, Validation Error: 81.03781718200247, Validation Top-3 Accuracy: 0.0, Training Error: 83.54833750498436\n",
      "Epoch 7, Batch: 1000| Training Loss: 0.41971308320760725 | Training Accuracy: 0.164484375\n",
      "Epoch 7, Batch: 2000| Training Loss: 0.41806499084830284 | Training Accuracy: 0.166953125\n",
      "Epoch 7, Batch: 3000| Training Loss: 0.4177352369725704 | Training Accuracy: 0.16677083333333334\n",
      "Epoch 7, Batch: 4000| Training Loss: 0.41728776232898235 | Training Accuracy: 0.166671875\n",
      "Epoch 7, Batch: 5000| Training Loss: 0.4169720638334751 | Training Accuracy: 0.166934375\n",
      "Epoch 7, Batch: 6000| Training Loss: 0.416523935392499 | Training Accuracy: 0.16698697916666666\n",
      "Epoch 7, Batch: 7000| Training Loss: 0.4164100900292397 | Training Accuracy: 0.16696651785714287\n",
      "Epoch 7, Batch: 8000| Training Loss: 0.4163022871948779 | Training Accuracy: 0.16703125\n",
      "Epoch 7, Batch: 9000| Training Loss: 0.41612518872817356 | Training Accuracy: 0.16729166666666667\n",
      "Epoch 7, Batch: 10000| Training Loss: 0.4158881556421518 | Training Accuracy: 0.1674359375\n",
      "Epoch 7, Batch: 11000| Training Loss: 0.41592909452048216 | Training Accuracy: 0.16732670454545454\n",
      "Epoch 7, Batch: 12000| Training Loss: 0.41586152750998734 | Training Accuracy: 0.16730598958333334\n",
      "Epoch 7, Batch: 13000| Training Loss: 0.41582583477176155 | Training Accuracy: 0.1672920673076923\n",
      "Epoch 7, Batch: 14000| Training Loss: 0.41570627106939045 | Training Accuracy: 0.1672265625\n",
      "Epoch 7, Batch: 15000| Training Loss: 0.4156182323118051 | Training Accuracy: 0.16721041666666667\n",
      "Epoch 7, Batch: 16000| Training Loss: 0.41551424852199853 | Training Accuracy: 0.1671904296875\n",
      "Epoch 7, Batch: 17000| Training Loss: 0.41546441737869205 | Training Accuracy: 0.16725275735294118\n",
      "Epoch 7, Batch: 18000| Training Loss: 0.4153466069549322 | Training Accuracy: 0.16729947916666665\n",
      "Epoch 7, Batch: 19000| Training Loss: 0.415213895888705 | Training Accuracy: 0.16744654605263157\n",
      "Epoch 7, Batch: 20000| Training Loss: 0.41508777083158493 | Training Accuracy: 0.1675515625\n",
      "Epoch 7, Batch: 21000| Training Loss: 0.41497914593702273 | Training Accuracy: 0.16761681547619048\n",
      "Epoch 7, Batch: 22000| Training Loss: 0.4148331160856919 | Training Accuracy: 0.16756107954545454\n",
      "Epoch 7, Batch: 23000| Training Loss: 0.4147950167344964 | Training Accuracy: 0.16765625\n",
      "Epoch 7, Batch: 24000| Training Loss: 0.4147258864591519 | Training Accuracy: 0.1676875\n",
      "Epoch 7, Batch: 25000| Training Loss: 0.41467091508746146 | Training Accuracy: 0.167739375\n",
      "Epoch 7, Batch: 26000| Training Loss: 0.41448787352328115 | Training Accuracy: 0.16779146634615386\n",
      "Epoch 7, Batch: 27000| Training Loss: 0.4144253096072762 | Training Accuracy: 0.16777025462962963\n",
      "Epoch 7, Batch: 28000| Training Loss: 0.41438376961967777 | Training Accuracy: 0.16775390625\n",
      "Epoch 7, Batch: 29000| Training Loss: 0.41431359226641984 | Training Accuracy: 0.16776831896551725\n",
      "Epoch 7, Batch: 30000| Training Loss: 0.4141986496557792 | Training Accuracy: 0.16780625\n",
      "Epoch 7, Batch: 31000| Training Loss: 0.4140977683278822 | Training Accuracy: 0.16781401209677418\n",
      "Epoch 7, Batch: 32000| Training Loss: 0.41403592057246713 | Training Accuracy: 0.16777978515625\n",
      "Epoch 7, Training Loss: 0.413978304061088, Validation Error: 80.65568422769532, Validation Top-3 Accuracy: 0.0, Training Error: 83.21550220753204\n",
      "Epoch 8, Batch: 1000| Training Loss: 0.4083538945913315 | Training Accuracy: 0.1678125\n",
      "Epoch 8, Batch: 2000| Training Loss: 0.40757601109147074 | Training Accuracy: 0.170390625\n",
      "Epoch 8, Batch: 3000| Training Loss: 0.40687451724211376 | Training Accuracy: 0.170109375\n",
      "Epoch 8, Batch: 4000| Training Loss: 0.4068443677797914 | Training Accuracy: 0.17003515625\n",
      "Epoch 8, Batch: 5000| Training Loss: 0.4068409416913986 | Training Accuracy: 0.170421875\n",
      "Epoch 8, Batch: 6000| Training Loss: 0.40687988079090914 | Training Accuracy: 0.170453125\n",
      "Epoch 8, Batch: 7000| Training Loss: 0.4067479651357446 | Training Accuracy: 0.17059821428571428\n",
      "Epoch 8, Batch: 8000| Training Loss: 0.40678355825319884 | Training Accuracy: 0.17045703125\n",
      "Epoch 8, Batch: 9000| Training Loss: 0.4067523973782857 | Training Accuracy: 0.17021354166666666\n",
      "Epoch 8, Batch: 10000| Training Loss: 0.4067803908973932 | Training Accuracy: 0.1702421875\n",
      "Epoch 8, Batch: 11000| Training Loss: 0.40670261909474026 | Training Accuracy: 0.17033522727272726\n",
      "Epoch 8, Batch: 12000| Training Loss: 0.40677089051157234 | Training Accuracy: 0.17027994791666667\n",
      "Epoch 8, Batch: 13000| Training Loss: 0.4067146525360071 | Training Accuracy: 0.17042908653846153\n",
      "Epoch 8, Batch: 14000| Training Loss: 0.40670139783620834 | Training Accuracy: 0.17046763392857142\n",
      "Epoch 8, Batch: 15000| Training Loss: 0.4066473125477632 | Training Accuracy: 0.17040208333333334\n",
      "Epoch 8, Batch: 16000| Training Loss: 0.40668028490431607 | Training Accuracy: 0.1703994140625\n",
      "Epoch 8, Batch: 17000| Training Loss: 0.40664030805756063 | Training Accuracy: 0.170359375\n",
      "Epoch 8, Batch: 18000| Training Loss: 0.40651118942267367 | Training Accuracy: 0.17035763888888888\n",
      "Epoch 8, Batch: 19000| Training Loss: 0.40638401185681944 | Training Accuracy: 0.17037171052631578\n",
      "Epoch 8, Batch: 20000| Training Loss: 0.4063801406905055 | Training Accuracy: 0.17038515625\n",
      "Epoch 8, Batch: 21000| Training Loss: 0.40632851780596235 | Training Accuracy: 0.1702797619047619\n",
      "Epoch 8, Batch: 22000| Training Loss: 0.4062115273285996 | Training Accuracy: 0.17032457386363636\n",
      "Epoch 8, Batch: 23000| Training Loss: 0.40611448290166646 | Training Accuracy: 0.17041779891304348\n",
      "Epoch 8, Batch: 24000| Training Loss: 0.4060638549414774 | Training Accuracy: 0.170306640625\n",
      "Epoch 8, Batch: 25000| Training Loss: 0.40598198451757433 | Training Accuracy: 0.1703375\n",
      "Epoch 8, Batch: 26000| Training Loss: 0.40593504187235463 | Training Accuracy: 0.1703425480769231\n",
      "Epoch 8, Batch: 27000| Training Loss: 0.4058843576102345 | Training Accuracy: 0.17032233796296295\n",
      "Epoch 8, Batch: 28000| Training Loss: 0.4059040178688509 | Training Accuracy: 0.1703794642857143\n",
      "Epoch 8, Batch: 29000| Training Loss: 0.4058912663274798 | Training Accuracy: 0.17024407327586208\n",
      "Epoch 8, Batch: 30000| Training Loss: 0.40584704039990904 | Training Accuracy: 0.17026041666666666\n",
      "Epoch 8, Batch: 31000| Training Loss: 0.4057465207807479 | Training Accuracy: 0.17031804435483872\n",
      "Epoch 8, Batch: 32000| Training Loss: 0.405686811638996 | Training Accuracy: 0.170361328125\n",
      "Epoch 8, Training Loss: 0.405643593790698, Validation Error: 80.3015122212643, Validation Top-3 Accuracy: 0.0, Training Error: 82.96203237073087\n",
      "Epoch 9, Batch: 1000| Training Loss: 0.40359382882714273 | Training Accuracy: 0.170859375\n",
      "Epoch 9, Batch: 2000| Training Loss: 0.40209779070317747 | Training Accuracy: 0.17165625\n",
      "Epoch 9, Batch: 3000| Training Loss: 0.40176233504215875 | Training Accuracy: 0.17205208333333333\n",
      "Epoch 9, Batch: 4000| Training Loss: 0.4011815617009997 | Training Accuracy: 0.17225390625\n",
      "Epoch 9, Batch: 5000| Training Loss: 0.4013022011935711 | Training Accuracy: 0.172146875\n",
      "Epoch 9, Batch: 6000| Training Loss: 0.4013112168113391 | Training Accuracy: 0.17229166666666668\n",
      "Epoch 9, Batch: 7000| Training Loss: 0.4012257807127067 | Training Accuracy: 0.172140625\n",
      "Epoch 9, Batch: 8000| Training Loss: 0.4011548073776066 | Training Accuracy: 0.171728515625\n",
      "Epoch 9, Batch: 9000| Training Loss: 0.4011797513928678 | Training Accuracy: 0.17174131944444446\n",
      "Epoch 9, Batch: 10000| Training Loss: 0.4011004794239998 | Training Accuracy: 0.17150625\n",
      "Epoch 9, Batch: 11000| Training Loss: 0.40107176263765854 | Training Accuracy: 0.17124147727272726\n",
      "Epoch 9, Batch: 12000| Training Loss: 0.4009432619139552 | Training Accuracy: 0.17124739583333334\n",
      "Epoch 9, Batch: 13000| Training Loss: 0.40092733496656785 | Training Accuracy: 0.17135697115384615\n",
      "Epoch 9, Batch: 14000| Training Loss: 0.40085285179104124 | Training Accuracy: 0.17142075892857142\n",
      "Epoch 9, Batch: 15000| Training Loss: 0.40073411579529444 | Training Accuracy: 0.1713375\n",
      "Epoch 9, Batch: 16000| Training Loss: 0.4006780085042119 | Training Accuracy: 0.1711943359375\n",
      "Epoch 9, Batch: 17000| Training Loss: 0.4006867782876772 | Training Accuracy: 0.17124632352941177\n",
      "Epoch 9, Batch: 18000| Training Loss: 0.40055856409006646 | Training Accuracy: 0.17150347222222223\n",
      "Epoch 9, Batch: 19000| Training Loss: 0.4005081440806389 | Training Accuracy: 0.17154276315789474\n",
      "Epoch 9, Batch: 20000| Training Loss: 0.4004583315983415 | Training Accuracy: 0.171709375\n",
      "Epoch 9, Batch: 21000| Training Loss: 0.4004434190974349 | Training Accuracy: 0.1716763392857143\n",
      "Epoch 9, Batch: 22000| Training Loss: 0.4003682247833772 | Training Accuracy: 0.17165838068181818\n",
      "Epoch 9, Batch: 23000| Training Loss: 0.4003439983440482 | Training Accuracy: 0.17168410326086955\n",
      "Epoch 9, Batch: 24000| Training Loss: 0.40036042044932646 | Training Accuracy: 0.17168684895833333\n",
      "Epoch 9, Batch: 25000| Training Loss: 0.4002904065859318 | Training Accuracy: 0.1717575\n",
      "Epoch 9, Batch: 26000| Training Loss: 0.40024022464912673 | Training Accuracy: 0.17178004807692307\n",
      "Epoch 9, Batch: 27000| Training Loss: 0.4001693528747117 | Training Accuracy: 0.17175462962962962\n",
      "Epoch 9, Batch: 28000| Training Loss: 0.40018720870358604 | Training Accuracy: 0.17176897321428572\n",
      "Epoch 9, Batch: 29000| Training Loss: 0.400160314121123 | Training Accuracy: 0.17185344827586208\n",
      "Epoch 9, Batch: 30000| Training Loss: 0.4001386026442051 | Training Accuracy: 0.17184947916666668\n",
      "Epoch 9, Batch: 31000| Training Loss: 0.4001117541251644 | Training Accuracy: 0.1718876008064516\n",
      "Epoch 9, Batch: 32000| Training Loss: 0.40006351806223395 | Training Accuracy: 0.17189794921875\n",
      "Epoch 9, Training Loss: 0.40005117684397484, Validation Error: 80.20364890369784, Validation Top-3 Accuracy: 0.0, Training Error: 82.80671243472287\n",
      "Epoch 10, Batch: 1000| Training Loss: 0.39807792943716047 | Training Accuracy: 0.1688125\n",
      "Epoch 10, Batch: 2000| Training Loss: 0.3979387579113245 | Training Accuracy: 0.1719453125\n",
      "Epoch 10, Batch: 3000| Training Loss: 0.39761403929193817 | Training Accuracy: 0.17306770833333332\n",
      "Epoch 10, Batch: 4000| Training Loss: 0.39776186338067054 | Training Accuracy: 0.17268359375\n",
      "Epoch 10, Batch: 5000| Training Loss: 0.39752704250216486 | Training Accuracy: 0.1727125\n",
      "Epoch 10, Batch: 6000| Training Loss: 0.3974840909093618 | Training Accuracy: 0.17301302083333334\n",
      "Epoch 10, Batch: 7000| Training Loss: 0.39747695621848106 | Training Accuracy: 0.17293303571428573\n",
      "Epoch 10, Batch: 8000| Training Loss: 0.3975292502641678 | Training Accuracy: 0.172818359375\n",
      "Epoch 10, Batch: 9000| Training Loss: 0.3973897691004806 | Training Accuracy: 0.17297222222222222\n",
      "Epoch 10, Batch: 10000| Training Loss: 0.39745566565692425 | Training Accuracy: 0.1728140625\n",
      "Epoch 10, Batch: 11000| Training Loss: 0.3973017782948234 | Training Accuracy: 0.17295738636363636\n",
      "Epoch 10, Batch: 12000| Training Loss: 0.3972150003736218 | Training Accuracy: 0.17300520833333333\n",
      "Epoch 10, Batch: 13000| Training Loss: 0.39715123212337494 | Training Accuracy: 0.1729951923076923\n",
      "Epoch 10, Batch: 14000| Training Loss: 0.3971847256485905 | Training Accuracy: 0.172875\n",
      "Epoch 10, Batch: 15000| Training Loss: 0.3972005658427874 | Training Accuracy: 0.17283333333333334\n",
      "Epoch 10, Batch: 16000| Training Loss: 0.39714635092765094 | Training Accuracy: 0.172931640625\n",
      "Epoch 10, Batch: 17000| Training Loss: 0.3970866591965451 | Training Accuracy: 0.1729485294117647\n",
      "Epoch 10, Batch: 18000| Training Loss: 0.3969750682049327 | Training Accuracy: 0.17283506944444443\n",
      "Epoch 10, Batch: 19000| Training Loss: 0.3969324726117285 | Training Accuracy: 0.17281661184210526\n",
      "Epoch 10, Batch: 20000| Training Loss: 0.3969782497242093 | Training Accuracy: 0.1727953125\n",
      "Epoch 10, Batch: 21000| Training Loss: 0.3969911198133514 | Training Accuracy: 0.17285193452380954\n",
      "Epoch 10, Batch: 22000| Training Loss: 0.39689180115407163 | Training Accuracy: 0.17295880681818182\n",
      "Epoch 10, Batch: 23000| Training Loss: 0.3968624649034894 | Training Accuracy: 0.17294904891304347\n",
      "Epoch 10, Batch: 24000| Training Loss: 0.3968027205032607 | Training Accuracy: 0.17290104166666667\n",
      "Epoch 10, Batch: 25000| Training Loss: 0.39678883843302726 | Training Accuracy: 0.172915625\n",
      "Epoch 10, Batch: 26000| Training Loss: 0.3967807819591119 | Training Accuracy: 0.17291165865384617\n",
      "Epoch 10, Batch: 27000| Training Loss: 0.396756119126523 | Training Accuracy: 0.172890625\n",
      "Epoch 10, Batch: 28000| Training Loss: 0.39676333318650725 | Training Accuracy: 0.17297935267857142\n",
      "Epoch 10, Batch: 29000| Training Loss: 0.39674995122798556 | Training Accuracy: 0.1729520474137931\n",
      "Epoch 10, Batch: 30000| Training Loss: 0.3967455799271663 | Training Accuracy: 0.172896875\n",
      "Epoch 10, Batch: 31000| Training Loss: 0.39678996218212187 | Training Accuracy: 0.17282308467741936\n",
      "Epoch 10, Batch: 32000| Training Loss: 0.3967792380815372 | Training Accuracy: 0.17283349609375\n",
      "Epoch 10, Training Loss: 0.3967705671105668, Validation Error: 80.04986369037911, Validation Top-3 Accuracy: 0.0, Training Error: 82.71034008964645\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_decoder(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('transformer-decoder-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_images/seq-transformer-exp-1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 (different training schema -- only predict last token of sequence and backprop on that alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207314\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 128\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "model = ChessTransformerTwo(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.65 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m train_last_token(\u001b[43mdevice\u001b[49m, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_last_token(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('transformer-decoder-last-token-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 (higher d_model & num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16970002\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=3,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 256\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 2e-4\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.8 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 3.1954081513881682 | Training Accuracy: 0.0639765625\n",
      "Epoch 1, Batch: 2000| Training Loss: 1.9873469138741493 | Training Accuracy: 0.075162109375\n",
      "Epoch 1, Batch: 3000| Training Loss: 1.492823544184367 | Training Accuracy: 0.08463932291666666\n",
      "Epoch 1, Batch: 4000| Training Loss: 1.2290759043172002 | Training Accuracy: 0.0930126953125\n",
      "Epoch 1, Batch: 5000| Training Loss: 1.065340001809597 | Training Accuracy: 0.100221875\n",
      "Epoch 1, Batch: 6000| Training Loss: 0.9532748395303885 | Training Accuracy: 0.10650065104166667\n",
      "Epoch 1, Batch: 7000| Training Loss: 0.871701592539038 | Training Accuracy: 0.11183537946428572\n",
      "Epoch 1, Batch: 8000| Training Loss: 0.809373230125755 | Training Accuracy: 0.1165244140625\n",
      "Epoch 1, Batch: 9000| Training Loss: 0.7600478604402807 | Training Accuracy: 0.1206796875\n",
      "Epoch 1, Batch: 10000| Training Loss: 0.720000497496128 | Training Accuracy: 0.124434765625\n",
      "Epoch 1, Batch: 11000| Training Loss: 0.686838518535549 | Training Accuracy: 0.12776882102272727\n",
      "Epoch 1, Batch: 12000| Training Loss: 0.658870872537295 | Training Accuracy: 0.1307197265625\n",
      "Epoch 1, Batch: 13000| Training Loss: 0.6348483976675914 | Training Accuracy: 0.1335234375\n",
      "Epoch 1, Batch: 14000| Training Loss: 0.6140285094529391 | Training Accuracy: 0.136064453125\n",
      "Epoch 1, Batch: 15000| Training Loss: 0.5957370584050814 | Training Accuracy: 0.13864895833333332\n",
      "Epoch 1, Batch: 16000| Training Loss: 0.5795677316877991 | Training Accuracy: 0.140907958984375\n",
      "Epoch 1, Batch: 17000| Training Loss: 0.5651749873950201 | Training Accuracy: 0.14309604779411764\n",
      "Epoch 1, Batch: 18000| Training Loss: 0.5522074064148796 | Training Accuracy: 0.145162109375\n",
      "Epoch 1, Batch: 19000| Training Loss: 0.5405589927607461 | Training Accuracy: 0.14715439967105262\n",
      "Epoch 1, Batch: 20000| Training Loss: 0.5299370450109243 | Training Accuracy: 0.1489572265625\n",
      "Epoch 1, Batch: 21000| Training Loss: 0.5202157428321385 | Training Accuracy: 0.1506781994047619\n",
      "Epoch 1, Batch: 22000| Training Loss: 0.5113086978793144 | Training Accuracy: 0.15235227272727273\n",
      "Epoch 1, Batch: 23000| Training Loss: 0.5030384586816249 | Training Accuracy: 0.15404415760869566\n",
      "Epoch 1, Batch: 24000| Training Loss: 0.49540188705921173 | Training Accuracy: 0.15565169270833334\n",
      "Epoch 1, Batch: 25000| Training Loss: 0.4883785317122936 | Training Accuracy: 0.157105625\n",
      "Epoch 1, Batch: 26000| Training Loss: 0.48175791277908364 | Training Accuracy: 0.15855528846153846\n",
      "Epoch 1, Batch: 27000| Training Loss: 0.4755939991650758 | Training Accuracy: 0.15992057291666667\n",
      "Epoch 1, Batch: 28000| Training Loss: 0.46982592465302775 | Training Accuracy: 0.1612035435267857\n",
      "Epoch 1, Batch: 29000| Training Loss: 0.4643946778671495 | Training Accuracy: 0.16252491918103448\n",
      "Epoch 1, Batch: 30000| Training Loss: 0.45930290575722854 | Training Accuracy: 0.16373033854166666\n",
      "Epoch 1, Batch: 31000| Training Loss: 0.4545087062081983 | Training Accuracy: 0.1649444304435484\n",
      "Epoch 1, Batch: 32000| Training Loss: 0.44995840385276825 | Training Accuracy: 0.16611279296875\n",
      "Epoch 1, Batch: 33000| Training Loss: 0.4456817871097362 | Training Accuracy: 0.16721969696969696\n",
      "Epoch 1, Batch: 34000| Training Loss: 0.441591592850054 | Training Accuracy: 0.16827688419117648\n",
      "Epoch 1, Batch: 35000| Training Loss: 0.4377518483068262 | Training Accuracy: 0.16934040178571427\n",
      "Epoch 1, Batch: 36000| Training Loss: 0.4340638774699635 | Training Accuracy: 0.17036436631944443\n",
      "Epoch 1, Batch: 37000| Training Loss: 0.4305546542027512 | Training Accuracy: 0.17133266469594594\n",
      "Epoch 1, Batch: 38000| Training Loss: 0.4272144203766396 | Training Accuracy: 0.17230982730263159\n",
      "Epoch 1, Batch: 39000| Training Loss: 0.42399871058188954 | Training Accuracy: 0.17324639423076924\n",
      "Epoch 1, Batch: 40000| Training Loss: 0.42094070152044294 | Training Accuracy: 0.174139453125\n",
      "Epoch 1, Batch: 41000| Training Loss: 0.418000498347893 | Training Accuracy: 0.17503839557926829\n",
      "Epoch 1, Batch: 42000| Training Loss: 0.4151829435520229 | Training Accuracy: 0.17590401785714285\n",
      "Epoch 1, Batch: 43000| Training Loss: 0.41248568326650664 | Training Accuracy: 0.17671275436046513\n",
      "Epoch 1, Batch: 44000| Training Loss: 0.4098907330286774 | Training Accuracy: 0.17754847301136364\n",
      "Epoch 1, Batch: 45000| Training Loss: 0.40740214053856 | Training Accuracy: 0.17833177083333332\n",
      "Epoch 1, Batch: 46000| Training Loss: 0.40500489985618904 | Training Accuracy: 0.1790811820652174\n",
      "Epoch 1, Training Loss: 0.40393040721032136, Validation Error: 76.51897314601473, Validation Top-3 Accuracy: 0.0, Training Error: 82.05490237757382\n",
      "Epoch 2, Batch: 1000| Training Loss: 0.2921119864284992 | Training Accuracy: 0.21730078125\n",
      "Epoch 2, Batch: 2000| Training Loss: 0.29102611550688745 | Training Accuracy: 0.21887109375\n",
      "Epoch 2, Batch: 3000| Training Loss: 0.29083643878499665 | Training Accuracy: 0.21928125\n",
      "Epoch 2, Batch: 4000| Training Loss: 0.29039102866500616 | Training Accuracy: 0.2201328125\n",
      "Epoch 2, Batch: 5000| Training Loss: 0.29006133940517903 | Training Accuracy: 0.220309375\n",
      "Epoch 2, Batch: 6000| Training Loss: 0.28977868508547544 | Training Accuracy: 0.22023828125\n",
      "Epoch 2, Batch: 7000| Training Loss: 0.2894178409938301 | Training Accuracy: 0.2204575892857143\n",
      "Epoch 2, Batch: 8000| Training Loss: 0.2891884609628469 | Training Accuracy: 0.220552734375\n",
      "Epoch 2, Batch: 9000| Training Loss: 0.28891410314540067 | Training Accuracy: 0.22087760416666666\n",
      "Epoch 2, Batch: 10000| Training Loss: 0.28864848934262993 | Training Accuracy: 0.2210640625\n",
      "Epoch 2, Batch: 11000| Training Loss: 0.2883631592623212 | Training Accuracy: 0.2213387784090909\n",
      "Epoch 2, Batch: 12000| Training Loss: 0.2881083238559465 | Training Accuracy: 0.22164908854166668\n",
      "Epoch 2, Batch: 13000| Training Loss: 0.2878477275543488 | Training Accuracy: 0.22186658653846153\n",
      "Epoch 2, Batch: 14000| Training Loss: 0.28760006719295467 | Training Accuracy: 0.2221880580357143\n",
      "Epoch 2, Batch: 15000| Training Loss: 0.2873189961304267 | Training Accuracy: 0.22257317708333332\n",
      "Epoch 2, Batch: 16000| Training Loss: 0.28705201731901614 | Training Accuracy: 0.222868408203125\n",
      "Epoch 2, Batch: 17000| Training Loss: 0.2867850364665775 | Training Accuracy: 0.22307582720588234\n",
      "Epoch 2, Batch: 18000| Training Loss: 0.28654294961359766 | Training Accuracy: 0.22334722222222222\n",
      "Epoch 2, Batch: 19000| Training Loss: 0.28628317280662685 | Training Accuracy: 0.22362027138157894\n",
      "Epoch 2, Batch: 20000| Training Loss: 0.28603923709988593 | Training Accuracy: 0.2238984375\n",
      "Epoch 2, Batch: 21000| Training Loss: 0.2857948222500937 | Training Accuracy: 0.22416536458333333\n",
      "Epoch 2, Batch: 22000| Training Loss: 0.2855969064682722 | Training Accuracy: 0.22434446022727272\n",
      "Epoch 2, Batch: 23000| Training Loss: 0.28535627642273903 | Training Accuracy: 0.22466134510869565\n",
      "Epoch 2, Batch: 24000| Training Loss: 0.28511951672968766 | Training Accuracy: 0.2248828125\n",
      "Epoch 2, Batch: 25000| Training Loss: 0.28489719118654727 | Training Accuracy: 0.22513296875\n",
      "Epoch 2, Batch: 26000| Training Loss: 0.2846666345315484 | Training Accuracy: 0.22540534855769231\n",
      "Epoch 2, Batch: 27000| Training Loss: 0.28445286109712387 | Training Accuracy: 0.22561053240740742\n",
      "Epoch 2, Batch: 28000| Training Loss: 0.2842523886247405 | Training Accuracy: 0.22583844866071429\n",
      "Epoch 2, Batch: 29000| Training Loss: 0.28405992975214434 | Training Accuracy: 0.22609577047413792\n",
      "Epoch 2, Batch: 30000| Training Loss: 0.2838697056949139 | Training Accuracy: 0.22631028645833334\n",
      "Epoch 2, Batch: 31000| Training Loss: 0.2836491720517797 | Training Accuracy: 0.2265616179435484\n",
      "Epoch 2, Batch: 32000| Training Loss: 0.2834506822978146 | Training Accuracy: 0.226796875\n",
      "Epoch 2, Batch: 33000| Training Loss: 0.2832385133337794 | Training Accuracy: 0.22703705018939394\n",
      "Epoch 2, Batch: 34000| Training Loss: 0.283021075547618 | Training Accuracy: 0.22727389705882353\n",
      "Epoch 2, Batch: 35000| Training Loss: 0.2828150704643556 | Training Accuracy: 0.22750558035714286\n",
      "Epoch 2, Batch: 36000| Training Loss: 0.2826025030546718 | Training Accuracy: 0.22772113715277778\n",
      "Epoch 2, Batch: 37000| Training Loss: 0.28240035265321667 | Training Accuracy: 0.2279210304054054\n",
      "Epoch 2, Batch: 38000| Training Loss: 0.28220427069813014 | Training Accuracy: 0.22809272203947367\n",
      "Epoch 2, Batch: 39000| Training Loss: 0.2820110658754905 | Training Accuracy: 0.2283115985576923\n",
      "Epoch 2, Batch: 40000| Training Loss: 0.2818147237583995 | Training Accuracy: 0.22852490234375\n",
      "Epoch 2, Batch: 41000| Training Loss: 0.2816235690215012 | Training Accuracy: 0.22872618140243903\n",
      "Epoch 2, Batch: 42000| Training Loss: 0.28143901642979613 | Training Accuracy: 0.22897395833333334\n",
      "Epoch 2, Batch: 43000| Training Loss: 0.28124864302714203 | Training Accuracy: 0.2291788699127907\n",
      "Epoch 2, Batch: 44000| Training Loss: 0.2810740913498131 | Training Accuracy: 0.22937890625\n",
      "Epoch 2, Batch: 45000| Training Loss: 0.2808883197353946 | Training Accuracy: 0.22959001736111112\n",
      "Epoch 2, Batch: 46000| Training Loss: 0.2807148593560509 | Training Accuracy: 0.2297970448369565\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_decoder(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('training_results/transformer-exp-3-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4 (no checkmate model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16628344\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=3,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 256\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 2e-4\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.8 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['<UNK>', 'CLS', 'e4', 'd5', 'exd5', 'Qxd5', 'Nc3', 'Qd8', 'd4', 'e6', 'Nf3', 'Nf6', 'Bg5', 'Be7', 'Qd2', 'h6', 'Bh4', 'g5', 'Bg3', 'g4', 'Ne5', 'h5', 'h3', 'Bd6', 'Be2', 'Nbd7', 'O-O', 'd3', 'c6', 'Nxf3+', 'Bxf3', 'Ne2', 'Bc5', 'c3', 'Re8', 'a4', 'Bf5', 'b4', 'Be4', 'Bxe4', 'Rxe4', 'Bxf6', 'Qxf6', 'Ng3', 'Re6', 'Qd3', 'Qg6', 'Qf3', 'Rae8', 'Nh5', 'Qc2', 'Qd1', 'Qxc3', 'Rb1', 'Re2', 'e5', 'exd4', 'Qxd4', 'Nc6', 'Qe3', 'd6', 'Bb5', 'Bxc6', 'bxc6', 'dxe4', 'Nxe4', 'Qxe4', 'Qxc6', 'Bd7', 'Qb7', 'Rb8', 'c4', 'dxc4', 'Nd5', 'Qa4+', 'Qd7', 'Qxc4', 'Nxc3', 'bxc3', 'Bf4', 'e3', 'Bd3', 'Bxg5', 'Nxg5', 'Qe7', 'Qb3', 'Na5', 'Bc6', 'Qd6', 'Bxc6+', 'Qxc6+', 'Bb4', 'dxe5', 'Nc5', 'a3', 'Ba5', 'Qxc5', 'Bb6', 'Qc4', 'Qe4', 'f5', 'Qc4+', 'Be6', 'Qa4', 'exf4', 'Kh8', 'Qxf4', 'Nd4', 'Nxd4', 'Bxd4', 'Nb5', 'Rfe1', 'Rad8', 'c5', 'a5', 'axb6', 'cxd3', 'bxa7', 'd2', 'Red1', 'Bd5', 'Nc7', 'Rf8', 'a8=Q', 'Rxa8', 'Nxa8', 'gxf6', 'Bxc3+', 'cxd5', 'Qe6', 'dxc6', 'O-O-O', 'Bg4', 'Qc5', 'b6', 'Rfd1', 'Rd6', 'Rxd6', 'Qxd6', 'Qb5', 'Rd8', 'Qa6+', 'Kd7', 'Rd1', 'Qxd1+', 'Bxd1', 'Ke7', 'Bf3', 'Qe5+', 'Kf8', 'Bh5', 'Rd7', 'Qh8+', 'Qxh7', 'Kd6', 'Qxf5', 'Re7', 'Bxf7', 'Kc6', 'Qf6+', 'Kc5', 'Qxe7+', 'Qxe4+', 'g6', 'Bg7', 'Nh6', 'Be3', 'Bb7', 'Nxg4', 'Qf6', 'Nxe5', 'Re1', 'Nxd3', 'Qxd3', 'Nd7', 'Rad1', 'Qg5+', 'Qg3', 'Qxg3+', 'fxg3', 'Rfe8', 'Ne4', 'f6', 'Ng4', 'Be5', 'Bxe5', 'fxe4', 'Nh6+', 'Kg7', 'axb4', 'bxa3', 'a2', 'Ne3', 'Ra8', 'Nc2', 'f4', 'Bc4', 'Bxc3', 'Bxf4', 'Na4', 'Ng5', 'h4', 'Bd2', 'Rxf5', 'gxf5', 'Qxg5', 'Qxg5+', 'Bb3', 'Nb6', 'Rf1', 'Rf4', 'N8d7', 'Rg8', 'Qe2', 'a6', 'Nxc4', 'b5', 'Qd4', 'Bxd6', 'Rxe8+', 'Nxe8', 'fxe5', 'Qg5', 'b3', 'd6+', 'Qe3+', 'Kh1', 'Rf8+', 'Kh7', 'Qg8+', 'Kg6', 'Qe6+', 'Rxf6+', 'Qxf6+', 'Kh5', 'Qf5+', 'Qf7+', 'Kh4', 'g3+', 'Kh3', 'Qd5', 'cxd6', 'Qg2+', 'Nxd5', 'Nb4', 'Nc2+', 'Kd2', 'Nxa1', 'Qxa1', 'gxf3', 'Ke1', 'Bxc5', 'Bh6', 'Qb1', 'Rfd8', 'Qd2+', 'Kf1', 'Bc3', 'Rg1', 'Qd1+', 'Qxd1', 'Rxd1+', 'Kg2', 'Rxg1+', 'Kxg1', 'cxb5', 'Bxb5', 'Rd1+', 'Bc2', 'Rd2', 'Rd3', 'Bxe6+', 'Rxa3', 'Kf3', 'Bd8', 'Rxa5', 'Bxb3', 'Ba2', 'Nd2', 'Bh2', 'Kh2', 'Rxe5', 'Rae1', 'Rxg5', 'fxg5', 'Ng7', 'Qf4', 'Ne6', 'Qc7', 'Bxh2+', 'Qb8', 'Ne7+', 'Rxd7', 'hxg5', 'Qg4', 'Bxe7', 'Rdxe7', 'Qh5+', 'Kg8', 'Ne8', 'Qc6', 'Qxd7', 'Rxe7', 'cxd4', 'Nxc6', 'f3', 'Qb6', 'Bf2', 'Qa6', 'Bxe6', 'fxe6', 'Na2', 'Qxa3+', 'Kb1', 'Rxb4+', 'Re1+', 'Bxg4', 'hxg4', 'Qxg4+', 'Qh3+', 'Kg1', 'Nh2', 'Bf1', 'Qh4', 'Na3', 'Qg4+', 'Bg2', 'Nc4', 'Re4', 'Rhe8', 'Rxe8', 'Nxf4', 'Qf1', 'Ne2+', 'Qxe2', 'Rxe2', 'Rxf2', 'Re8+', 'Nd8', 'Rxd8+', 'Kxd8', 'Bb2', 'Nf5', 'Bxf5', 'exf5', 'Rc8', 'Nbxd5', 'Nxf6+', 'Nxf6', 'Nbd2', 'Bxg2', 'Kd8', 'Qxh8+', 'bxc4', 'Nxd1', 'Bb4+', 'Bxd2+', 'Kxd2', 'Nxe4+', 'Ke3', 'Ng4+', 'Kxe4', 'Kf4', 'Nxf2', 'Nxh1', 'Nf2', 'Nd3+', 'Kg3', 'f4+', 'Bxc4', 'g5+', 'Kxg5', 'Rhg8+', 'Kh6', 'Rd6+', 'Kxh7', 'Bxa2', 'Bxd3', 'Rxd3', 'Rd5', 'Nf7', 'Rxg2', 'Rh5+', 'Bg8+', 'Rxh6+', 'Rh7', 'Rxh7+', 'dxc5', 'Qa5+', 'Qb4+', 'Qa5', 'Bxb4', 'cxb4', 'Qxb4+', 'Qxe2+', 'Bxe2', 'axb5', 'Ne7', 'Rf7', 'Nb3', 'Rxd5', 'Rc3', 'Bg6', 'Rc5', 'Rxh5+', 'Bc4+', 'Rh8+', 'Bxg7', 'Kxg7', 'Nf4', 'Bxd5', 'fxg6', 'Rdg1+', 'Qxd2+', 'Nf3+', 'Nxg1', 'Rxg1', 'Ra7', 'Ke4', 'Kf5', 'Kxe5', 'Raf7', 'Ng6', 'Rhg1', 'h7+', 'Rec8', 'Rc1+', 'Qh5', 'Ba3', 'Nxe7+', 'Qxe5', 'Qxc7', 'Qxf2', 'Qxh1+', 'Kb2', 'Qxh4', 'Qxd2', 'Nf1', 'Nxh5', 'Rdd8', 'Qc3', 'cxb3', 'Rac8', 'Rc1', 'Qb2', 'Na7', 'Rxc1+', 'Qxc1', 'Bxa3', 'Qxa3', 'Nce7', 'Bc1', 'exd6', 'dxe6', 'Qb6+', 'Qxa8', 'Bf8', 'Rf3', 'Ref1', 'Qb4', 'Qxb2', 'Qxa2', 'Rxb7', 'Bc8', 'Rb5', 'Bxg3', 'Rxg3', 'Qa1+', 'Rh1+', 'Qe1+', 'Kg4', 'Ba6', 'Bh7', 'Nxa7', 'Qxd8+', 'Bxd8', 'Nd6+', 'Nxc8+', 'Nd6', 'Bc7', 'Nxf7', 'Kxf7', 'Bxc7', 'Bxc2', 'Nh3', 'Rxh7', 'Bb5+', 'Qxa6', 'Rac1', 'Rxa2', 'Rfa8', 'dxc3', 'exf6', 'Kxe8', 'Qe2+', 'Qh7', 'Rxd8', 'Rxe6+', 'Ra6', 'Rxa6', 'bxa6', 'Kf2', 'Kb6', 'Kxa6', 'Bf7', 'Bxg6', 'Kb4', 'Ka5', 'Qxe5+', 'Rxe5+', 'Bd3+', 'Ndf6', 'Bxb8', 'Bf6', 'Bxa7', 'Qxf3', 'Rhd1', 'Qxb6', 'Qa2', 'Kc2', 'Qa2+', 'Kd3', 'Kb8', 'g3', 'Ne1+', 'Nxh2+', 'Nh2+', 'Ke2', 'Qxf8+', 'Qxe8+', 'Kc7', 'Qd7+', 'Re5', 'Nce2', 'Ke6', 'Ng5+', 'Kf6', 'Ne4+', 'Bh3', 'Kd5', 'Bg2+', 'Nxb6', 'Nf8', 'Bb1', 'Rg4', 'Rxg6+', 'Qxg6+', 'Qg7', 'Qf3+', 'Qh7+', 'Qg8', 'Qb8+', 'Nh4', 'Bxh6', 'Rg3+', 'Rh3', 'Qxh3', 'gxh3', 'Rxg5+', 'Kxh4', 'Rg1+', 'Kd4', 'Ba4', 'Bxe3', 'Rxe3', 'Rxf3', 'Rxf6', 'Nh8', 'Qxb8+', 'Qc8', 'Bxh3', 'Qxf7+', 'Qe8', 'Rh1', 'Rad1+', 'Ke8', 'Rxh5', 'Nf4+', 'f3+', 'Ng7+', 'Rxg7', 'bxc5', 'Nge2', 'Qxc2', 'Rfb8', 'Rxb8', 'Nc1', 'Qa3', 'Qxb2+', 'Kxb2', 'Nxf3', 'Rhf1', 'Rb2+', 'Nf2+', 'Ne5+', 'Nxc5', 'Nd2+', 'Rf2+', 'Rb8+', 'Ng2', 'Nf8+', 'Bxf8', 'Rxf8', 'Rxe3+', 'Rb7', 'Rxf7+', 'Kc4', 'Kb3', 'Nxh3', 'Kxa3', 'Kc3', 'Qg6+', 'Qxh5', 'Qxh6', 'Bxd7+', 'Kxe7', 'Nxd5+', 'Qh4+', 'Kf7', 'Qxb7+', 'Rhd8', 'Rxd2', 'Rxe1+', 'Nxe2+', 'Rf2', 'Bxf2', 'Nxb5', 'Nfd4', 'Na7+', 'Qf5', 'Qxg4', 'Rhg8', 'Qxg8', 'Rxg8+', 'Rg8+', 'Rg7+', 'dxe3', 'Rxc7+', 'Nc7+', 'e2', 'e1=Q+', 'Nxb4', 'Qxb4', 'Nbc3', 'Nxf5', 'Ra4', 'Qf7', 'Qxd6+', 'Ka8', 'Rxc6', 'Nxd7', 'Ngf6', 'Rab1', 'axb3', 'Ra3', 'Qb2+', 'Ra1', 'cxd7+', 'Bxd7', 'Ng8', 'Nxe7', 'Nxe6', 'Bxb2', 'Nxb2', 'Nb2', 'Rxc1', 'bxa4', 'Nxa4', 'Ned7', 'Qa7', 'Rxc3', 'Rxb3', 'Bd5+', 'Qxd5+', 'Qxb3', 'Nfd7', 'Qxc3+', 'Qxa1+', 'Bxf3+', 'Kxf3', 'Nxd4+', 'Qc1', 'gxh6', 'Re7+', 'Nxd6', 'Nxc8', 'Rxc8', 'Rxc4', 'Qd5+', 'Bxg7+', 'd7', 'R8e7', 'Rd4', 'Rxg2+', 'Rxh2+', 'Kc8', 'Bc5+', 'exf7', 'Rxf7', 'Qf8', 'Bxh8', 'Qxh8', 'Qe5', 'Kb5', 'c4+', 'Ka4', 'Bxb7', 'Rxb2', 'Rb3', 'Qc6+', 'Nxc7+', 'Bh3+', 'Kxh3', 'Nh7+', 'Nb7', 'Bxf1', 'Qxf1', 'fxe3', 'Bxf7+', 'Nxh8', 'Nxd3+', 'Nxd8', 'Qxd8', 'Nd1', 'Neg4', 'Qh6+', 'Rg6', 'Na6', 'Qxb7', 'Rb6', 'Qxa7', 'Bh5+', 'Rxd4', 'Ke5', 'Rhf8', 'Rf5', 'Rdf8', 'Nxa5', 'bxa5', 'Qxa5', 'bxc7+', 'Rxf2+', 'Bxa6+', 'Qxb5', 'Ka7', 'Qxe8', 'Qxf7', 'Qxe6', 'Nge7', 'Ba7', 'Bxa4', 'Kb7', 'Kxc6', 'Rxa7', 'Qxe7', 'Nhf4', 'Nxd2', 'Rxh3', 'Bh2+', 'Nxe3', 'Bh4+', 'Qb5+', 'gxf4', 'Nxe2', 'Nc6+', 'Nxc1', 'Ne6+', 'Nxf8+', 'Re5+', 'Rg2+', 'Rg7', 'd1=Q+', 'Rxd1', 'Kxd1', 'Nh7', 'Be1', 'Ra2', 'Bd1', 'Rb2', 'axb2', 'Rxe6', 'Qh3', 'Qh8', 'Qf4+', 'Qd4+', 'Ka6', 'b5+', 'Qxc8', 'Qc5+', 'b6+', 'b7', 'Kxb7', 'Rb5+', 'Nbxd2', 'Nxa2', 'Nbd3', 'Re3', 'Rxc7', 'Ne1', 'Qxe6+', 'Rcd1', 'Rab8', 'Ree2', 'Nxg3+', 'hxg3', 'Rxf1+', 'Rxf1', 'Bd4', 'Nb1', 'Kxf1', 'Bxh7+', 'Rb1+', 'Rce1', 'gxh4+', 'h2', 'Nxg3', 'Bxd4+', 'Bxa5', 'Bxe1', 'Bd4+', 'Ba3+', 'Kxd5', 'gxh4', 'Bxh4', 'Qe1', 'Nxh4', 'Qxe3+', 'Rdh8', 'Ngf3', 'Ra5', 'Rxa4', 'Kxb5', 'Nxd7+', 'Bxf2+', 'Kxf2', 'hxg3+', 'Rf5+', 'Rxg3+', 'Rf4+', 'Rg5+', 'Rd5+', 'Re6+', 'Rd7+', 'Qxf5+', 'Bf4+', 'Qxh2+', 'Qxh2', 'Kxh2', 'Rg3', 'Nxg6', 'Rxh8', 'exd3', 'Qh6', 'Qxg7', 'Nxc6+', 'Nd4+', 'Bxa8', 'Qf8+', 'Nb4+', 'Kxc7', 'Qb7+', 'Qxd7+', 'Nc8', 'Rh4', 'Rxf3+', 'Qxa7+', 'Rc7', 'Qf2', 'Rh7+', 'Rxe4+', 'bxc3+', 'Rxe1', 'dxc7', 'Nd3', 'c8=Q+', 'Bxc8', 'Rxb4', 'Ra8+', 'Rxb8+', 'Qxb8', 'b2', 'Rxb1', 'Kxb1', 'Nc5+', 'Nxb2+', 'Kxg2', 'Nxg2', 'Qe7+', 'N2f3', 'Bxh1', 'Qxd3+', 'Qb1+', 'Qe4+', 'Rfe1+', 'Nxb3', 'Rfxd1', 'Ng1', 'Rxf4+', 'Rh6', 'Rdh1', 'Rxe7+', 'Nb6+', 'Rxb6', 'cxb6+', 'Kxb6', 'Ndb4', 'exf3', 'Kxf8', 'Nxb8', 'Qd8+', 'Qxf2+', 'Qf2+', 'g6+', 'Nbxd7', 'Nxd6+', 'Nxa3', 'Qg2', 'Rfc8', 'Bxc1', 'Qxb1', 'Nxf4+', 'hxg6', 'Bb8', 'Rde1', 'Rxf4', 'Nbc6', 'N7g6', 'Rfc1', 'Rdc1', 'Qg7+', 'Nce5', 'Nf6+', 'Qxe3', 'Rg5', 'Rhe1', 'Nc4+', 'Rxh6', 'Rc6', 'R7xc4', 'Kxd4', 'Ncxe5', 'Rxh3+', 'c3+', 'Qe8+', 'Qxa4', 'N7f6', 'Rb4', 'e7', 'Rd8+', 'Rf7+', 'Rxg6', 'Rxc5', 'Rf6', 'Rxb5+', 'g2', 'Ndf3', 'fxg4', 'Rcf1', 'R1f2', 'Be6+', 'Rc4', 'f5+', 'Rfe6', 'Nxe1', 'Be8', 'Nbd4', 'Rc8+', 'Rxf8+', 'Qa1', 'Rdf1', 'Kc1', 'Qxg3', 'Nxb7', 'Nfg4', 'Ndb5', 'Ncd7', 'Bd2+', 'Qxe1', 'Qg3+', 'N1f3', 'Rxh4', 'Rh5', 'gxh5+', 'Qxa4+', 'd7+', 'Bc7+', 'Rxa7+', 'd8=Q+', 'Bc8+', 'Nce4', 'N3d2', 'Ngxf2', 'Nh4+', 'Qh2+', 'Bg3+', 'Qxg2+', 'Rdg8', 'Ra2+', 'Nef4', 'Rbc1', 'Qxf8', 'R7xd6', 'Qxc8+', 'Kxe2', 'Nxf8', 'Qxf3+', 'Nh5+', 'Bf6+', 'Qxg8+', 'Kxd7', 'Bh6+', 'Raf8', 'Qd3+', 'Rh2', 'Nxe8+', 'Neg5', 'Nxg6+', 'Qxg6', 'Rbe8', 'Bf3+', 'Rd2+', 'Rhd3', 'Re2+', 'Rf3+', 'Bxe8', 'Nxf1', 'Nxc2', 'Rh8', 'Rg6+', 'Rxd6+', 'Kxd6', 'Bxf5+', 'Qd6+', 'Nhf2', 'Qxc5+', 'Rxh2', 'Qg1', 'Bg4+', 'cxb4+', 'Rh3+', 'Kxb4', 'Qxe1+', 'fxg7', 'Bxa6', 'Rxc2', 'Rxg4', 'Qc8+', 'Qc2+', 'Qc1+', 'h4+', 'Qxf4+', 'Nfd2', 'Rfxf6+', 'Nbd5', 'Bd6+', 'Ba5+', 'e5+', 'Bxd2', 'Rb7+', 'Ra1+', 'Bg7+', 'Nxa6', 'Qxh7+', 'Qb3+', 'Rbb2', 'Rc2', 'Nb8', 'Rgg2', 'Qh1+', 'Rg2', 'Nxg7', 'Be5+', 'Kxh8', 'Ndxe4', 'Bxb1', 'cxb6', 'Bxb6', 'Nd5+', 'Reb1', 'Rbb7', 'Nec6', 'Kxg6', 'Kg5', 'Bh1', 'Rxd7+', 'Bxh2', 'Ka3', 'Kxa2', 'Kxb3', 'Nfxd4', 'Bg5+', 'N2b3', 'N4f3', 'Kxd3', 'Rde8', 'Nef6', 'Qxa8+', 'Rxc2+', 'b3+', 'Be4+', 'exf2+', 'Qxd4+', 'Bxe2+', 'Qxg2', 'dxe7', 'Bxa1', 'Rxa1', 'Kxf6', 'Nxh3+', 'Nhg5', 'Nde5', 'Ndxe5', 'Nde4', 'Rh2+', 'Qa8+', 'Qc7+', 'Kxg3', 'Na4+', 'Rag8', 'Bxh7', 'c2', 'Raf1', 'Rdd2', 'Bg6+', 'Bxh5', 'Kxh6', 'a7', 'Kxa8', 'h1=Q', 'Rxh1', 'Kxh1', 'Kxf4', 'Kxe6', 'e8=Q+', 'Ncd4', 'Nxg8', 'Kxg8', 'Rxc8+', 'Qc3+', 'Bxe3+', 'Rbe1', 'Rge1', 'Ned5', 'Nxc7', 'Qxh6+', 'Bxd6+', 'Bxf8+', 'Kxa4', 'Bc6+', 'Kxc2', 'd8=Q', 'e8=Q', 'Qeg6+', 'fxg2', 'Bxg2+', 'Nxc1+', 'Kd1', 'Raxd1', 'Nxh4+', 'Nh1', 'Ncxe2', 'Kxe3', 'Bxe4+', 'Rc6+', 'Rf6+', 'Nxa2+', 'Rc7+', 'h7', 'gxf2+', 'fxe1=Q+', 'fxe4+', 'Kxg4', 'f2', 'Rf1+', 'Nxh2', 'Rbc8', 'Rfe8+', 'exf7+', 'Rbxf2', 'Rc5+', 'Be3+', 'Rxb7+', 'Nb5+', 'Qxc2+', 'Ree1', 'Rcb8', 'Rea1', 'Rcc8', 'R1xb2', 'Reb8', 'Rbd1', 'Rbd8', 'Kxe1', 'Qxh1', 'R6e7', 'Reg8', 'Rff3', 'd3+', 'Nxh7', 'Rah8', 'R8h2+', 'Ng6+', 'Nfxg4', 'dxc5+', 'Kxc5', 'dxc6+', 'Rfd1+', 'Rxg8', 'Rc2+', 'e6+', 'Rcxc7+', 'Qh2', 'R4e2', 'Rxg4+', 'cxb2', 'Qxh3+', 'g4+', 'Ba1', 'Ba8', 'R1e5', 'Kxa7', 'axb6+', 'Ka2', 'Nc3+', 'Raa1', 'Red8', 'Rxg7+', 'Rff7', 'Rag7+', 'Nfxd2', 'Ba6+', 'Qxb1+', 'Nxg5+', 'Rde4', 'c7', 'b8=Q', 'Rxb5', 'Nxh7+', 'Nexg4', 'Rdg1', 'bxc7', 'Bxd5+', 'Qxg7+', 'Rxe2+', 'bxa2', 'a1=Q+', 'Ndxc7+', 'N5xe6', 'dxe5+', 'Ne8+', 'Rgf8', 'R8f7', 'Rce2', 'd5+', 'exd5+', 'e4+', 'e3+', 'e2+', 'cxb7', 'Rhc8', 'Re3+', 'Rhb1', 'Bxf4+', 'Rxc5+', 'Kxh5', 'a1=Q', 'g7', 'R1d5', 'cxb2+', 'Ree7', 'Bf5+', 'bxc4+', 'Kxc4', 'exf5+', 'Kxf5', 'c1=Q+', 'Qcxf4+', 'Qf1+', 'Qxg1+', 'Raxd8', 'exd2', 'Re4+', 'Nf5+', 'Qxa5+', 'Bxb5+', 'Raa8', 'Bxg5+', 'Rhb8', 'Rh6+', 'gxh2+', 'Nxc2+', 'f7+', 'Nxh8+', 'Bxe5+', 'Rxb3+', 'Qa8', 'gxh5', 'Nde2', 'Nfd5', 'h3+', 'h1=Q+', 'Kxc3', 'e1=Q', 'Nxc5+', 'Rxd3+', 'Rgc8', 'b7+', 'bxc8=Q+', 'Ra7+', 'Rfb1', 'Rag1', 'Ndxb5', 'Rcd8', 'Nhf6', 'Bg8', 'Ra6+', 'Rxf5+', 'exf6+', 'Ndb3', 'Nbxa4', 'Rbd4', 'R1d2', 'R2d3', 'Rbd6', 'Rexd8', 'hxg2+', 'Bxc5+', 'Nec1', 'Na1', 'Ref7', 'Nxg2+', 'Raf1+', 'Ngxe3', 'Rc3+', 'Rd3+', 'Bh7+', 'Qxh5+', 'Rhe8+', 'Rb6+', 'Rb3+', 'Ncb5', 'Na8', 'Rdh2', 'Rgg7', 'R1e2', 'h8=Q+', 'Rcxd8', 'Bc3+', 'R6xb4', 'Rbb8', 'Qa7+', 'cxd2', 'Rbxc8', 'Rxd4+', 'Nxb3+', 'Rfxd8', 'Rhf4', 'R3f4', 'Rg4+', 'Nxe3+', 'Qac6+', 'Rcf8', 'fxe3+', 'f2+', 'd2+', 'fxg3+', 'Rfe4', 'a8=Q+', 'Ka1', 'c8=R', 'Rb4+', 'Ra3+', 'Nb3+', 'Ngxe4', 'Nbxd4', 'Rbb1', 'a5+', 'g1=Q', 'R5h6', 'gxf5+', 'c1=Q', 'f7', 'Ng3+', 'Nd7+', 'Qfc6+', 'Qcg6+', 'Qgxg5+', 'Nge5', 'Nge4', 'Nxg4+', 'N1e2', 'Ree8', 'h5+', 'R1d3', 'Rdg2', 'gxf4+', 'Ba4+', 'Bxe8+', 'Qxb5+', 'Rdd7', 'Rhc1', 'Ng2+', 'Nbxc3', 'Bxg8', 'Nxf7+', 'Qxf1+', 'Bxb7+', 'Nexc5', 'Rfxe1', 'Nf1+', 'Red7', 'R8d3', 'R7e5', 'Nce4+', 'Ngf2+', 'Rxd2+', 'Nxa7+', 'Rdf7', 'Rhxd8+', 'N7b6', 'Bxf6+', 'Ne3+', 'Nfg5', 'Bf7+', 'Rxc4+', 'Rxa3+', 'Rda1', 'Qxc7+', 'R8g6', 'Qcf5', 'hxg2', 'Ref8', 'Qxb3+', 'Qxc1+', 'Kxc1', 'Rfe3', 'Ngxe7', 'Rge8', 'O-O-O+', 'hxg6+', 'Rxh1+', 'Rgf1', 'Nfe3', 'Nxf5+', 'dxc3+', 'Rfxf7', 'Rbg7+', 'Rah1', 'Rexe2', 'Rexe4', 'Nh3+', 'a3+', 'b4+', 'Nfxd5', 'Bd7+', 'Ned4', 'fxg4+', 'f6+', 'f8=Q+', 'Ncxe4', 'Reg1', 'R6g2', 'R7g4', 'Nec5', 'b1=Q', 'Nfxe4', 'Rac1+', 'Bf2+', 'Bxb4+', 'Nd8+', 'Be7+', 'N5b6', 'Bb2+', 'Nge3+', 'R8h5', 'Rgh5', 'cxd4+', 'Rh4+', 'R1f4', 'R4f5', 'Bxc2+', 'Rbxc1', 'Rcg1', 'Rad8+', 'Rgg8', 'Rd4+', 'N2a4', 'Ba7+', 'Nde7', 'Nxh6+', 'N4e5', 'Nexg6', 'Rdxd2', 'Nxe5+', 'Nxd8+', 'Ncxd4', 'g8=Q', 'R1h4', 'Nxh1+', 'Nxh6', 'Rbf6', 'Nxd2+', 'Ra4+', 'dxe8=Q+', 'Rcxd6', 'Nexd5', 'Nxf2+', 'Rfg1', 'Nef5', 'exf1=Q+', 'Bxg4+', 'd1=Q', 'exf3+', 'c5+', 'g8=Q+', 'Rae2', 'Nf7+', 'Nfe5', 'c8=Q', 'Bxg1', 'Raa2', 'Rab2', 'bxc2', 'Rfd8+', 'Qxa2+', 'N8f6', 'Ndb8', 'Nac6', 'R8a2', 'Bxb3+', 'b1=Q+', 'Bh8', 'Rxb1+', 'f1=Q+', 'Qff2+', 'Rbg8+', 'Rhh8', 'g2+', 'f1=Q', 'Rxb6+', 'Rgf7', 'Nexd2', 'R1d7+', 'R5d7+', 'gxf3+', 'Rdb1', 'fxg5+', 'Rff6', 'b8=Q+', 'Ndxe7+', 'Ngf5', 'Kxa5', 'Nec3', 'Ngxe5', 'Kxb8', 'cxd5+', 'Rcc7', 'Be1+', 'cxd8=Q+', 'fxg6+', 'hxg5+', 'Ngh7', 'Qa3+', 'Rhh2', 'Rhg2+', 'axb7', 'Rdxf8', 'bxa4+', 'Raxc8', 'Ndf3+', 'R8e6', 'Qxc4+', 'Qxg1', 'Nexf5', 'Rae8+', 'Rdxc7', 'gxf7', 'axb7+', 'Nfxe5', 'Rff1', 'cxd7', 'R2xa3', 'gxf6+', 'Rfxc1', 'Rhxe8', 'Rdf5', 'R1c3', 'R5xc4', 'Bxg3+', 'R8e2', 'dxe2', 'Rdc7', 'Nde8', 'Ncd5', 'Bb3+', 'Rxa1+', 'cxb7+', 'bxa8=Q', 'Rxc6+', 'Rfxe8', 'Qxb6+', 'Ncd5+', 'gxh7', 'hxg8=R', 'Bxa7+', 'Bb8+', 'Ngh2', 'Raxd7', 'dxc1=Q+', 'Rcb1', 'Bxe7+', 'R1g6', 'N8g6', 'N4a5', 'Ndxb3+', 'Rhe7', 'Nxe6+', 'Rbc5', 'Rfxe7', 'Nac7', 'fxe5+', 'Nxc3+', 'Nb7+', 'Bxg6+', 'Kxc8', 'Ndc3', 'Rce8', 'Rfxf2', 'Rhb2', 'Nexc4', 'Nce3', 'R2f3+', 'cxd3+', 'h8=Q', 'exd7', 'Rff8', 'N3e2', 'Rexb7', 'Be8+', 'R5c4', 'R4c3', 'h6+', 'Rhg4', 'R7g5', 'Rhh6', 'Bg1', 'Nxb5+', 'axb8=Q', 'Bb6+', 'Nfg4+', 'Rxc3+', 'R5e3', 'Bxh4+', 'exf4+', 'exd6+', 'Ndxe6', 'Qxh4+', 'Nexg2', 'Ngxe6+', 'R8h3+', 'Rdd3', 'Rgg3', 'Rec3', 'Rgd8', 'Rcc4', 'N8g7', 'N6xe5', 'dxc2', 'Rha8', 'R5b3', 'R3b2', 'R2a3', 'c6+', 'R8a4', 'Rbc8+', 'g1=Q+', 'Nexd7', 'R8f5', 'Ned6', 'Ra5+', 'Qab7+', 'Rgh8', 'Raxb1', 'Bxa2+', 'Rda8', 'Nfe2+', 'Rcd7', 'Rbc7', 'Bxc7+', 'N1c3', 'Raxc1', 'Qg1+', 'Nd1+', 'Rc4+', 'Bg1+', 'Kc1+', 'Raa6', 'h2+', 'Rfg8', 'fxg7+', 'Rgd1', 'R4d3', 'Rfd3', 'Rdb6', 'f8=Q', 'g7+', 'R6e4', 'R4e3', 'Nfxg5', 'Rexd1', 'Rbg1', 'R4e6', 'Rxa6+', 'Ncxd5', 'Nfe7', 'Nde6', 'Nhf3', 'Rbd7', 'Rxa5+', 'Nxg7+', 'Bxc4+', 'Ndxf3', 'd4+', 'Nbc5', 'Ngxe2', 'Rhh5', 'Rgh7', 'Rfa1', 'Rfg4', 'Nxe1+', 'N3e4', 'Rcf6', 'N2f4', 'Nexc7', 'Rcc6', 'Rha1', 'Nexd4', 'Rec1', 'Bxh6+', 'Kxa1', 'Rxb2+', 'Nxc4+', 'Rbxb2', 'Ned2', 'Ngf4', 'Nfxd3', 'Nxa1+', 'Bc2+', 'Rxa4+', 'Ndxe7', 'fxg2+', 'Rea4', 'Nhf5', 'Rxa2+', 'Rdd1', 'dxe4+', 'Rge1+', 'hxg7', 'N5xg6', 'N5g6', 'cxb1=Q', 'Rac8+', 'Qh1', 'Nce1', 'N8e7', 'Ndxf3+', 'a4+', 'Rdc8', 'Rcc2', 'Nc1+', 'Rbxd8', 'Rxh4+', 'Kc4+', 'Bd8+', 'g8=R', 'Rba6', 'Ndf5', 'Ncb4', 'Kd7+', 'axb5+', 'Bxa3+', 'Bd1+', 'Bxe1+', 'Rgxb7', 'R6d4', 'Na5+', 'Rca1', 'R8g5', 'Rfd4', 'R8d6', 'fxe7', 'R8f3', 'Rfxe3', 'R1c6', 'N5c6', 'Rag8+', 'R1c6+', 'Rfxf2+', 'Nfe4+', 'Rgxg3+', 'Ndc5', 'Nac5', 'Nxa3+', 'Rhb6', 'R6b3+', 'Rxa8+', 'N7d5', 'N5c3', 'Nxb1', 'Rhe1+', 'hxg4+', 'N2c3', 'Ref2', 'Rfg7', 'R2g6', 'Ree6', 'Rhg6+', 'Rfe7', 'Nb2+', 'Rhg5', 'Rgb8', 'Rhg7', 'N1d2', 'Qge8', 'Qfd7', 'Qec8', 'Qdb7', 'Qca8+', 'Rdxe3', 'Rdxf2', 'axb4+', 'Bxc8+', 'Nxb7+', 'Nec8', 'R5f6', 'R1d6', 'Rha5', 'R5a7+', 'R8a7+', 'R1xe2', 'O-O+', 'Rdd6', 'Rhh7', 'Rhd7', 'R1e2+', 'R8e3+', 'Rxh8+', 'Bxh3+', 'Rbe3', 'R8d7', 'Rcg7', 'Nab4', 'Nca3', 'Nh1+', 'Nac4', 'Nxb6+', 'Nxh5+', 'Nfh5', 'Rfc7', 'R5d7', 'R1b3', 'Rba3', 'Rcxd4+', 'Ncd2', 'Rdf1+', 'Rbxb4', 'R1e6', 'Rec2', 'R6c4', 'Rdb8', 'Bxh8+', 'Rea8', 'Rxd5+', 'Rac3', 'Rgxe8', 'Rexd7', 'Rcd3', 'Rcd8+', 'Qhd7+', 'dxc8=Q+', 'Rcc1', 'R5d6', 'Rae7+', 'Nxb4+', 'Reb2', 'Rbxd1', 'Na1+', 'Kc5+', 'bxc6+', 'bxa6+', 'Nxg1+', 'Ned8', 'gxh6+', 'Rhxh5', 'Bxh5+', 'Nef3', 'Nbd6', 'Nxd1+', 'Red3', 'axb3+', 'Nfg3', 'Ke8+', 'N6d7', 'R3e4+', 'Rge7+', 'Rcb2', 'R3xb2', 'Rdh3', 'Bc1+', 'R8a7', 'R2a4', 'Rgxg6+', 'Rhf6+', 'Rfe2', 'Nfe6', 'R4h6+', 'R7h5+', 'Bxd8+', 'Rdc2', 'N5f6', 'Ncb3', 'Nge6', 'Ncxb3+', 'Qxa6+', 'Raxc2', 'Rexe4+', 'Qee2', 'Qff1+', 'Rdxd6', 'Rdb4', 'dxc4+', 'Rdxd7', 'Rhxf4', 'a1=B+', 'Reh8', 'N7c6', 'Nec4', 'Ncxb4', 'cxd1=Q+', 'R8c2', 'Ngh5', 'Rff2', 'Ngxf2+', 'Ngxf3', 'Rcxc2', 'R4f7', 'R1f6', 'Rfg6+', 'Nfd5+', 'Kg8+', 'R8d2', 'gxh1=Q', 'Qbb7+', 'Qcc8+', 'Nbxc6', 'Rexd4', 'Rgh4', 'R6e3', 'Rad4', 'cxb1=Q+', 'dxe6+', 'R1c7', 'Rbg8', 'Nhf7', 'Rhxh2', 'N8c6', 'dxe2+', 'Red8+', 'Reg5', 'Nfe7+', 'a6+', 'Ncb4+', 'Rge2', 'Ree5', 'Rhxh3+', 'R7d6', 'R1d4', 'Nac3', 'dxc7+', 'Ngxf7', 'Nbxc8', 'Rcg2', 'gxf7+', 'Nfe2', 'Rbc2', 'Rcd2', 'Qcc2+', 'R2e4+', 'Rhxf8', 'Ncxe6', 'Rcf7', 'Rdc5', 'Rdxe2', 'exd7+', 'Ngxf6', 'Qfg2+', 'R3e2', 'Ndc6', 'b2+', 'N4c3', 'exd2+', 'Bb7+', 'Nfe4', 'Rfe2+', 'Ndc7+', 'Rad7', 'Rgh1', 'Rde2', 'R1xd5', 'R6f7', 'Ref6', 'Rge6', 'Ndxe2', 'Q6e8', 'Qcd8+', 'Qhf6+', 'R1c7+', 'Rgxd3', 'Rgc2', 'cxd1=Q', 'Raxa7', 'Ref4', 'N6e5', 'Rgf6', 'Nbc7+', 'gxh7+', 'Rbxa7', 'Nexd3+', 'Rgh2', 'Rcxf5', 'R3xd2', 'Rbxe5', 'R7e4', 'Rba1', 'Rbb4', 'cxd6+', 'h8=N+', 'c8=N', 'b8=N', 'f8=N+', 'Rhg1+', 'Rab8+', 'Qgg4+', 'Qad4+', 'fxg8=Q', 'Rhxg8', 'R7xb2', 'Rca4', 'R8b6', 'Reb5', 'Kf1+', 'Be2+', 'Nac1', 'Rab4', 'Ndc2', 'hxg7+', 'Rhh3', 'Rhe3', 'Rde3', 'Rae7', 'Bf1+', 'Ree3', 'Na2+', 'Rde1+', 'R1e7', 'R2xf3', 'Rcxd1', 'Qbb7', 'Qgc7+', 'Ngf6+', 'R8d5', 'Raf3', 'R7d3', 'Rhxd5', 'Rbc6', 'Rba2', 'R1b5', 'Rexe5', 'Rcxc3', 'N6h5', 'R7b4', 'Rfe5', 'Qgb2+', 'Qbh6+', 'Qdg5+', 'Ncxb5', 'Rdd4', 'R6c7', 'N4a6', 'Nexd6', 'Na3+', 'Rbb6', 'Ndb6', 'N6e4', 'R6g4+', 'Rdf3', 'R3f2', 'Rff5', 'Rgg6', 'N8h7', 'Rhf6', 'R8c4', 'Rbb3', 'Rbc3', 'Rexd6', 'Rcxa5', 'dxe3+', 'Nba6', 'Rdxc1', 'R2e3', 'Red2', 'Rbxe8', 'fxe6+', 'Rgxf4', 'Nfd1', 'Rab1+', 'Bxc1+', 'Rae6', 'Ngxf4', 'Rde7', 'Nxf1+', 'Rda7', 'exd4+', 'Rab5', 'Bxa4+', 'R5b2', 'Rcxf6', 'Nhxf5+', 'Rac7', 'R7h6', 'Rac6+', 'Rhd6+', 'Rca8', 'Rhf8+', 'Rhd8+', 'R5xg4', 'R5d2', 'exf2', 'fxe2', 'Rab3', 'Qae1', 'Qgf1+', 'Qee2+', 'Qeg2+', 'Qfh1+', 'Bxb2+', 'cxb5+', 'N4g3', 'Rch8', 'R8g2+', 'R1g7', 'Rgg1', 'N1h2', 'Rbd3', 'Rhxd8', 'Nexf2', 'Qcb5+', 'Qbb2+', 'Rexe6+', 'Reg6', 'Rdxf7', 'Rbc1+', 'dxc1=Q', 'R7f3', 'Rcf5', 'R5f2', 'Nec7', 'R1e4', 'dxe1=Q+', 'R4f3', 'R3e4', 'Bxd3+', 'N2g3', 'Nfg8', 'Rdb7', 'h1=R', 'Rcg2+', 'Ncb5+', 'Rhxh7', 'Nhxg5', 'Raxb8', 'Nbxc2+', 'Qeh7+', 'R7xc6', 'R2xc5', 'd1=R+', 'R8b4', 'R1c4', 'R8c7', 'Nc8+', 'R1f3', 'R1g2', 'R3g2', 'Rfc6', 'R1c2', 'Rhg7+', 'Rhxf6', 'exf8=Q+', 'R1h6+', 'a7+', 'Rfd7', 'Rde6', 'N2e4', 'Rfc3', 'Rae1+', 'Nhxg4', 'R1h3', 'Rexe3', 'Rfd2', 'R8e4', 'Rfxd3', 'Raxa2', 'R8xe3', 'R6d5', 'Nhxf5', 'Rgc1', 'Rdb5', 'Rbxf6', 'Rbxe1', 'Qdf3+', 'Q4xg3+', 'Qgxg6+', 'Qgc6', 'Qcc4', 'Qbb3+', 'Nca6', 'N4c6', 'Rdg5', 'Rbf1', 'R1f6+', 'gxf2', 'N1c2', 'N4b3', 'Rexb3', 'Rcc3', 'Ncd6', 'Qfd1+', 'e7+', 'Raxe8+', 'Ncb6', 'R1xd4', 'Rga8', 'Rcxc4+', 'Rcxe2', 'Rbf5', 'Reg2+', 'Rhc2', 'Ndxe3', 'Nhxf6', 'R7f6', 'Rfg6', 'R6g3', 'Qbb3', 'R1a4+', 'Nbc4', 'Rbe6', 'b1=R', 'Rcb5', 'fxe8=Q', 'Rcg4', 'R8c7+', 'Rcd6+', 'Rdxd1', 'Rgf3', 'Rcc5', 'Nb1+', 'R1e7+', 'N2d4', 'Neg5+', 'R4h5', 'Rfb1+', 'Rbg7', 'gxf8=R+', 'Raa7', 'c7+', 'cxd8=R+', 'R7c2', 'Nfd3+', 'Raf2+', 'Bxd1+', 'Rgh3', 'bxc5+', 'c2+', 'Rfc8+', 'Rgd2', 'R6d7', 'Rcxc8', 'gxh8=Q', 'Rbg5', 'Ree4', 'Rhc5', 'R5c2', 'Rbxc2', 'R4d5', 'Rexc6', 'Kc7+', 'Nbc2', 'R4g7', 'Ncxe7', 'Naxc4', 'Raxa4', 'Rgd4', 'Ref3', 'Nde3', 'Ndf4', 'N5a3', 'N4h5', 'Rdg7+', 'Ngh4', 'Rfd6', 'Rbe7', 'Ncxd6+', 'Nca4', 'Rbxg7+', 'Reb6', 'R1xd2', 'axb2+', 'Rfd5', 'Rbf7+', 'Rhf7+', 'Ref7+', 'Rfxb1', 'Rhe5', 'R5e4', 'Rde5', 'dxe8=Q', 'Nab2', 'Rec5', 'Rba4', 'R4a3', 'Rgxe6', 'R8f6', 'Rag1+', 'Qgd8+', 'Nhf6+', 'Nfxh7', 'Rhf7', 'Rexc1', 'Rgf4', 'Raa4', 'Nbd5+', 'R8xe6', 'Qhc4+', 'Qgd5+', 'Qff3+', 'N5f3', 'Rfxc8', 'exd3+', 'Nab6+', 'Rexf7', 'R2d6', 'Raxd1+', 'Ke2+', 'Nce6', 'Ndxf7', 'Ncd1', 'Neg4+', 'Ndf1', 'Rhh1', 'Nhf4+', 'Rff4', 'Raxe8', 'R7d4', 'R6c5', 'Na6+', 'R2d7', 'Rbxb3', 'axb1=Q+', 'Ncxa2', 'N5xd4', 'Raf2', 'Rce5', 'R7e3', 'Rbe2', 'Rhg6', 'Rbf8', 'Rce1+', 'Rde8+', 'Rca6', 'Nab5', 'Ngf2', 'Rfxf5', 'Qfc4+', 'exd1=Q', 'Ngf5+', 'Nce6+', 'Reg3', 'Rag2+', 'Rca7', 'Nhf5+', 'Nfd7+', 'Rcg3', 'Rdf6', 'R3d2', 'Rbxd7', 'bxa7+', 'R4c6+', 'Ba2+', 'Nexf4', 'Qhd4+', 'Qce5+', 'Qexe4+', 'Qbe4+', 'Qdf2+', 'fxe8=Q+', 'Nfxg3', 'Qee1+', 'Rad6', 'R8xd5', 'Bxg8+', 'Rbd5', 'Rgxg2+', 'Rbf2+', 'R2xa6', 'Nfxe6', 'h8=R', 'f8=R', 'Ndxf6', 'gxh1=Q+', 'Rfd7+', 'Raxd2', 'R3d5', 'Rgf7+', 'N5xf4', 'R1h7+', 'Qdd2', 'N2xf3', 'Raf5', 'N5e4', 'Rea2', 'Rbe7+', 'Rde7+', 'Rfxf6', 'Rac6', 'Nhxf4', 'Rdf4', 'R4f2', 'Ndxf4', 'N2c4', 'Rgxg7+', 'Bf8+', 'Qgd1+', 'Qce2+', 'Qdf1+', 'Nfxh5', 'R1f7+', 'Rdg8+', 'dxe7+', 'Rfxe6', 'R1e3', 'cxb3+', 'Nexd3', 'N3xe5', 'Qhh5', 'Nxa6+', 'Rdxd4', 'gxf8=Q+', 'Nxa5+', 'Rdxc3', 'R8b7', 'Red1+', 'Rdc1+', 'Nexc7+', 'cxb8=Q', 'Rac2', 'N6e7', 'Rhb8+', 'R3xf6', 'Rcxc6+', 'Qhf3+', 'Rga2', 'Rcg7+', 'Ndxc3', 'Naxc6', 'Rbxa2', 'Rdg6', 'Rdxe5+', 'Rfxg2', 'R2g3', 'Qfxd7+', 'Rgxg5+', 'R1b2', 'Rdg2+', 'Rhe2', 'Rfa2', 'R2a3+', 'Naxc5', 'R1a3', 'N4h6', 'Ncxe3', 'Nexd8', 'Ndf4+', 'a2+', 'Rexb8', 'R8b3', 'R6xd5', 'N6b5', 'Rgf2', 'Rdf2', 'Red6', 'Nfh6', 'Rbxd4', 'R6g7', 'Rhxh6', 'R8e5', 'R1g7+', 'Nhg4', 'N7xd5+', 'Rgd2+', 'Nfg6', 'Rgxe1', 'Nexf6', 'Ng1+', 'Raxa6', 'R8c6', 'dxc2+', 'g8=B', 'N3e5', 'Rec1+', 'Ngh3', 'R4g2', 'Rac4', 'Rdxc5+', 'Neg3', 'N3c5', 'R8h4', 'Rcxc1+', 'Nxb1+', 'Rbe5', 'R8e3', 'Nac6+', 'R7xb6', 'Rbf7', 'bxa2+', 'Raxg2', 'Nge3', 'Nfh2', 'Rcxa3', 'R5e2', 'Rcg8', 'Rfxc7', 'N5a4+', 'N3a4+', 'Nexc3', 'Rch1', 'Reh7', 'Nexg5', 'Raxa3', 'Rag3+', 'Rhb3', 'Qeg3', 'Q1e1+', 'gxh3+', 'h1=R+', 'Nhg7', 'cxd2+', 'Rhxe5', 'Rdf6+', 'N4c5', 'Rcb6', 'R6b3', 'R1b2+', 'R3b2+', 'Nbxa7', 'Ncxb6', 'Nde2+', 'Nhg3', 'Kf6+', 'Nxa8+', 'Rgf5', 'Reg7+', 'R1d6+', 'gxf1=Q+', 'N5h4', 'N3g5+', 'Rec6+', 'R2h6', 'Rce4', 'Red5', 'Nfxd7', 'R8b7+', 'R7b6+', 'R1b4', 'R4b6', 'Ngh6', 'Ndxc4', 'Rdxb2', 'R4b3', 'Raxb4', 'Rcxa2', 'Raxd5', 'Neg6', 'Naxb6+', 'Bxb6+', 'R1a2', 'N3h4+', 'Nde7+', 'R2f5', 'R6d2', 'Rae3', 'Nb8+', 'Ndf7', 'Rbc2+', 'Rad2', 'exd8=Q', 'R8g7', 'N3h4', 'R5c6', 'Qhe8', 'Qge6+', 'Q6d7+', 'Qec8+', 'Qdb7+', 'Nxg8+', 'R2b7', 'Rhf3', 'fxe1=Q', 'f1=R', 'Rdc6', 'R8c3+', 'Rcxd2', 'R2d4+', 'R3c2', 'Rbxa4', 'Rexf2', 'Rexd2', 'Rdxd3', 'gxf1=B+', 'Bxa5+', 'Nbd6+', 'g1=N', 'R2xb3', 'Rexe1', 'Rgb7', 'Nac2', 'Kg2+', 'Nfe8', 'R4g3', 'Rbg2+', 'Ndc4', 'N7e6', 'R1xd3', 'R1h2', 'Nexc6', 'Qbe2+', 'Rba5', 'N4xh6', 'Bxb8+', 'Nef3+', 'Nfd6', 'R4e7+', 'Rexg6', 'Rcxg7+', 'Nexf7', 'Rgd7', 'N4xd5', 'R5g7', 'Nhg6', 'bxc8=Q', 'Qhg5+', 'Qdg8+', 'Qhh2+', 'R8xf6', 'Raf8+', 'Rdb2', 'Nef2', 'Rexe7', 'Rbxf7', 'Rgxg1', 'Rdxe6', 'bxa1=Q+', 'Bb1+', 'Reg6+', 'Rdf8+', 'Bxh1+', 'Ndxb4', 'Kf3+', 'a8=R', 'Rbf2', 'Rbxb7', 'Rgb5', 'R8h3', 'Rce7', 'Rhxf1', 'R2g7', 'Qce6+', 'Nce7+', 'R4g6', 'Nfe5+', 'Nbxc5', 'R6xe5', 'Nef8', 'Nge4+', 'bxa5+', 'a1=R+', 'Ncxd3', 'R3b6', 'Rde3+', 'Rhxh4', 'Rhxg5', 'R5g3+', 'Nba4', 'R1d7', 'Rfxc6', 'Qdd7+', 'R7e2', 'gxf1=N+', 'Q3d3+', 'Qec4+', 'Qdb3+', 'h1=N', 'Ncb8', 'Rgxd5', 'Rcxd3', 'Nxa4+', 'R8f2+', 'Raxc3', 'Rdb3', 'N4d5', 'Rhc4', 'R6f5', 'Raxf2', 'Rab7', 'Rbxc5', 'Rgc7', 'Kd4+', 'Rdxe8', 'Rfxd4', 'Nfg6+', 'Rdg7', 'Rha2', 'Nh8+', 'Nec2', 'Rexf1', 'Nhf1', 'gxh2', 'Rcd5', 'R5c3', 'Rbd2', 'Rch3', 'Rga6', 'Rcxe7', 'Ned3', 'R8a6', 'R1xc3', 'Na8+', 'Rbxa8', 'Rdd5', 'R7c5', 'Rcd1+', 'R1g2+', 'Rbe1+', 'Rbxf6+', 'R7f6+', 'Kf4+', 'R2f7+', 'Qef7+', 'Bxb1+', 'Rhxd1', 'c8=N+', 'N3e5+', 'Nexf6+', 'Nbxc4', 'R5xf3', 'Rfxf8', 'N3xd4', 'Rexb4', 'Rfg5', 'Rdxf7+', 'N3f4+', 'Nfd3', 'R8b2', 'Rdxf5+', 'Rgxg7', 'Ndc3+', 'Rexb1', 'Rdxa4', 'N6d5', 'Ndxf4+', 'Rfc1+', 'Nfd4+', 'Rexb2', 'Rcxc5', 'hxg1=Q', 'c8=R+', 'N7c6+', 'Bxf1+', 'Rcd4', 'R6e2+', 'Neg8', 'Nef4+', 'Rdxd2+', 'Rfxd6', 'Q6c8+', 'Rcf2', 'R8xd4', 'R1d2+', 'R2d3+', 'Rbxd3', 'Rhc7+', 'R6xf7', 'Rexd8+', 'Rdxc5', 'Raf4', 'R4d2', 'R8xf5', 'Rfxg1+', 'R4d7', 'Rcxe7+', 'R8d7+', 'bxa1=Q', 'R1g6+', 'R5f7', 'Ndf8', 'Nce2+', 'Ngh8', 'Rba7', 'Rgb1', 'Kf2+', 'N8d6', 'Rexe6', 'Rgf6+', 'N6f4+', 'R7f2', 'Rhb2+', 'Qfh3+', 'R2xa5', 'Nfh4', 'Rae4', 'bxc2+', 'hxg8=Q+', 'Reb3', 'Bxg1+', 'R4c5', 'R5c7', 'Ngf7', 'Ncd3', 'Rexc7', 'R6xf5', 'Rcd6', 'Q1d3+', 'Ned2+', 'R3c7', 'Rfb2+', 'Rab2+', 'Rcb2+', 'Rcg8+', 'Rdxb7', 'bxa8=R+', 'Qfg7', 'Q7f8+', 'R2c4', 'Rcg1+', 'Rea3', 'Rhd5', 'R7d6+', 'Qdd1', 'Qdxg2+', 'R3d4', 'R8d4', 'exf8=Q', 'Qeg3+', 'Q3g5+', 'Qed7+', 'Q5c6+', 'R3d7', 'Ncxe3+', 'Rfg3', 'Rhg2', 'Rfh6', 'Rexf6+', 'R8a4+', 'Rfxa4', 'N3f4', 'Rcxd4', 'Ndxf1', 'R2f6+', 'Rda3', 'N5h3', 'axb8=Q+', 'Rbb5', 'Ref5', 'Rhxf7', 'Nfxg4+', 'Kd3+', 'Ndxc5', 'Red4', 'Rdc3', 'exd8=N', 'R5g4', 'Reh2', 'R2h7+', 'Rdxe4', 'N1xe2', 'R6xf2', 'Rcxc7', 'R5e6', 'Rbf4', 'R6f4', 'Rhf2', 'Raxf3', 'Rca3', 'N7xf6', 'Ndxf2', 'Rfh1', 'Qhg7+', 'Ndb1', 'R1g5+', 'Rgxd1+', 'R6c2', 'N3g5', 'Raxc7', 'N4b5', 'R8xa4', 'R6a7', 'Rga1', 'R2xc3', 'Rcxf7+', 'Kc6+', 'R1xf2', 'Rfxf4', 'Qdd4+', 'Qee3', 'Qdd2+', 'Rexd5', 'Rfh2', 'R2h6+', 'Rec4', 'Rbxe2', 'Ref2+', 'Rbc4', 'exf8=R+', 'Raxf6', 'R2f3', 'Ned6+', 'Kh3+', 'R6xb3', 'R7xe4', 'R4e2+', 'R7e5+', 'R6xd3', 'Raa3', 'Rhxh4+', 'Rba8', 'R5a7', 'Rcxc6', 'Rab6', 'Rgb2', 'Reh1', 'N3d4', 'Nbd3+', 'Rbxc6', 'R2d5', 'Ng8+', 'Rbxa6', 'g8=R+', 'Rbxa3', 'Rea5', 'Rfxe5', 'Rcb1+', 'Rfxd2', 'R5f4', 'Ncxb3', 'R5g6', 'Ngxe6', 'Raxa5', 'R7e6+', 'R1xc2', 'Nbc3+', 'Rdxc6', 'cxd8=Q', 'R8xc3+', 'Rexf5', 'Rhc3', 'Nec7+', 'Rbxb8', 'R8d3+', 'R6f3', 'Nec5+', 'Rfxe2', 'Rfc4', 'exf1=Q', 'Bh1+', 'Rhxh3', 'Rdf3+', 'Rfxg7', 'Qdb1+', 'Qca2+', 'N7c5', 'Rgd1+', 'R6d3', 'Raxg2+', 'Nab6', 'Qad5+', 'Q5f5+', 'Rfb2', 'Ndxb1', 'Ncxd6', 'fxe7+', 'R1xf3', 'Rexh7+', 'Nfg5+', 'Rbxb6', 'Rge3', 'Qcxd7', 'Rbxe4', 'Rcg3+', 'Rfh3', 'Rbxf8', 'R3g2+', 'Rfg2', 'R2g4', 'Rbh1', 'Rexg2+', 'Nhf3+', 'Rbxa1', 'Rac4+', 'Reh3', 'N5d4', 'Raf6', 'R3f7', 'Rfb6', 'Nca7', 'hxg1=Q+', 'Rdg3+', 'R8xh5', 'R8c4+', 'Rgh6', 'R1h2+', 'Raxc6', 'axb1=R+', 'R2e7', 'Nde4+', 'R2f7', 'Ngxh7', 'Rbxf7+', 'Rbxb6+', 'R7xe5', 'exd8=Q+', 'Ncxd2', 'N7xe5+', 'Kd5+', 'Kf8+', 'R1e5+', 'R6c3', 'Reg5+', 'Ndxc8', 'Rca2', 'Rbe2+', 'R1b6', 'Rexb2+', 'Rae5', 'Rbh8', 'Ref8+', 'Rfb8+', 'Rdc7+', 'N4f5+', 'R1xh6', 'Rcxg2', 'Ndb2', 'Ngh1', 'Nexf3', 'bxc1=N', 'b1=B', 'R3a4', 'R8e2+', 'Qbf7+', 'Qexc5+', 'Qbxb5+', 'Rec7', 'R5b7', 'Rexf3', 'N5f4', 'Rag7', 'Nhxf3', 'N6xd5', 'R8xb6', 'Rfh4', 'Rfxg5+', 'R8b5', 'Bh8+', 'Qbb1+', 'R8h7+', 'R4h2', 'Rhg4+', 'R4e7', 'R8d2+', 'Qhe1+', 'Qfh5+', 'R2xd3', 'N4f3+', 'Ngxf4+', 'R6g5', 'Rfxg4', 'Kxd5+', 'a1=R', 'Rfh8', 'Rexd3', 'Qdb2+', 'Rexe8', 'R1a7+', 'R7g6', 'Rhd2', 'b8=R', 'Rgc8+', 'R4xb2', 'N6g5', 'R1e3+', 'Ndf7+', 'R8h2', 'Rec7+', 'R4d3+', 'R8xb5', 'R2xe5', 'Nhxf6+', 'R1g3', 'Reg2', 'fxe2+', 'Rfxf3', 'Qee4+', 'Qde5', 'Q4f4+', 'Qeg5+', 'Qfh4+', 'Rhxh6+', 'Raxe6+', 'N8h6', 'R1xb3', 'R5b6', 'Rga7', 'Rdxd3+', 'Qhe2+', 'N3h2', 'N3b5', 'R8xf4', 'Rge4', 'N4f6', 'Rge7', 'N3a4', 'R7xg6', 'Nhxg6', 'Rcxb5', 'R1h5', 'R5h7+', 'Rch7', 'R6h7+', 'Qbb4+', 'dxc1=N', 'R8f7+', 'Qgf7+', 'Qeh8+', 'Rdxf4', 'bxa3+', 'Ngf3+', 'R5f3', 'Ndxc6', 'Rcf2+', 'Kxf2+', 'Rfxd5', 'R4xb7', 'Rac5', 'Qfh8+', 'Bxa1+', 'Ncd4+', 'Nef7', 'Rgd5', 'exf8=B+', 'Qef4+', 'R2xe3+', 'Qhd6+', 'Rexe5+', 'Nef2+', 'Qae8+', 'N7e5', 'Nfe6+', 'Rdxf1', 'Neg1', 'Ncxd1', 'Nba3+', 'Ndc4+', 'Rgxd2', 'Raxh2', 'R3h2+', 'Qag8', 'Qfh7+', 'Rfc7+', 'Nfxh4', 'Nbxd5+', 'N7xc6', 'R2e3+', 'N3d5', 'R5g2', 'R5h4', 'Reg4+', 'Rbxb2+', 'R7xe6', 'Q1d1+', 'Nfg1', 'Rfxg3', 'R2c3', 'R3e7', 'Raxe5', 'Qac3+', 'Qef1+', 'Q8xe7+', 'Q3xg5+', 'Rgxf6', 'Rcxe3', 'R5e7', 'Qbxf8+', 'Qdxa5', 'N3d5+', 'Rcxe4', 'Rfxh4', 'Qaa5+', 'Qah5+', 'Qaa2+', 'Ngxh6+', 'R6f2', 'Nfd6+', 'Ncxd7', 'Ndxe5+', 'Rag2', 'Rfxf8+', 'Rdxd5', 'dxc8=Q', 'Rfxe4', 'Rhh4', 'axb1=Q', 'R7g3', 'Rch2', 'Nce8', 'Ke6+', 'Nbxd6+', 'dxe8=N', 'N4g5', 'N1xd2', 'R1a7', 'Nge6+', 'Raxb2', 'Qfb5+', 'Q5d5+', 'Qbd7+', 'R8xf3', 'Rcf7+', 'Q1xf6+', 'Rcxc4', 'Nbxd3', 'Ngh4+', 'Rdxc2', 'Rgxg6', 'Rbc6+', 'R8g3', 'Rge5', 'R7xd5', 'Rhc7', 'Rce3', 'Rbc7+', 'Rdc4', 'Rgc4', 'Rfh5', 'Rdxg7', 'R4d6', 'fxg8=Q+', 'N6c5', 'N5e6', 'Rhxe3', 'hxg8=Q', 'Qee3+', 'Qeg1+', 'Rcxd7', 'Rcxb2', 'Rexg4', 'R1g4+', 'Rexa3', 'Kd6+', 'Red6+', 'Rcf6+', 'Rfxe1+', 'h8=R+', 'Rfh7', 'R8c3', 'dxc8=R+', 'Rcxg2+', 'f8=N', 'Rfxf7+', 'Rbh5', 'Rgxf2+', 'Rgf2+', 'R6xb5', 'Rec8+', 'Rfxh5', 'Nde6+', 'R3e5', 'Rbxe3', 'Rfxg5', 'Rfxc4', 'N5e6+', 'Rdxc8', 'Raxe1', 'R8c5', 'Rgc5', 'R3h2', 'N3xc5', 'Nbxd4+', 'e1=R', 'Qdd8+', 'R1c5', 'Raxb1+', 'Rhxf5', 'R8xc4', 'R4h7', 'Rdh6', 'R7g2', 'N6c4', 'Rhe2+', 'Nge5+', 'Rbh2', 'R1b7+', 'Rea7', 'Rcxf4', 'Rcxf2', 'R7c2+', 'Raxf8', 'R7a2', 'Rhxg4', 'Qhe6+', 'Kg4+', 'Rdf2+', 'Rfa7', 'N3c4', 'Rexa6', 'R6a7+', 'Nxb8+', 'Rcxd7+', 'Ncd6+', 'R1xd2+', 'N2xe4+', 'R8b2+', 'R3xa4', 'Ngf8', 'Rgxf5', 'exd1=Q+', 'Kf5+', 'R3xc4', 'Rbd8+', 'N7g5', 'N7xe5', 'R3a6', 'Rgxf2', 'Nbc7', 'Rgf4+', 'Ref4+', 'Rfb3', 'Rac3+', 'R3a2', 'Rdxf5', 'R6e5', 'Rbxg2+', 'R1xe4', 'Qcxb5+', 'Rcd2+', 'R8xd6', 'R2g7+', 'Rexf8', 'bxa1=R', 'Naxc2', 'R8xb7', 'R8g4+', 'R2g5+', 'R4g5', 'Qab2+', 'Raf7+', 'Rcf3', 'R2xd6', 'Raxd3', 'Ncd2+', 'Qfc8+', 'R7d2', 'N3g4', 'Raxg1+', 'Rfxf1', 'Reg4', 'Naxb3', 'Qhxh3+', 'Qcf5+', 'R2d4', 'Rcxb7', 'R4xe5', 'exf8=B', 'Q8xd5+', 'bxc1=Q+', 'Ndxc2', 'Raxc5', 'Ncxa6', 'R4xc2', 'Rcg6', 'R8d6+', 'Rdf7+', 'Nab1', 'Nhg5+', 'R1xf4', 'Nce3+', 'Raxd7+', 'R5b4+', 'R1b3+', 'Nab3', 'Rexc4', 'R4c2', 'Raxc2+', 'Nexf3+', 'Rcxb3', 'Nce5+', 'Rfa3', 'Rbxb5', 'Rdxd7+', 'R7b5', 'N1xh2', 'N8b7', 'N7d6', 'Rdxd6+', 'Rfxc5', 'Kf7+', 'Rcb7', 'R5d6+', 'Rhxf2', 'Rhe4', 'N3xd5', 'R1a6', 'Reb4', 'R4b7+', 'Nbc1', 'R3e5+', 'R8e6+', 'fxg1=Q', 'Nef6+', 'Ngf1', 'Rexf4', 'R7f3+', 'Ndc7', 'R8f2', 'Qea1+', 'Nfxe7', 'N7f5', 'Nef7+', 'dxe1=Q', 'Rfb7', 'Rfc5', 'Rdxb8+', 'Rfa4', 'Rgxg4', 'Nac8', 'Rcb3', 'Rhxg1', 'Rhf1+', 'R8f4', 'Rah6', 'R1b6+', 'Rbxc3', 'Nfe1', 'Rcb4', 'Qbb1', 'Qaxf2', 'Q5c2', 'Rgxg2', 'Rcxf2+', 'Rhxd3', 'Rfxe3+', 'R1e6+', 'Rexg6+', 'Nfxg2', 'fxg1=Q+', 'Rdh4', 'Raxd4', 'R8xg4', 'Qaxa2+', 'Qgg6+', 'Qce4+', 'Rce2+', 'Kc3+', 'Nbd4+', 'N8xg6', 'Rga3', 'R1d5+', 'N1h3', 'Rgxg4+', 'R1xa4', 'R5a2', 'Rexc8', 'N2xh3', 'gxf8=Q', 'Qef2+', 'R7xf6', 'R6f2+', 'R1f2+', 'Rca5', 'N6f5', 'R7c6', 'R3c6', 'Nhf7+', 'Nge8', 'R4b7', 'R3g4', 'Rfxb8', 'N6b4', 'N1a3', 'Rhd6', 'Rfc2', 'Rcxd5', 'Reg1+', 'Qdd3', 'Qcc2', 'Raxd8+', 'N4h3+', 'Qed2+', 'Rbg2', 'R2g5', 'Rfe3+', 'Ned3+', 'Rdxg6', 'Q6f3', 'Q1g2+', 'Ndxb3', 'N6e4+', 'N5c4+', 'R7c6+', 'Rcxe4+', 'Nba5+', 'Nca5+', 'Raxc6+', 'Rhxe4+', 'Rgxf7', 'R4c6', 'Qgh5+', 'R4c7', 'N8c7', 'R8c2+', 'Qce1+', 'Qhg3+', 'Reh6', 'R1xe5', 'Rbg3', 'Rgb7+', 'bxc1=Q', 'R4xf3', 'Rbg1+', 'Rdg3', 'R6g4', 'Qcc7+', 'Ngxf3+', 'h8=N', 'Qha5+', 'Rhg3', 'Qfb3+', 'g1=B+', 'R3c4', 'Nfxd6+', 'N6a5', 'R1xf5', 'Rcxc3+', 'Rexf6', 'N4d6', 'Red5+', 'N5xc6', 'Rdxa5', 'N6h5+', 'N6d4+', 'Nexd2+', 'Nexc2', 'Rdxa6', 'Q8h4+', 'Qaa5', 'Rdh5', 'Qfc3+', 'Qbd2+', 'Qce3+', 'Ke5+', 'Rbd2+', 'Rde2+', 'R8xg7', 'Rgg5', 'R3c4+', 'R8a3', 'Rbxe7', 'R2b5', 'N7b5', 'R3e6', 'Ncxa4', 'R7xf4', 'N8xd7', 'Rag5', 'Rcxd3+', 'R1xa6', 'Reb7', 'R1c2+', 'Rfxd5+', 'Qaxc3+', 'Nfxd6', 'R1xc6', 'Qdxf4+', 'Qgxg4+', 'R7f5', 'Rfxb7', 'Rfxb3', 'N1e3', 'Qha8+', 'Rcf4+', 'R8a5', 'R5a4', 'Rbh7', 'R2h3', 'Rbxc7', 'Raxg1', 'Rgb6', 'Qdd6+', 'Rbd5+', 'N3xe4', 'Rhf5', 'Qag1+', 'Qfe2+', 'Qfg6+', 'Qgg2+', 'Qed5+', 'Rce8+', 'Nba3', 'R8a2+', 'R1a2+', 'N7xd6', 'Ned7+', 'Rgxc7', 'N4f5', 'Rcxc1', 'Qhf8+', 'Qeg7+', 'Rhxc3', 'Naxb5', 'Rdxe5', 'Rbxd8+', 'Rge2+', 'Kxh4+', 'R4f6', 'R2b4+', 'Kc2+', 'Rcg5', 'Qbb4', 'Qec1+', 'N3c4+', 'Qcg1+', 'Qgxg2+', 'Nhxf7', 'Nhxf2', 'Rdg4', 'Rdxf6+', 'Rexf1+', 'Nexd6+', 'Rbd1+', 'Qed7', 'Rgxg5', 'b8=R+', 'R5b6+', 'R6b7+', 'Rda6', 'Qbg2+', 'Rcf5+', 'Rge5+', 'Qde6', 'Rhb7', 'Raxb6', 'Rhxf3', 'N5c4', 'N3xe5+', 'Rbxe6', 'Raxb7', 'Ndf5+', 'Rab7+', 'Raxe3', 'R5d3', 'Qff4+', 'Qgf4+', 'R4b5', 'Ncb6+', 'N7xf6+', 'Qhxf8+', 'Nfxg6', 'R2xf3+', 'Rgc6', 'Rhxg7+', 'a8=N', 'Rdxd8', 'Ref1+', 'Ncb7', 'Rhxc8', 'Rdxb6', 'R3f6+', 'Nhg4+', 'Nhxf1', 'Rcxc2+', 'Rhd4', 'Nhf2+', 'Ndf2+', 'R3xb5', 'Rbxc4', 'Rcf8+', 'N7e5+', 'Nexg4+', 'Rfxc7+', 'Rda5', 'Ned1', 'Reh4', 'Rab5+', 'Rgd3', 'Qcb7+', 'N2xb3', 'Rag3', 'Rexc5', 'R5xe2', 'R2xa4', 'R2xc7', 'N2xc3', 'R8xe5', 'Rexf4+', 'Rgxf7+', 'cxb1=B', 'Rec4+', 'R1f5', 'g1=R', 'R5h7', 'Rdxg2+', 'N8a6', 'Ngxh5', 'N3b4', 'R7d5', 'R1h6', 'bxa8=Q+', 'R2h3+', 'Rhxc4', 'Naxc3', 'Qbe5+', 'R8xe4', 'R6a5', 'R1a6+', 'Rgxd6', 'R3f6', 'Rgxf8', 'Qfa1+', 'N4xe5', 'Qcxf5+', 'Qfe6+', 'Qdf6+', 'Qee7+', 'Qff8+', 'Rcf4', 'Rcxf1', 'R2xa3+', 'Rce6', 'd1=N', 'Ke4+', 'Ndxf5', 'Rbxh2', 'R8f3+', 'Rbxb3+', 'Rfc3+', 'R3xe2', 'R6xd4', 'R5xf6', 'Ngxf8', 'Rexh2+', 'R7e6', 'Qdc4+', 'R7a5', 'Rgxb6', 'N5f4+', 'Qhg2+', 'Qfa2+', 'Q1g5+', 'Rgxd1', 'R1xh2', 'Rec6', 'N1e3+', 'Qgd2+', 'Qca1+', 'Kg7+', 'Rcxd5+', 'g8=N', 'R3xe4', 'R3xh4', 'R2e4', 'R4e5', 'Kg3+', 'Reh5', 'R1a5', 'Raxb6+', 'Nexg8', 'Nde1', 'Nbxc2', 'Rexc2', 'N7xd5', 'Rexf8+', 'R3h4', 'Kh6+', 'Qhb2+', 'Qcc3+', 'Qcc1+', 'R7xh6', 'Qdg3+', 'R8xe7+', 'Q5h6+', 'R2b3', 'Qab8+', 'R6e2', 'R8xd7', 'R7xa3', 'N2g4+', 'N6h7', 'R4xc5', 'R8xh6', 'R3xc6', 'Rdxb4', 'e1=R+', 'N2d4+', 'Qgxe4+', 'Qbc2+', 'Rea6', 'R3h6', 'N4xf5', 'R6b7', 'gxf8=N+', 'Qhf4+', 'Rexe7+', 'Rfxd7', 'Rfg1+', 'R6xd7', 'N5xc4', 'Qhh7+', 'Qexb7+', 'N6f4', 'Rcxb6', 'Rdxc4', 'Rexe8+', 'c1=R', 'Raxa2+', 'Ref3+', 'Qea4+', 'Rdc3+', 'Rdb1+', 'Qcxe4+', 'N5xc3', 'Qcxc6+', 'Rfxf1+', 'Nfxe2', 'Rgxg1+', 'Rcxf7', 'Nhxf8', 'Rdxa2', 'R2a7', 'Qeh3+', 'e1=N', 'b1=N', 'f1=N+', 'g1=N+', 'Ndf1+', 'a1=B', 'h1=B+', 'Rfxd2+', 'R8h7', 'Qec6+', 'R7xg6+', 'Nfh3', 'Qad1+', 'cxb8=Q+', 'Qbxe8+', 'Rgf8+', 'Nac5+', 'Nab4+', 'Rdxh7', 'R6h3+', 'Rdxf6', 'Rbe8+', 'Rfe7+', 'Nexg3+', 'Ncxd7+', 'Rad2+', 'Nhf8', 'Rcxf8', 'Rhxh2+', 'Rhxe2', 'a8=N+', 'N5d4+', 'Rgd6', 'R7c5+', 'R8a3+', 'Rhe5+', 'Qab7', 'R7f4+', 'Rexe2+', 'Kb4+', 'N5xf6', 'Rexe3+', 'N4xg5', 'Nexf2+', 'Qbd4+', 'Q5c5+', 'Qdb4+', 'Qca5+', 'R2xc4', 'R6h3', 'Rbxd2', 'Rgxf8+', 'Rexa5', 'Ngxe3+', 'Raxe7', 'Ncxa7', 'Rexf2+', 'R3xd4', 'Rexf5+', 'Rbxd5', 'Rfg7+', 'Rgc7+', 'Qee8+', 'R8g7+', 'Qexb8+', 'Nbxd3+', 'fxg1=N+', 'Qdg2+', 'Qee6+', 'Qeg4+', 'Raxf1', 'Rbg3+', 'Reb8+', 'R8e5+', 'R3b4', 'Rgc3', 'Rda2', 'Nbxa2', 'Nbxc7', 'Qea5+', 'Qda8+', 'gxh8=Q+', 'Rfxb5', 'Nab8', 'Qeh1+', 'Rfxc3', 'R1xc5', 'Rgxe5', 'R1b7', 'R8xf7', 'Qff3', 'Ndxb2', 'd8=N', 'Nbd7+', 'R8e7+', 'R2f5+', 'N5b4', 'Qhf1+', 'Qbe1+', 'R3d6', 'R4xf5', 'R2c5', 'Rdxd5+', 'Nfxe3', 'Nbxd6', 'Rhxe1', 'e1=N+', 'Qgh7+', 'R3d5+', 'Q1b1+', 'N3c2', 'Qde2+', 'Q4e2+', 'Qfc1', 'Rec2+', 'R3d7+', 'Qgxg3+', 'Rexc2+', 'R3c6+', 'R1xf6', 'Rbf3', 'Ngf7+', 'Kxc2+', 'R5d4', 'R4g3+', 'Q7xe6', 'Q6xf5+', 'N2b4', 'Kh7+', 'R8e4+', 'N2c4+', 'Rch5', 'N3xf5', 'N5xf6+', 'R5xe4', 'Rexc3+', 'R1xe6', 'Kb6+', 'R1e4+', 'Qbg3+', 'Qhh4+', 'Raxd6', 'N6xe4+', 'Qexb4+', 'Rfd2+', 'Ndxf2+', 'Qcd6', 'Qexf3', 'R7d5+', 'Rgxf3', 'N2xa4', 'exd8=R+', 'Rfg8+', 'Rgxh5', 'Rhxf1+', 'Rgxf4+', 'Qda4+', 'Q4a2+', 'R7b6', 'Qhh5+', 'R7d2+', 'Qfxg7+', 'R2f4', 'Nab7', 'R6b5', 'Qgxf7+', 'Q8e8+', 'Qfd7+', 'Qfb4+', 'Rhxh7+', 'Ncd3+', 'Nfg7', 'Raxc4', 'Nac7+', 'R7c4', 'Nfxe2+', 'Rcxa3+', 'Rfd3+', 'Ree6+', 'Qhf5+', 'Rcxe6', 'R8xe7', 'Rah7', 'Ndxf6+', 'Rgxc6+', 'Ndf2', 'Rgg4', 'g1=R+', 'Qac6', 'Kxh5+', 'Qdb5+', 'Qca6+', 'Qdf5+', 'Qfd3+', 'Qec2+', 'Qca4+', 'R2xe3', 'Qbd8+', 'Qdxd7+', 'Qcc5+', 'Qaa4+', 'Qaa3', 'R5a3+', 'Raxb3', 'Kd2+', 'Rcxd6+', 'Reg7', 'Neg7', 'Rcxb8', 'N6d5+', 'Raa5', 'Rhxg8+', 'Rhb7+', 'R6h2', 'R7c3+', 'Raxf7', 'Q8b4+', 'Qbf1+', 'R5xc7', 'Rdh7', 'Nhg8', 'Ngxh6', 'N7g5+', 'Qgf5+', 'Qff7+', 'Qhxd1+', 'Ndb5+', 'R1b4+', 'Nef8+', 'Rbe4', 'Qbxc7+', 'Qhe4+', 'Nfh4+', 'Rcxd2+', 'Ncb1', 'Kg1+', 'Rgxb8', 'N5g4', 'Qaa8+', 'Qhxf7+', 'Rcxc8+', 'Rcxe8+', 'N5a4', 'N5g6+', 'R2c7', 'R7h2', 'R7h3', 'Qce3', 'Qhh6+', 'Qae2+', 'Qef6+', 'Q8h8+', 'Qexh2+', 'Nexg6+', 'Rbxa5', 'R3f4+', 'R7g6+', 'R6g5+', 'R2xb4', 'Nba5', 'Kxg5+', 'Rcxa7', 'Nfxd3+', 'R5e3+', 'Rcg4+', 'Nhxg3', 'Rda4', 'R2b5+', 'Rhxe3+', 'Rfg2+', 'Kh4+', 'Qbxg3+', 'N2xe4', 'R3g6', 'R4a7', 'Qcd7+', 'Rdxe1', 'Kxf4+', 'Ka6+', 'N3f5', 'Raxg8', 'R4xd3', 'R4a2', 'R6f5+', 'Ncd8', 'Rfxh6', 'R5h6+', 'N6xd7', 'R1xg2', 'Qae4', 'R3xd6', 'N8a7', 'R3f5', 'Kg5+', 'N6f5+', 'Rgxa5', 'Rexh5', 'R6xf4', 'R1xe7', 'Rcxg7', 'dxe8=R+', 'R2f6', 'Kg6+', 'Rhb5', 'N6xa5', 'Neg3+', 'Qdxc8+', 'N4xh5+', 'Qgg7+', 'Qee5+', 'Qfa8+', 'Nde8+', 'R7c3', 'Rhf2+', 'N6xc4', 'Rdxd8+', 'R4d6+', 'N1f2', 'Rad5', 'R1xc7+', 'R7d3+', 'Qfh2+', 'R5g3', 'R5c4+', 'R4c3+', 'N8xc6', 'Qhg4+', 'Raxe1+', 'Rdxa3', 'Qbb5+', 'Naxb2', 'Rexh2', 'R3xg4', 'Rgxd8', 'Nhxf4+', 'Rge8+', 'Rcxe5', 'Rcb8+', 'Kxf6+', 'N2xc3+', 'Ngh2+', 'Ba8+', 'Ke3+', 'Rexa4', 'R8xa3', 'Qde2', 'Q1f1+', 'Qgg6', 'Rdxh3+', 'Ref6+', 'R5f6+', 'Qaxf7+', 'R3xb4', 'R8xd3', 'N1xb2', 'Rgxc7+', 'Qfxe7+', 'Qfxe2+', 'Qed4+', 'Qdf4+', 'Reg3+', 'R7xa5', 'Raxf3+', 'Qcxf6+', 'Naxb7', 'N6g7', 'Nfg3+', 'Qff7', 'Qgg8+', 'Rgxb7+', 'Rexa7', 'Rbxc5+', 'Rdxg5+', 'Ncd7+', 'Ngf4+', 'Kd8+', 'Rhe6', 'Qbxb8+', 'R1xc4', 'R6xc2', 'N2h3+', 'N1xe3', 'Rfxf3+', 'Qge7', 'Qfd8', 'Qfg7+', 'Nge7+', 'R3xd7', 'R6a2', 'R6xh5+', 'R3h4+', 'Rha3', 'Rdd1+', 'Qcxe3+', 'Qgxf2+', 'Ngxf5', 'Ndxf7+', 'N3xc4', 'Nef5+', 'R6g2+', 'Rgxh7', 'Rcxe8', 'a1=N', 'Rhd7+', 'R3h7+', 'Nexd5+', 'Rhxh5+', 'Nac3+', 'Nfxd4+', 'Ncxd4+', 'Qdh1+', 'Q2e7+', 'Qge4+', 'Qeb7+', 'R5a6', 'R7e4+', 'h8=B', 'Nhg6+', 'Rcxb2+', 'R3h6+', 'Reb3+', 'Rbxb5+', 'Rhxc5+', 'R1xa7', 'Rhxe4', 'R6f3+', 'Nfxe4+', 'Rbxe6+', 'R6e5+', 'Kh8+', 'Red4+', 'Qcxe1+', 'Ngxf7+', 'R6d7+', 'Qcb7', 'Qbd5', 'R3b4+', 'Rdxe8+', 'R5b4', 'Rdxf3', 'N5c6+', 'Qdd1+', 'd8=R', 'Ncxd8', 'Rhxc1', 'Nec4+', 'R8b4+', 'Qbxb2+', 'Qfc7+', 'R4b6+', 'Rexg4+', 'Q8h7', 'Ndc1', 'Qbd5+', 'Qee6', 'R4a5', 'Rbg4', 'R4g5+', 'Rad3', 'Rbxe7+', 'h1=N+', 'Qhb2', 'Q1a1+', 'N2xd4', 'Raxe4', 'Rgxg3', 'Rdxh8', 'Reb2+', 'b1=R+', 'R8g6+', 'Rbxg7', 'R8g4', 'Rad3+', 'R2e5', 'Ndxf5+', 'R8xe5+', 'R2xe4', 'Rbxe1+', 'Rexg2', 'Nhxg2', 'R1xd6', 'Ndxf8', 'R8xf2+', 'Qea2+', 'Rexh7', 'R7h4+', 'R4h3', 'Qab8', 'Qbc8+', 'R4h7+', 'Qgxc5+', 'Q5b6+', 'a8=B', 'R1g4', 'R1c3+', 'Raxe2', 'Rhxe6', 'Qab3+', 'R4f5+', 'c1=N+', 'Red2+', 'Rdc2+', 'Rac2+', 'Rfe6+', 'Rgd8+', 'R7a3', 'Rexd4+', 'Qae5+', 'Ngh7+', 'gxf1=Q', 'Qhe5+', 'Ke7+', 'N6xe4', 'Qaxa6+', 'Qab5+', 'Q2b4+', 'Qgc8+', 'Qaxc6+', 'R7xh3', 'Rfb7+', 'Qfe3+', 'Qgb6+', 'Ngxh2', 'Qgxd4+', 'Rgxd7+', 'Qhb5+', 'R2b4', 'd1=R', 'R1h5+', 'Qgg7', 'N1d3', 'Qgd1', 'Qde5+', 'Qee8', 'Qeg8+', 'N7xg6', 'R2xg3+', 'N2xc4', 'Qfd6+', 'Q4b6+', 'Qdb8+', 'R1xh5', 'R2xd5', 'R5c7+', 'Rexc3', 'N3xa4', 'Q1d6', 'R1g5', 'R1xb4', 'Qfd4+', 'N7xe6', 'Qcd3+', 'Q3e2+', 'N6d4', 'R3d4+', 'Rga5', 'Nhxg6+', 'R6xb7', 'Qch5+', 'R8f4+', 'R1a4', 'Qexf7+', 'R8h6', 'Qbg6+', 'Reb1+', 'Qfxf3+', 'R3g5', 'Naxb4', 'R1xe3', 'b1=N+', 'Rfxc2', 'Ngxf6+', 'Rgxe3', 'Nca5', 'Nexg3', 'Rbxg6', 'Raxg5', 'Qge6', 'R6e7+', 'R8xd4+', 'N4d5+', 'R1d4+', 'Rhg3+', 'Rcxf1+', 'Rfxb4', 'Nbc8+', 'Kh2+', 'Qhxd4+', 'Q3c4+', 'Qdc5', 'Q5b5+', 'N5h4+', 'R2e6', 'R3b7', 'R8g2', 'Ndb3+', 'Qgd4+', 'N6xh5', 'Rcxg3', 'h1=B', 'Rdc6+', 'Rfg4+', 'c1=B+', 'exf8=N+', 'Qae5', 'Rfb4', 'Rhxe2+', 'R7a6', 'R7b3', 'Rgd7+', 'Ndxb4+', 'N4xe3+', 'Qdd5+', 'Rdxb3', 'Qde4', 'Qbf5+', 'Qdxg4+', 'Rhxg2+', 'Qbd6+', 'Raxa3+', 'Nfxe8', 'R3e6+', 'Rbxg3', 'Rgxe4', 'R6h5', 'Rce7+', 'Nac2+', 'R2a6+', 'R7a6+', 'R5a6+', 'R6xc4', 'Ka5+', 'Nfxe5+', 'Rfxd1+', 'R5h3', 'Rbxd6', 'N2e4+', 'R3xf4', 'Rgxc3', 'N5xe4', 'N2xf4', 'Rhd1+', 'Qaf1+', 'Rhxd2', 'Ncxe5+', 'Kb7+', 'Rdxb7+', 'N7a5', 'Nbc8', 'Nexg5+', 'R7g3+', 'Qac2+', 'Qdxc1+', 'Qdd4', 'Qff4', 'N7c5+', 'Kd1+', 'Nde3+', 'Qbc5', 'N4d3+', 'Qgd8', 'R3xd4+', 'R6xf3', 'Qde7+', 'Q4g5+', 'Ndb7', 'Qbd1+', 'Qfe2', 'Qexg2+', 'Q1h1+', 'R4c7+', 'Q6c5+', 'R6d5+', 'Rhxd5+', 'N2g4', 'Naxc7', 'Qff6+', 'Rfc2+', 'Rgf1+', 'Rhxd6', 'Raxb7+', 'Rfa6', 'dxc8=R', 'e8=R+', 'Qae1+', 'Rbxf8+', 'Ncxd3+', 'b8=B', 'N4xh3', 'N4d3', 'Qdb6+', 'Nfxe7+', 'Nexf8', 'R5c6+', 'exd8=R', 'Qef8', 'Qff5+', 'Qca3+', 'Rhxf7+', 'Rdxe7+', 'Nbc2+', 'Ndxc5+', 'N4xd3', 'R1xa3', 'Rexg7', 'Qdf8+', 'Raxc3+', 'Raxf7+', 'c8=B', 'f8=R+', 'Qexe8+', 'Qfxe8+', 'N5xa4', 'R6c7+', 'R3xf2', 'R4h6', 'Qcd5', 'Ndf6+', 'R2a5', 'Q2d6', 'Q8d7+', 'Rbd3+', 'Qbc1+', 'Rexg3+', 'Rfd6+', 'Qdxd3', 'R7xa4', 'Rde5+', 'Rfe5+', 'Nhg1', 'exf1=R+', 'R2g3+', 'bxa8=R', 'Red7+', 'R2xb6', 'Rfxh2', 'Qaa6+', 'R1xh3', 'Rdxg8', 'Raxa8', 'Rbf1+', 'Qhxg3+', 'Rhxe7+', 'R8xc3', 'Rhc6', 'c1=N', 'R1xb5', 'Rdxh2', 'R5h2+', 'Qbb6+', 'Qcb4+', 'N5d6', 'Qde6+', 'Qcc6+', 'N3e4+', 'Qhb1+', 'Raxb5', 'Q8xf7', 'Rah2', 'R2xg4', 'R8xb3', 'Q2d3+', 'Q3d7+', 'Rcb7+', 'Qda2+', 'Rhxe1+', 'Rhg5+', 'dxc1=R+', 'R7xa6', 'Rexd2+', 'R7b2+', 'Q1a3+', 'Ndxb7', 'Rgxd7', 'Nexf7+', 'N4c3+', 'Rexd1+', 'Rbxd7+', 'Nfg2', 'Raxg6+', 'N6xc5', 'R1d3+', 'Qdc7+', 'Q7c5', 'Rfxb2', 'Rhxc7+', 'Q8b3+', 'Qaxa7+', 'Qaxa3', 'R3b5', 'Rbf6+', 'R8xc5', 'Qcxf3+', 'Qaxf2+', 'Qfg3+', 'Qhxg5+', 'Rhxb5', 'Qfb5', 'Q1b6', 'Q6c7+', 'R1f5+', 'Q5e7+', 'Ncxa3', 'R7xf5', 'Rcf1+', 'Ndxe8', 'Rgxa7+', 'c1=R+', 'Rfxg8+', 'Rga4', 'Rfxg6+', 'Rgxe7', 'Qbf4+', 'Q3g4+', 'N4xf3', 'R7g2+', 'R1c5+', 'Rgf3+', 'Rgxd4', 'Ncb3+', 'Rbxc8+', 'Qdxe4+', 'Ndxc7', 'Qgb2', 'Rbxf3', 'Rhxa5+', 'Nde5+', 'Rdb7+', 'Qcf4+', 'R6xg5+', 'Qcg3+', 'Rhb1+', 'Rgxh2', 'Rad7+', 'R8xc7', 'Qde8+', 'Nac4+', 'R1xd6+', 'Raxa5+', 'Rfxe6+', 'R8h4+', 'Qfe5+', 'Qgxc7+', 'Qed6+', 'Qdxd4+', 'fxg8=N', 'fxe1=N+', 'Ned5+', 'Rfb4+', 'R5d3+', 'R4xa5', 'Nfxd5+', 'R3c5', 'Qha4+', 'Qeh2+', 'Rcxd8+', 'R6xd5+', 'e8=B+', 'Rhc2+', 'Rgxf1', 'R2b7+', 'Rdxb5', 'R8d4+', 'Qfb8+', 'Qgf8+', 'Rbd4+', 'R8xc2', 'Qgxc4', 'Qdxd2+', 'Rhxg1+', 'Rfxd8+', 'Qdxf5', 'R2a6', 'R2c4+', 'Rexa2', 'Rdxg3', 'N1xf3', 'Rcxc5+', 'R6a4', 'Nfd8', 'Rfxg4+', 'Rcxf6+', 'Rexf3+', 'Rbxf3+', 'R8xd2', 'R6xa7', 'Nec3+', 'Q8d6+', 'Qee5', 'Rfxa6', 'Rgb2+', 'Neg2', 'Rbxb1', 'Qhc3+', 'Q8c6+', 'Qed5', 'N7f5+', 'Qhh8', 'Qhe8+', 'Rae3+', 'Kb5+', 'R1xf4+', 'Reb4+', 'Naxb6', 'Rhc8+', 'R4e6+', 'Rcxb7+', 'Rah3', 'Rbxh7', 'Rhxh1', 'Rgb8+', 'Qdb1', 'Q1b7', 'Rhxd7+', 'Kh5+', 'R7xh5', 'Qhxg4+', 'R6xe4', 'R1xe7+', 'Qdc5+', 'Q6a4+', 'Raxh4', 'Nhxg7', 'Qge2+', 'Nbc4+', 'Ncxe7+', 'Ndc8', 'N6c5+', 'Rfxc4+', 'Ndc2+', 'Rgxg8+', 'R8xg6', 'Raxe2+', 'Rag4', 'e8=N', 'Nec6+', 'Rgxc6', 'Nef1', 'R2c6', 'Rgxb2', 'R2xc6', 'N3xd5+', 'R2xh3', 'R3e2+', 'Qdd3+', 'Rgxg8', 'bxa1=B', 'Rbxb7+', 'Rfxh7', 'Nexg7', 'Rcd7+', 'Q4h6+', 'Qcxd7+', 'Qbc7+', 'R8xe3+', 'Rhf3+', 'Kxh3+', 'Rhxg2', 'e8=R', 'Rhxd4', 'R4xe7', 'Rcxg8', 'Rhxb8', 'axb8=B', 'R5xd7', 'Qfd1', 'Qec2', 'Kb2+', 'Qhxh4+', 'Qec3+', 'Rbxb4+', 'Qfd5', 'N8xa6', 'Rcxe1+', 'Nexc1', 'Rfxd7+', 'Rcxe6+', 'R8xf6+', 'R5f4+', 'Qaf8+', 'Q5xh5+', 'Qce8', 'Kb3+', 'R6c5+', 'Ncxb4+', 'Naxc3+', 'Raxc8+', 'Raxe6', 'Ndxe6+', 'R2xb5', 'Nba2', 'Nfxg7', 'Kxf3+', 'R3c2+', 'N7d5+', 'Rexb6', 'N5h6', 'Rhxd3+', 'R1xf3+', 'Nhxf3+', 'Qcd6+', 'Rcxa4', 'R5a4+', 'R1a3+', 'Rcxb1+', 'Qde1+', 'Qge1+', 'Qfd2', 'Qaa4', 'Qcb6+', 'Qac5+', 'R7xg5', 'R7f4', 'R7e3+', 'Qch1+', 'Raxg7+', 'N6xg4', 'R8xc6', 'Raxb3+', 'Rae4+', 'Qgxg7+', 'N1b3', 'Qdxg6', 'Kxf7+', 'Qac1+', 'Naxc1', 'N6xf5', 'Rhxe7', 'b8=N+', 'Rcxb5+', 'Rdxg2', 'Nbxc7+', 'Q8e7+', 'R4xg3', 'Nhg2', 'Rhxg5+', 'Rfg5+', 'Rdxe6+', 'Raxb2+', 'Qga2', 'Rexb6+', 'R3e7+', 'R3xc5', 'Q8xc2+', 'R7h6+', 'Rhe7+', 'Raxf4', 'N5xg4', 'Rexb3+', 'Nca4+', 'R7h4', 'Rexg5+', 'Nbxa5', 'R2xh5', 'R5e6+', 'Rgxd6+', 'R5b2+', 'Rfg3+', 'Rdxh4', 'Qcf7+', 'Reb7+', 'R5xf4', 'Rgxf5+', 'hxg8=R+', 'Kxd6+', 'Rexg7+', 'R4g7+', 'Rbh4', 'Qhxh5', 'Q4h2+', 'Rdxd4+', 'Qde3+', 'Rgxh6+', 'Rhc1+', 'd8=R+', 'Nfh5+', 'R8xg5', 'Qaa2', 'R3d6+', 'Qae4+', 'Qed3+', 'Qfxg5+', 'Qhd8+', 'Rfxb1+', 'R7xd4', 'Qec7', 'Rhxf3+', 'Qhg8', 'Rab6+', 'Qac8+', 'Q8g4+', 'Rhd2+', 'Nca3+', 'Qhb7+', 'Qfd5+', 'Qdg6+', 'Qdg4+', 'R6xc7', 'Rhxg3', 'N4xd5+', 'R2xd4', 'R8h5+', 'N1a2', 'Nfe3+', 'R7xb5', 'Red3+', 'e8=N+', 'R4xd2', 'Rcxa1', 'Rexf7+', 'Raxe7+', 'Nfxh5+', 'd1=N+', 'Rhxd6+', 'Qhb8+', 'd8=B', 'R8f6+', 'R1f4+', 'R6f4+', 'Rdxc7+', 'R4b2', 'N7xb6', 'Rdg6+', 'Qdc2+', 'Qef5+', 'Q5f3+', 'R5xb3', 'Rexa4+', 'Qfa6', 'Rcf3+', 'N8e6', 'Nfh3+', 'R7h2+', 'Rfxe2+', 'Rhxg3+', 'Qbb8+', 'Naxb8', 'R2h4', 'Rbg5+', 'Rdxf8+', 'R6xd2', 'R8xa6', 'Rfxf5+', 'Qeh5+', 'Q4g4+', 'R6h4', 'Rhc4+', 'Qah8+', 'Q4h7+', 'R4xd5', 'N3xf4', 'Qhxg8', 'Raxc5+', 'R1h3+', 'Qdxg5+', 'Qea7+', 'Qbh2+', 'N4e5+', 'N6g4+', 'R2c3+', 'Qdxf3+', 'R4e3+', 'Rgxc4', 'R1xc2+', 'Nexc2+', 'Rhb4', 'Qfe4', 'Qfxf2+', 'Qaa7', 'Rdxa7', 'R2b6', 'Nbc5+', 'Rab4+', 'Ngxe5+', 'Rgxa3', 'Rdb3+', 'fxe8=N+', 'R4f6+', 'Qe8e6+', 'Q3b6+', 'R4xh5', 'Ncxe8', 'R7xc6+', 'Rdd7+', 'Qexd8+', 'Qdxc7+', 'R7f2+', 'R8c6+', 'Rag5+', 'R7c4+', 'N1xd3', 'Ncxe4+', 'Rgxa4', 'N6a7', 'Raxa7+', 'R1xa2', 'R1f7', 'N1g3', 'N8xe7', 'Q7c5+', 'Qff1', 'Rgc2+', 'Rhxb7', 'Rbd7+', 'Q1d7+', 'Qfxf6+', 'Qgg5', 'Qgxf4+', 'N7a6', 'Nba7', 'R2d6+', 'R5xe6', 'Neg6+', 'Rcd5+', 'Nbxa2+', 'R7e2+', 'Qgxc5', 'Q1c4+', 'Q1c3', 'Raxb5+', 'R8c5+', 'N4b6', 'Rbxc6+', 'R5xh4', 'Qbc2', 'Rgxh6', 'R5g2+', 'Q1b2+', 'Rcxb4', 'Ke1+', 'Qce5', 'R6xg4', 'N4e3', 'Qhg7', 'Qba7', 'Ka4+', 'Qeh4+', 'R7d4+', 'R3a5', 'Rfxe4+', 'Raxa6+', 'Rfxa1', 'Rfxg7+', 'exd8=B', 'Nhg3+', 'N4g6', 'Rhxd2+', 'Qbxd1+', 'N2e3', 'Qhxe4', 'Kxd3+', 'Qcc4+', 'Rhxd7', 'Q7g5+', 'Qfg4+', 'Qde7', 'Qhh3+', 'Qdc6+', 'Rdxh6+', 'Qbc8', 'Rhxb3', 'Rhxg7', 'R8xf7+', 'Qef3', 'Q7f5+', 'R8b3+', 'N2a3', 'R5xa7', 'N8f7', 'Nbd2+', 'Nfxh3+', 'Rdxg4', 'Rgxd3+', 'Rgb3', 'R8xa5', 'Ncxb7', 'Qcc3', 'R7xc2', 'R3f7+', 'R3g7', 'Rhd5+', 'Kxe3+', 'Kxa6+', 'Bxa8+', 'Rcxh4', 'Qhxg2+', 'Rdb2+', 'Rcxe2+', 'Rah4', 'Qcc6', 'Q8c5+', 'R4e5+', 'R4xf7', 'R4xb5', 'Qhxe5+', 'Qdxc5+', 'Qca7+', 'Q8xa7+', 'Q4a6+', 'Rfb5', 'Nbc6+', 'N5f6+', 'N2f3+', 'Raxa1', 'Rhxg4+', 'Qexe7', 'Raxd6+', 'Qfe4+', 'R2h5+', 'Raxh3', 'R7xc3', 'R1xc3+', 'Ndb4+', 'Nab5+', 'Nca2', 'Raxf5+', 'N2xg3', 'Nexf1', 'R5xd4', 'Rdxe2+', 'Raxd2+', 'Rha7', 'Kxc3+', 'Qexg3+', 'Rbxg2', 'R6xb2', 'N6xd4', 'Ngh3+', 'Qhd7', 'Qcb8+', 'Qac7+', 'Qdd6', 'Qce7+', 'Qeg7', 'Qhf6', 'Qec5+', 'a8=R+', 'gxf8=N', 'R6h2+', 'Rexe1+', 'Rdc8+', 'Nfxh6+', 'R2h7', 'R5e4+', 'R3f2+', 'h8=B+', 'R2h5', 'Qgd7+', 'R6xa3', 'R4xe3', 'R1xe2+', 'R2xb7', 'Kb8+', 'Q1d5+', 'Q8xg5+', 'Rde6+', 'Rexh6+', 'R6xc5', 'Qbe3+', 'N3b4+', 'Qdxd7', 'Rbg4+', 'R6f7+', 'R6d4+', 'e1=B+', 'Qad3', 'Qcd1+', 'Q1d2+', 'Rae2+', 'Rce3+', 'Q8xc3+', 'Qcxc2', 'N2xd4+', 'f1=R+', 'Rae6+', 'Naxc8', 'Qga3+', 'R7g4+', 'Q1xd5+', 'N4g3+', 'Rfxa5', 'Qhf1', 'Qef2', 'Q2g2+', 'R1b5+', 'Rfxb7+', 'Qgc2+', 'Qab3', 'Rhxc5', 'Qgxg7', 'Qfh6+', 'c8=B+', 'Rexb7+', 'Qgg3+', 'R2b3+', 'Ncxb8', 'Rdxg1', 'Nexd4+', 'Qaf4+', 'Qdxd6', 'R5xd3', 'R5h2', 'Rfxb6', 'Qhd1+', 'dxe8=N+', 'Rcxf3', 'Bexc6', 'Qec7+', 'Rch6', 'Rag6', 'Ndxb6', 'R6d2+', 'Q1d5', 'cxd1=N', 'Qcxe6', 'exf1=R', 'Rcxh7', 'R4c5+', 'N5xh4', 'Qab2', 'Q1c1+', 'Q8d4+', 'Rcxb1', 'Rgxf1+', 'Qcxe5', 'Qfd2+', 'N7e6+', 'Rgxh3', 'R1f3+', 'Rbg6', 'Rfxe5+', 'Qag8+', 'R3f5+', 'gxh1=R+', 'Rcxh6+', 'Qaf3+', 'Q1g2', 'axb8=B+', 'R1h7', 'Ncxd5+', 'N1b2', 'Rdb5+', 'Ncxe6+', 'Rcxa7+', 'Rexd7+', 'Q4e6+', 'Q8f7+', 'Qhd8', 'N7h6', 'R7xf6+', 'Qdg1+', 'N8e7+', 'R8f5+', 'Q4b2', 'Qaxb1', 'R2e7+', 'Q8xg4+', 'Qcd4+', 'Qgc8', 'R8g3+', 'Q7e6+', 'Rgxe2+', 'R5b3+', 'gxf1=N', 'Rgxc5', 'Rhxa3', 'R7xe3', 'Rbxg6+', 'Qcf8+', 'Nfxd1', 'Qga8+', 'Qgh8', 'N5a6', 'Qdh8+', 'Rgxc8', 'Qhxc2', 'Qah7+', 'R2g4+', 'R4g6+', 'Rhxa5', 'Q4c6+', 'Qaxc1+', 'Q5xb4+', 'Kxe4+', 'R5a3', 'Nfe8+', 'R7f5+', 'Rgf5+', 'Nfd2+', 'Rcd4+', 'Qhd4', 'Qcf8', 'Rab3+', 'Q3g2', 'Q2h2', 'Qgg2', 'Qhd2+', 'Qhh8+', 'R6xd7+', 'Rhe4+', 'N6xb4', 'Rbxe4+', 'Qbf6+', 'Q8h6+', 'Rgxd4+', 'Rfxa7', 'Qaxc8+', 'R4xb6', 'Q3xf5+', 'Rac7+', 'N4xb6', 'R8d5+', 'Q2b3+', 'Qcxf7', 'R6b2', 'Rha6', 'R2c7+', 'Rgxb1', 'Qac4+', 'Rhxg6', 'Qef8+', 'Qgg1+', 'Kxh6+', 'Rdc5+', 'Qfb4', 'Qee7', 'Qbb8', 'N1xd3+', 'R2xg3', 'Qeb1+', 'Q1b3+', 'Qhd5+', 'Q5d2+', 'Rbxc1+', 'Q8f4+', 'Q3xg4+', 'R6h7', 'gxh8=N+', 'R6a5+', 'Qdc7', 'Qfc3', 'Rdxg6+', 'Q3f2', 'Qeh2', 'Qhg1', 'Q1h2', 'Qgg1', 'Qhg2', 'Qah4+', 'Ndb6+', 'Qhh1+', 'Ned4+', 'Raf6+', 'Nge1', 'R3b7+', 'Qaxa5+', 'Qdxf7', 'R5xg7', 'R7g5+', 'Qdxf1+', 'Qhd6', 'Qfe7+', 'Qaxg7+', 'Qad8+', 'R3xa5', 'Qac3', 'Qdxd5+', 'Qce1', 'Q1xa5', 'fxe8=R+', 'Qge2', 'N6e5+', 'Qaxb7', 'R3h5', 'Ngxe7+', 'Rbf3+', 'Rgb1+', 'N7xg6+', 'Rcxa6', 'R8xe6+', 'Rexg3', 'N4e3+', 'R4a3+', 'd1=B+', 'Qfc5+', 'R6e4+', 'exd1=R+', 'Ncxa5', 'N4xf5+', 'Qfxg3+', 'R3xg6', 'R6g7+', 'g8=N+', 'N1f2+', 'Qef3+', 'N7xa6', 'Qhh7', 'Rhxf2+', 'R6xg5', 'N3d4+', 'Qaf3', 'Qdf3', 'Qgf3', 'N4b5+', 'Qgxe6+', 'Ree1+', 'Rfa5', 'Qaa3+', 'Qaa7+', 'Rgxf6+', 'Q1e7+', 'Rdg5+', 'Rdxf4+', 'Qbf8', 'Qha8', 'Qaf8', 'Qff6', 'Qhf8', 'Q1a7', 'Qae7', 'Qfg1+', 'N8xa7', 'Rgxf3+', 'N3g4+', 'f8=B', 'Rgxa2', 'Q4e8+', 'Naxb4+', 'Nfxh2', 'Rhxb4', 'R3b5+', 'Qhg8+', 'Qbg1+', 'Qcf2', 'Qgf2+', 'Q1g1+', 'Nba2+', 'gxf1=R+', 'R7b2', 'Ncxb1', 'Ndxc2+', 'Ref5+', 'Qed8', 'Qdc8', 'Qhxg1+', 'Q1xf1+', 'R1xg5', 'R7xb4', 'R3xe6', 'Rge3+', 'R1xd7', 'Nfxd7+', 'Ncd8+', 'Rgxc1', 'Qdf4', 'R3xb4+', 'a8=B+', 'd8=B+', 'Rbxf2+', 'Qhh3', 'Rdxg1+', 'Qcd2+', 'Qcxc3+', 'Rfxe8+', 'Qhh6', 'Qhf2+', 'R7a4+', 'Qad4', 'Naxc8+', 'Qcg7+', 'N6g4', 'Rfxh2+', 'Raxg7', 'Qhxg6', 'Ngxh2+', 'Ngf8+', 'Rfxg6', 'Raxh7', 'Rgxb5', 'Kxd4+', 'R3xc2', 'Rgxe3+', 'gxh1=N+', 'R6xa4', 'Qbh7+', 'R4xg5', 'Qch8+', 'Kh1+', 'Rbxc2+', 'Nfh6+', 'N8b6', 'Q5d4', 'Rhxf5+', 'Qbxd4+', 'Qge3+', 'Qgf3+', 'Qbg8+', 'R4xf2', 'Kc8+', 'Qhc6+', 'Rhd3+', 'Rhxb6+', 'Q7f7', 'Rbxf4', 'Raxb8+', 'Qaf7', 'R7a4', 'R5g4+', 'Rdb8+', 'R2xf7', 'Qhxh7', 'Rgxe2', 'Nge2+', 'Ba1+', 'Qfxf8+', 'Q3c6+', 'Qaf4', 'Qcxb2+', 'Ndc6+', 'Qeb4+', 'Qcxf2+', 'Qhxf3+', 'Q3e5+', 'Qge7+', 'Q5c7+', 'Kxb8+', 'Qgc4+', 'Qab6', 'Rdg4+', 'R4xf6', 'Q8e3+', 'Rbxf5', 'Rdxg7+', 'Qdxc6+', 'R2d7+', 'R7xc5', 'N2d3+', 'R8xb2', 'Ncb2', 'Nhxg8', 'Qgb8+', 'N3xh4', 'Qdh5+', 'Q6d5', 'Rdxe3+', 'N2d3', 'Rexh4', 'Q8c4+', 'Rce4+', 'Qah1+', 'R5xf7', 'Rgb4', 'Nec8+', 'Nfxd8', 'Qfb7', 'Q1f7+', 'Rdb6+', 'R7h5', 'Qdh2+', 'R4c2+', 'Rhxf8+', 'Rdxe1+', 'R2xf4', 'Qdd7', 'Rhxa4', 'Rfxd6+', 'Q8c4', 'Q4c3+', 'dxc8=N+', 'Rhxb1', 'Rcg5+', 'R5xf3+', 'Qfc8', 'Qdc8+', 'Qcxc7+', 'Qbe8', 'fxg8=R', 'Qbc5+', 'R2xd3+', 'Rbxf1', 'Ngxe2+', 'R5xa4', 'Qfd3', 'Q1c2+', 'Qgg3', 'Qgg5+', 'Ngxh3+', 'Qfb1+', 'Q6c2+', 'Kxg4+', 'R3a2+', 'Raf3+', 'Qbxe1+', 'Q5a4+', 'R3xb7', 'Qgxg1+', 'N7b5+', 'Q3e1', 'Qgc1+', 'Q8d5+', 'Nfxh6', 'N6xb5', 'Qfc6', 'Qdxd1', 'Rad4+', 'Qfa6+', 'Qbe6+', 'N8xg7', 'Qgc4', 'Q3xb3+', 'Qda7+', 'Qfxg6+', 'Q8b7', 'Rbxh2+', 'R7xd4+', 'Rff1+', 'Qcxd5+', 'Qad7+', 'Qcxc5', 'bxa8=B', 'Qhxf8', 'Qae8', 'R4xc6', 'R5xc6', 'R7xb4+', 'Qbxh2', 'Qaf1', 'Qbe2', 'R6b4+', 'dxe1=R+', 'Rcc1+', 'R2c5+', 'Rgc1+', 'Qdc3+', 'Q1xa3+', 'f1=N', 'R4h3+', 'R7xe2', 'Qee1', 'Nba8', 'N3xd2', 'Qbb6', 'Qfxf7+', 'Rgg6+', 'N7f6+', 'Rhxc2', 'Qdf7+', 'N5g3', 'R4xc7', 'Qfxc6+', 'Q6d6+', 'Qcxc8+', 'Qaa6', 'R8h6+', 'Nba7+', 'Qbd4', 'Nexf5+', 'Q6xe2+', 'Q2d2+', 'R7xa6+', 'R7xa2', 'R7xe5+', 'R4a6', 'R3xa2', 'Qexg6+', 'Nfh7', 'Nfxg1+', 'R6e3+', 'N4xe6', 'R3h7', 'R4f3+', 'Qcc1', 'Naxc6+', 'R8b6+', 'Rcg6+', 'Rah5', 'Rexd5+', 'Qch7+', 'N4b3+', 'Rde4+', 'R8xg2+', 'Rge4+', 'Q8xf6', 'Qfxf3', 'Qfxg2+', 'Q2xf3+', 'R6xe5+', 'Q8d8+', 'Rgxe6+', 'N7xh6+', 'Qbc1', 'Rdb4+', 'Rcb4+', 'Qac8', 'Qbd7', 'Qdf7', 'Rfb5+', 'Qeb8+', 'Qfg5+', 'N4xa6', 'Rexg8', 'Qed1', 'Q2e2', 'Qdxf8+', 'Qcc5', 'N4xg3', 'Q5g2+', 'Qaf7+', 'Ndxc3+', 'Rdxf2+', 'R4f2+', 'N5g4+', 'N5b6+', 'Q7g5', 'Qhg6', 'Qge8+', 'Qbxd7+', 'Qcb2', 'R7h3+', 'N1g2', 'R1xb6', 'Qdxg7+', 'R7a2+', 'Qdxf7+', 'N3a2', 'N5xf4+', 'Qfa3+', 'Rdxe4+', 'R8xa7', 'R5xc2', 'Qexe6+', 'Qfxd5+', 'Qdxg5', 'Qfc2+', 'Raxd3+', 'R8xg3', 'Qbxh6', 'Rbxe2+', 'R2xb3+', 'R8xh7', 'Qeb6+', 'Nca6+', 'Q6xg5', 'Q1g4', 'Qexf2+', 'Rexh6', 'Qbxc7', 'Rfxa2', 'Rbxd4+', 'Qfc7', 'Qhxg7+', 'R8g5+', 'Ngxh7+', 'Qaxf3', 'Q1xd7', 'Q8xc8+', 'Nfxe1', 'Qaxf6', 'Rdxh1', 'Kxf5+', 'Qcb2+', 'R3g7+', 'Ngxh4', 'R5xe3', 'Rhf5+', 'Rbxh5', 'Rfxa8', 'Rdxf1+', 'Ngh6+', 'R2b6+', 'R2xe6', 'R2xb4+', 'R8xb4', 'Qbc3+', 'Qbb2', 'Qaa1+', 'Nbxa6', 'N2h3', 'Rexb5', 'R8b5+', 'Rbxf5+', 'R2f4+', 'N5d3', 'N5xa6', 'R3xd5', 'Rff8+', 'Rhxb2', 'Qhf7+', 'Qfxa3+', 'R4xd7+', 'R6h5+', 'N3xh2', 'Qff8', 'Qgf8', 'Qgg8', 'Q7h7', 'Qcg2+', 'Ree5+', 'Qhe2', 'Qfh2', 'Qgh1', 'R7xf3', 'Kxc5+', 'Ndxe2+', 'Nfxe6+', 'Qge3', 'Qeb3+', 'Raxc7+', 'Qdg6', 'Qgxb6', 'R3xe5', 'R1xg4', 'Nec2+', 'Qexh4+', 'Qdh4', 'N7h6+', 'bxc8=N+', 'Qcc8', 'Qfb1', 'Q1b4+', 'Raxc1+', 'R5d4+', 'N3xg2', 'Rdxd1+', 'Qgxc2+', 'Reb6+', 'N8xd6', 'Qfh7', 'Rgxc2+', 'Q5e3+', 'Q3g3+', 'Qce8+', 'Rch4', 'Qcf2+', 'Qgxh8', 'Rdxb8', 'Rhxc6', 'Rhxb4+', 'Kb1+', 'Qcd3', 'Rfxg8', 'R1xe5+', 'Qab6+', 'Qfd8+', 'Rhe6+', 'Rfxh3', 'Rbxh4', 'Qad3+', 'N2b3+', 'd8=N+', 'Qcb3+', 'N2h4', 'R3xh5', 'N4xb5', 'R8xh5+', 'Qdd5', 'Rcxe1', 'Raxg6', 'Rdxg3+', 'Qhe6', 'Qfe8+', 'Qcf1+', 'Qfxc5', 'R6a3', 'Qfe6', 'Qdxb7+', 'Ka2+', 'R7b5+', 'Qae3+', 'Rbxc7+', 'R2e6+', 'Nfxg5+', 'Nexc8', 'Qcc7', 'R5xb4', 'Qdxe2+', 'R7a5+', 'Q8xa7', 'R6b2+', 'Qfxb7+', 'Qfb2+', 'R5xd6', 'Q4b3+', 'Rbc5+', 'Rexd6+', 'R1xb4+', 'R1c4+', 'Rhxa7', 'Q7xh7+', 'fxg8=N+', 'N5xh6+', 'R6xa5', 'Qexe7+', 'Qcb1', 'a1=N+', 'Kxg3+', 'Rbxg4', 'R6a2+', 'Q1c3+', 'Qhc8+', 'Qcf6+', 'R1xf6+', 'Rhxb7+', 'N4xd6', 'N3c5+', 'Ngxe4+', 'N3xb2', 'N7g6+', 'Qgc7', 'R5f7+', 'Naxc5+', 'R3a7', 'Rff2+', 'Qhe7+', 'Ncxd2+', 'Rhxa2', 'R2xe7', 'bxa1=N+', 'Qhxd5+', 'Qfxd1+', 'Qdg7+', 'Rbxa5+', 'N6c4+', 'Qhxg2', 'Q5g6+', 'Rcxh2+', 'N1d2+', 'Nde1+', 'Ngxf1', 'R8xh4', 'Qea8+', 'Qexb2+', 'R4xh6', 'Q1f4+', 'Rfxh6+', 'N4g5+', 'Rfxc2+', 'Rexc1+', 'R1xe6+', 'Q8a8+', 'Qgb7+', 'Qhg1+', 'Q6b6+', 'Rcxg1', 'N5b3+', 'N7xg5', 'Rgxe7+', 'Nhxg2+', 'R1xg3', 'N3xb5', 'Nexd1', 'R1xg2+', 'R1xa5', 'N8xb7', 'N3xg5', 'Nbxa7+', 'Rcd3+', 'Rac5+', 'Rgd5+', 'Q6e6+', 'Rhxc7', 'Qhc1+', 'Q8xe4', 'Qac4', 'Qcd5+', 'Rfxd3+', 'R5xa2', 'Qdxe6+', 'Q8f7', 'Q5xg6+', 'Qhxg6+', 'Rhc3+', 'R6c4+', 'Rcxg4', 'N3xb4', 'cxd1=N+', 'fxe1=B', 'Rhe3+', 'Qexb5+', 'R8xe2', 'Q3d5+', 'Q8c3+', 'Qhd3+', 'Qgg4', 'Rcxb6+', 'R6b5+', 'Nbxa5+', 'Qhxb7+', 'Rce6+', 'Rgxc2', 'Nhxf2+', 'N4xb3', 'Rbxd5+', 'R8xh3', 'Qfxc5+', 'Q5xe5+', 'Qgb3+', 'Naxb3+', 'Rhb6+', 'Raxa4+', 'Qcxf7+', 'Q5f6+', 'Kxf1+', 'Raxh7+', 'N4xg6', 'Qgxd8+', 'R4xd7', 'Q8b6+', 'Q6e3+', 'Q6c6+', 'Nhxg3+', 'R1xh4', 'Rbc4+', 'Kxg6+', 'Ncxa7+', 'Qdxe7', 'N8xf6', 'cxd8=N', 'Q1xg2+', 'N6b5+', 'Qgxg8+', 'Rbh3', 'Kxb6+', 'N5xh6', 'Raxg3', 'Q1xe3+', 'Qcd4', 'Qch1', 'Rfxh7+', 'Kxe5+', 'g8=B+', 'R1xa5+', 'R5xa3', 'Rbxd3+', 'N6c7', 'Qbd3+', 'Qfxd6+', 'Qbxc4+', 'Qdxh2+', 'Rhxa1', 'Rhxf6+', 'Qgxf8', 'Qhxe6+', 'Rcxb3+', 'Qfg1', 'Qgh2+', 'N2h4+', 'Qbe4', 'Qexf5+', 'Kxa5+', 'R3xg2', 'R2xh6', 'Qec6', 'Qdb5', 'fxg8=R+', 'Kxe2+', 'g1=B', 'Qcg8+', 'Rcxe5+', 'Qexc3+', 'Qbe8+', 'Rbxa7+', 'R3g6+', 'Q8e5+', 'Qfxg4+', 'Ncxe2+', 'Rbe5+', 'Q7d6+', 'Qhxg7', 'Q7c4+', 'Qgc5+', 'Ngxf5+', 'N7xb5', 'Qcxf1+', 'Nhg2+', 'N1g3+', 'Qgd6+', 'Qfc4', 'Ncxe1', 'R2xd6+', 'Q6e8+', 'Qdh7+', 'N4h3', 'Q1f2+', 'N5xb6', 'Qab4+', 'N3f5+', 'Q8b7+', 'Qca5', 'Q5xc3', 'Nfxe3+', 'Ncxb5+', 'Qdxg6+', 'Rdxh5', 'R2xg5', 'Rexc5+', 'Rhxb6', 'R5e2+', 'Rcxd1+', 'Qdb2', 'Rgxh4', 'Rcxh3', 'Q2g8+', 'Rha4', 'Rdxh7+', 'R6xg7', 'R5e7+', 'Qgh8+', 'Q8xf5+', 'Q8d4', 'Ncxb2', 'Qgh6', 'Qhd1', 'e8=B', 'R3xf5', 'R6xe3', 'Raxh5', 'Nbxc1', 'N5b4+', 'Qdh6', 'R8xf3+', 'Ngxh3', 'Nfxh7+', 'Q3g2+', 'Qfh4', 'R1h4+', 'Qga1', 'c1=B', 'Rexc6+', 'Rfxd4+', 'Qdh2', 'R8xb2+', 'Qbf6', 'Q5d6+', 'Q8b8+', 'Qfc1+', 'Qcxg2+', 'R2xc3+', 'R1xh3+', 'Qah3+', 'Qdxe8+', 'R3xf6+', 'R1xa2+', 'Qed3', 'Rfxg2+', 'N6xg5', 'Qgb8', 'Qexf6+', 'R7xb3', 'Rbxc4+', 'Qcf7', 'R2e5+', 'N2g3+', 'Qgxa2+', 'Ncxa4+', 'N2a4+', 'R8xc2+', 'Ndxe4+', 'N7xh6', 'N5e3', 'Ndc5+', 'Qhxc2+', 'Nfxh8', 'Rbxb8+', 'Qhxh5+', 'R5h3+', 'N1xc3', 'b1=B+', 'R5xc3', 'R4xe2', 'Nbxa8', 'Qcg1', 'R8xg6+', 'R1xf2+', 'Rhxf4+', 'R5b7+', 'Qab1+', 'N7xc5', 'Rcxg3+', 'R4f7+', 'Q5xd3+', 'R6a4+', 'Q8f6+', 'f1=B', 'Qexd2', 'Nfe1+', 'Rfxf4+', 'Rexg5', 'R4xa7', 'Q2xe3+', 'R3g4+', 'Rbf4+', 'Rbg6+', 'Q8xf8+', 'R8a6+', 'N7xh5', 'cxb1=R', 'Qaxa3+', 'Rdxb3+', 'dxe1=N', 'N6f7', 'Q1h3', 'Q4g3', 'N4c5+', 'R2xh4', 'Qhe5', 'Rdxc2+', 'Nfxg8', 'Raf5+', 'Reg8+', 'Q6d5+', 'Rbc3+', 'Rcxh5', 'Q6xf4', 'Q4g3+', 'Q5b3+', 'Raxf2+', 'Qcxc1+', 'Nhg7+', 'Nca7+', 'Ree7+', 'R5g6+', 'Qbg1', 'Nbxc6+', 'Rgxd2+', 'Q1xd2+', 'Qdb6', 'Rcxb4+', 'Qaf2+', 'Q7g6+', 'Raxb4+', 'Qgxf6+', 'Q7d7+', 'Q4b5+', 'Nexf4+', 'Qfxd8', 'Qgh3+', 'Qexf4+', 'Qad2+', 'Q1e2+', 'Rbh6', 'Rcb6+', 'N4xe3', 'Rfxg3+', 'Rfb6+', 'Q5e1', 'Qbxf7+', 'Nexc3+', 'Nexg1', 'cxb8=B', 'Qhxe4+', 'Ndc8+', 'f1=B+', 'R1xg4+', 'Rcxh3+', 'Rdxb1', 'N2f4+', 'Q5e4+', 'Q4f3+', 'Qaxe2+', 'Q6c4+', 'Qfxf4+', 'N5e4+', 'Rcb5+', 'Rfxa2+', 'R5a2+', 'Rfxg1', 'Q1d4+', 'Raxh6+', 'N8xh6', 'R7xg5+', 'Rhxb2+', 'Rgd3+', 'R4b3+', 'Qae7+', 'Nhxg4+', 'Qae6', 'R1a5+', 'Rcxf3+', 'Qfh3', 'Qbxc6', 'R7b3+', 'Q6c3+', 'Qbf8+', 'Qbxb5', 'N8xb6', 'Qfxc7', 'Q6c7', 'Q5c2+', 'R8xf2', 'N5xe3', 'Qgxc3+', 'R2c6+', 'Rhxc3+', 'R3c7+', 'Qdc3', 'Qbg7+', 'Qcd2', 'N7h5', 'Rbxd2+', 'Qdxb3+', 'Qec3', 'Qdb4', 'Kxb5+', 'Qeb1', 'Rbe4+', 'Qaxd7+', 'Raxf8+', 'Qbxb6+', 'Qag5+', 'Qad8', 'Qha5', 'Qaa8', 'Qbxc2', 'Qcxa4+', 'Qee4', 'Q1a4', 'Qag2+', 'fxe1=R+', 'R5xe7', 'Qhxh7+', 'Qcg6', 'Q6f7', 'R1xg6', 'Rdxa2+', 'Bbd3', 'Qce2', 'R4xc3', 'Ree8+', 'Q2h4+', 'Q4d8+', 'Qexg7+', 'Qcxd3+', 'Ndxb8', 'Qexd4', 'R3a5+', 'Qhxf6+', 'Qbf5', 'Rdxh6', 'Rhxg6+', 'Qdf5', 'Qdf6', 'Rgxb3', 'Qcd7', 'R4xa3', 'Q7d2+', 'Q1xe2+', 'Qed1+', 'N6xa7', 'Q8d3+', 'Qdf1', 'Qde4+', 'N4xc5', 'Rbxc3+', 'N6xd4+', 'R2a4+', 'Q4f7+', 'cxb8=R', 'Qad6', 'Qhxg5', 'bxc8=N', 'N5xb4', 'Ned1+', 'Nfd8+', 'Nexd7+', 'Qaxd8+', 'Rhf4+', 'Qaxe8+', 'N4xa5', 'Raxd4+', 'Q4f6+', 'Qgb6', 'Qgh6+', 'Rgxa7', 'Q1xc3+', 'Qbf2+', 'Q3b5+', 'Qbc6+', 'Qhxd7+', 'Rhxe5+', 'Qhf7', 'Qda3+', 'N2xg3+', 'R4xb3', 'R2xb5+', 'Ndxb6+', 'Qge5+', 'R4xa2', 'R7xf2', 'Qcxc4', 'Rgd6+', 'Qbxg6+', 'Q1c2', 'Q3d2', 'R3xa7', 'Rfxe7+', 'Rdxh2+', 'Q3c5+', 'Q8f5+', 'Qcf3+', 'Q2g2', 'Qbh8+', 'R3xe4+', 'R1xc7', 'Rgxa6', 'f8=B+', 'Qfxc4+', 'R8xc7+', 'Q5h3+', 'Qbf4', 'Qbf7', 'Ndxe3+', 'Qgc3', 'Raxe4+', 'Qexg4+', 'R8xd2+', 'Qdxb2+', 'R2xh3+', 'Nfh1', 'Q1e4+', 'Q4c4+', 'Q1xh2', 'Qfg2', 'Q1h2+', 'Rbxg1+', 'Q3a4+', 'R4xd6', 'Raxe3+', 'Nbd8', 'Qcxg7', 'R3xf5+', 'Qaxa7', 'Qgd3+', 'Qff5', 'Qfe8', 'Qfxf7', 'Qah3', 'Q2f2+', 'exf1=N+', 'Rbxg5+', 'Qge4', 'Qef5', 'R7xb6+', 'N7a5+', 'N3xg5+', 'Qcxd8+', 'N2e3+', 'Qfg8+', 'cxb8=N', 'R8xa2', 'Qbc4+', 'Qbh4+', 'Qexc6', 'Qhc7+', 'Rfxc6+', 'R4xb7+', 'Rbxg8', 'Qfxd3', 'Q6b7+', 'Qfxh1', 'Qgxe5+', 'R1xg5+', 'R3c5+', 'Nbxa3', 'Qga7', 'R5xh3', 'R5xh6', 'Qgf6+', 'Qcxa6', 'R4b5+', 'N6xf4', 'Q6d4+', 'Qbxd3+', 'Qdxa5+', 'R6xh3', 'Qgb1+', 'Q5f4+', 'Qexe2', 'Rbxf4+', 'Qhf2', 'Qfe1+', 'Nfxg6+', 'Rfc6+', 'R5xd2', 'Rexd3+', 'Qhe4', 'Ngh5+', 'Raxf1+', 'N4xh5', 'Rhxa6', 'Q3c7+', 'Q8xg6+', 'R4a7+', 'Q4xf6+', 'Nbxa3+', 'Q8xf6+', 'Qcxh7+', 'Rexc8+', 'Rhc6+', 'Nfh2+', 'Qfa7+', 'Qhh1', 'Qcxg4+', 'Qaxe4+', 'Q4e4+', 'b8=B+', 'Rbd6+', 'Qcg4+', 'R4xg2', 'Rge6+', 'Neg8+', 'R3a7+', 'Rdxc8+', 'R6xc3', 'R2xe4+', 'Qga1+', 'N4xf6', 'R6xh5', 'Q3h4', 'Rgxd5+', 'Qaxe7', 'Raxg5+', 'R6g3+', 'R2xf5', 'R1xg6+', 'R5c3+', 'Nbd8+', 'Q8g5+', 'Q8g6+', 'R5h4+', 'Qdxd5', 'Rdxc6+', 'N3a5', 'Qaxe4', 'Kxd1+', 'Qexd7', 'N8g7+', 'Qhxc8', 'Rbxe8+', 'Qdd8', 'Qcxg5+', 'Q7xd7+', 'R6xa2', 'Qed2', 'R3xc2+', 'R2g6+', 'Qhc3', 'Q3d3', 'fxe8=N', 'Rad5+', 'Qah8', 'Q3h5+', 'R4d7+', 'R4h5+', 'N5xh3', 'R6xe3+', 'N7b6+', 'Qaxb4', 'Qbd2', 'Qexa5+', 'Qcxa6+', 'Qhe3+', 'R5g7+', 'Qab1', 'Qhf5', 'Naxc2+', 'Qaxd6+', 'Ned8+', 'N8xe6', 'Qfh5', 'R4xe6', 'Q2c4+', 'Nfh8', 'exd1=R', 'Rfxb2+', 'R8xh7+', 'R2d5+', 'Qhxg8+', 'R4xg6', 'Qhb8', 'Qdxb4+', 'Rdxb2+', 'fxg8=B', 'R5c2+', 'Qgb1', 'Qbd6', 'N8xc7', 'Nfxh4+', 'Q8f8+', 'Qdxf6+', 'N1c3+', 'Rfxa3', 'Nfg8+', 'N7xe6+', 'R8xe2+', 'Rbf5+', 'Qcxg7+', 'N3xf2', 'Nexc6+', 'Qfxb4', 'Q3f6+', 'Qaxe1+', 'Nhxg1', 'Q8xe8+', 'Rdxg5', 'Rbxd1+', 'Rfxb6+', 'R3g5+', 'R5xa6', 'Rdd8+', 'R1xh7', 'R1xc6+', 'R6d3+', 'gxh1=N', 'Qexc7', 'R1xh7+', 'Qaxe5', 'Qcxc5+', 'Qha1+', 'Qba3+', 'Q3e5', 'N1f3+', 'Rbxh3', 'R2xg7', 'Rbxb1+', 'Qch2+', 'N4h5+', 'Qexc7+', 'R2xg6+', 'Qec5', 'Rhxd4+', 'N4f6+', 'R3xh2', 'Rcxe3+', 'N4xc6', 'N4a3', 'Rcxf8+', 'Q4xe3', 'Ndxc1', 'Qcxg5', 'Rexc7+', 'Nfxh2+', 'Rbf8+', 'Rfe4+', 'R7xe4+', 'Qfxc8+', 'Rhxc1+', 'Raxf6+', 'Q5xa2', 'R7xd2', 'Qcf1', 'Q2g1+', 'Qgh1+', 'exd1=N', 'fxe8=B+', 'Qhb3+', 'Qfg5', 'Q4g5', 'Q7g6', 'Q7h6+', 'N2xd3+', 'R5xd6+', 'Kxa2+', 'axb8=R+', 'Bhe5', 'Bhb2', 'Qbc7', 'Qgb3', 'Rexg8+', 'Rcxf5+', 'Qfxa6', 'Q7a5+', 'Qhxf1+', 'Qexe4', 'R6xg3', 'Qfxe5+', 'Qhxe8+', 'R2xf6', 'Qcxe2+', 'R4d5+', 'Q3f4+', 'd1=B', 'Q8xe6', 'N3xh5', 'Qff2', 'R6xf3+', 'Qda6+', 'Qbd1', 'Qda1+', 'N4xa3', 'Qcxe5+', 'Qbxe5+', 'N3h5', 'Q8d2+', 'N5d3+', 'R6xe7', 'Q1b2', 'Q2a3+', 'Ndxc8+', 'R5xf2', 'N6xa4', 'Qag4+', 'cxb1=R+', 'R2xd7', 'R3xe7', 'Rcxg6', 'Qaxa4', 'bxc1=R+', 'Qga2+', 'Qaxd5', 'R7xg3', 'e1=B', 'Rce5+', 'exf1=B', 'Q6d2+', 'Rfxc1+', 'Q5c3+', 'Nbxd7+', 'Q6g5+', 'Qfxd2+', 'Qbc3', 'Qhb1', 'R3a4+', 'R4h2+', 'R5xb2', 'R4xf5+', 'Rfd4+', 'Qce4', 'Qdxf2+', 'Rdc4+', 'N5a6+', 'N3xa4+', 'Qbxg8+', 'Raxh6', 'R3b6+', 'Ndxc4+', 'R5d2+', 'Rgxh3+', 'bxc1=R', 'Rgg8+', 'Rfxc8+', 'Rgxe5+', 'Qexe1+', 'Rea3+', 'Rgd4+', 'Rfa4+', 'Qbxe4', 'cxd8=R', 'Rdxh3', 'Rcxg5', 'Q7xg6', 'Rexa1', 'Qeb8', 'N5d6+', 'Q8xa5+', 'N8xh7', 'Q8a7', 'Qed4', 'Q4d6', 'Q6f6', 'Nfxg3+', 'Qdxd3+', 'Raxf5', 'Qhh4', 'Rgxe8+', 'Raxe5+', 'R4g2+', 'Q3e3+', 'Rdxa3+', 'Q8e6', 'Q3e4+', 'Q7h7+', 'Q2f5+', 'Q8b4', 'Q3c3', 'Qbe5', 'Rgxb4', 'R2h4+', 'Ndxf1+', 'R7xf3+', 'Qac1', 'Raxg8+', 'Qexc6+', 'Q1e3+', 'Qaxc6', 'Qgxd2+', 'Qdh4+', 'Rdf5+', 'Raxh8', 'Q1h4+', 'Nba4+', 'Rhxe6+', 'Qhxa8+', 'Qgxd5+', 'R7xh6+', 'Rfxh1', 'Qbxb7+', 'Kxc4+', 'Qbxd5+', 'Rcxa8', 'Rgc4+', 'Q1xh5+', 'Qgc6+', 'Qea8', 'Qfb8', 'Q3a3+', 'R8xd7+', 'Q6f2', 'R7xg3+', 'cxb1=N+', 'Qdxb4', 'Qfg6', 'Rhxe8+', 'Rexc4+', 'Rhxc2+', 'Qda5', 'exf1=N', 'Qdxe6', 'N6g5+', 'cxd1=R', 'Qdb3', 'Q3e7', 'Q8a6+', 'Qhg3', 'Q6d3+', 'Qad5', 'Qexb3', 'Qeb5+', 'R8xf5+', 'Qeg4', 'Q8a6', 'Qaxc5', 'Qcf4', 'Ngxe8', 'Q5g3+', 'Rexh8', 'R3xg4+', 'Rcxh6', 'Qhc4', 'R5f3+', 'N3f2', 'Q8g7+', 'axb1=N', 'N8f6+', 'fxg1=R+', 'Qcxc7', 'Qexa6+', 'hxg8=N+', 'Kxg7+', 'Qef4', 'Q4f5+', 'Qbxd6+', 'Qbg4+', 'Qfxe4+', 'R8xc6+', 'Rec5+', 'Rag6+', 'Rbxd6+', 'Rgb6+', 'Qcxe7+', 'Qad7', 'Q7e5+', 'Q5h2+', 'Rbxg4+', 'N1xa2', 'fxg1=B', 'Nfh7+', 'N3f2+', 'Qbxd8+', 'Qaxb7+', 'Q5xc6+', 'Rdxb4+', 'Qdg4', 'N5xa3', 'N5h6+', 'Qfxe6+', 'Qaxd4', 'Qea3+', 'Qhh2', 'Qaxb3+', 'Q4e2', 'Qcf6', 'Q4xg5+', 'R8xg2', 'N6xf7', 'Raxg4', 'Qhxh6', 'Qhe3', 'Rgb3+', 'Q2c2+', 'Q7e3+', 'Qcxf5', 'Qfxg5', 'Nfxh3', 'Q8xh7', 'Qdg7', 'N3xg4', 'N4e6', 'Ncd1+', 'Q7c6+', 'Qfxh4+', 'Nab3+', 'Ka3+', 'R6b4', 'Q3g6+', 'Q6e2+', 'R3xf4+', 'Rhxh8', 'Qfa2', 'Qexd3', 'R6c2+', 'Qdf8', 'Qgxa2', 'Qaxa8', 'Qfxc3+', 'Q8g7', 'R4xa6', 'N1xa3', 'Qeg1', 'Qfxa2+', 'bxc8=R+', 'Rdxc1+', 'Q5d4+', 'Nexf8+', 'Qhxc5+', 'Qce7', 'Qaf6+', 'N4xg3+', 'R6xh2', 'R4a5+', 'R7xb5+', 'Qbxf7', 'Nexf1+', 'Rdxf3+', 'cxd1=R+', 'Qfd6', 'N7a6+', 'N7d6+', 'Qhg6+', 'Kxb3+', 'Qcxe8+', 'Q5e2+', 'Qhc1', 'Qcd1', 'Qaa1', 'R6xa2+', 'gxh8=B', 'Qdxh8+', 'axb8=N', 'Q5h8+', 'R7xe6+', 'Q1a5', 'Qfe5', 'Qac5', 'exf8=N', 'Qcxd2+', 'Qbf3+', 'Qcg5+', 'Rexa7+', 'Qfxc7+', 'Qaf6', 'Qdxe5', 'R3a6+', 'Qdxd4', 'Bab2', 'Qca2', 'N6a5+', 'R3xg5', 'Qfxc1+', 'Q5g4', 'Q1g3+', 'Qfxb5+', 'Qhc2+', 'R4xg3+', 'R8xg3+', 'Qbxf2+', 'Q2d4+', 'R8xg7+', 'axb1=R', 'Q3f5+', 'Q1xa7+', 'Q8a7+', 'Raxg3+', 'Qad1', 'Q1h3+', 'N5xg6+', 'Qdxh5+', 'Qhxh8', 'Nfg2+', 'Q2c3+', 'R3xd6+', 'Qhxh2', 'Q6b4+', 'Rexg1+', 'Raxd5+', 'Kxe6+', 'Q6a6', 'Q1e5', 'Qeh6+', 'R8xb3+', 'Kxc1+', 'N4e6+', 'N2xh3+', 'N4xc3', 'Q8b5', 'Q2f7+', 'N1b3+', 'R5xb6', 'N6xd5+', 'Q5c4+', 'Nexc4+', 'Rgxd8+', 'Q1a4+', 'N4xd3+', 'R7b4+', 'Qexg8', 'Ka7+', 'Qexf1+', 'Q3d1+', 'Qgb7', 'Q7e6', 'Q2e4+', 'gxh1=R', 'Rbxe5+', 'cxd8=B', 'Qaxe7+', 'R4xh5+', 'Qaf5+', 'Rfxc5+', 'R7a3+', 'Q7xg8+', 'Nab2+', 'Qef7', 'Q4xd5+', 'Rdf4+', 'Q3xd4+', 'R5xg3', 'Qdc1+', 'Qexf8', 'Rhxa8', 'Q1xf1', 'Qbxf4+', 'Qaxd4+', 'Q3xh7+', 'Qdxg1+', 'R2xh7', 'Qdxd2', 'Qaxb1+', 'Ndxb5+', 'Qgc3+', 'N5xf3', 'Qdxd6+', 'R8xb4+', 'N6a4', 'Qbxg1+', 'Q8c7+', 'Qah1', 'Qbh2', 'Qbxb2', 'Qbg6', 'Q6e5+', 'Qhb4+', 'Qcxa3+', 'Qfd4', 'N2xg4', 'Qfb7+', 'Qha7+', 'Qcg7', 'Rgxe4+', 'Ree3+', 'N6xf5+', 'R4d2+', 'Qda5+', 'Q5a2+', 'Qca1', 'Qcxd4', 'R3xa6', 'R6xb3+', 'Rhxb5+', 'Qgxb6+', 'Q7f3+', 'Qag3', 'Qfe1', 'dxc8=N', 'Qce6', 'Q4f5', 'R1xd3+', 'Q8xb5+', 'N3b5+', 'Qdxf5+', 'Rgxe1+', 'Qcxd6+', 'Q2f4+', 'R3xf7', 'Q8xh2+', 'R8a5+', 'Q1f3+', 'Rbxf1+', 'Qexh5', 'N8xf6+', 'Rcxh4+', 'Qhf3', 'R2xa7', 'Rcxg4+', 'Qdxc3', 'R7xd3', 'Rgxa6+', 'Qhc7', 'Rcxg6+', 'Qde8', 'Rag4+', 'Qfxf5+', 'Qba4', 'R5xh7', 'Q2c7+', 'dxc1=R', 'R5f2+', 'Qgxe8+', 'Qbxd3', 'Rexb5+', 'R4a2+', 'R6a3+', 'R4a6+', 'Qga7+', 'R1xg7+', 'gxh8=R', 'Q1xc2+', 'R2xe6+', 'Q3xh5+', 'Q8g3+', 'Qcb4', 'Q5a5+', 'Rfxh3+', 'Q6h6+', 'Q6xf7+', 'Qhb6', 'Qexb3+', 'Q1f6+', 'N5c3+', 'Qbxg2+', 'R2xg5+', 'Q4f2+', 'Rexa6+', 'Qbh1+', 'Nba1', 'Qcf3', 'Kxb4+', 'Rcxh2', 'R8xd6+', 'Q1a2', 'Qhxh6+', 'Qdg8', 'Qexg5+', 'R3xg5+', 'Q8d6', 'Qfg3', 'R3xe2+', 'Rdxa7+', 'Ncxb6+', 'Raxg4+', 'Qcxf8+', 'Qda1', 'Qeg2', 'Rfc4+', 'Q8xb7+', 'Qcb8', 'Rgxa1', 'Qexf8+', 'Qaxg5+', 'Qaxa2', 'R1xa7+', 'Qeb4', 'Qba6+', 'Q3d4+', 'Qbxg4+', 'Q7f4+', 'N2xf3+', 'R7xg2+', 'Qdxa7', 'Qaxb8+', 'Rcb3+', 'R3xb6', 'Qgxc4+', 'dxc1=N+', 'Q4e3+', 'Qcb3', 'Q5e6+', 'Nbxd1', 'Qgh2', 'fxe8=R', 'Qhxb2+', 'Qbxc1+', 'Q8xb6+', 'N4xc5+', 'Qdxg3+', 'N1xg3', 'Q3e2', 'Qgxf1+', 'Qad6+', 'Qbxc6+', 'Bhf6', 'Raf4+', 'Qea6+', 'Qch6+', 'Qah6', 'Qdxd1+', 'N1xe2+', 'Q3f8+', 'Q1xe4+', 'Q1xg1+', 'Qcxb7+', 'Qaxb2+', 'Rdxg4+', 'Qhd5', 'Rcxa5+', 'Qbxh7+', 'Q7xg7+', 'R7xg4', 'R1xf7+', 'Nexc5+', 'N4xe5+', 'Q8g8+', 'Q2f2', 'Q6g7+', 'R3d2+', 'Qbxe5', 'R6h4+', 'Q4h4+', 'Qeh4', 'Ngxh5+', 'Qaxd5+', 'fxg1=R', 'Rff7+', 'Qaxd6', 'Qaxc3', 'Q7b6+', 'Nac1+', 'Rcxg1+', 'Q5g5+', 'Qexc1+', 'Qhxd4', 'bxa1=N', 'Nfxg1', 'Neg2+', 'Qgh7', 'Q7h4+', 'Qhxe7+', 'Rdd2+', 'Q4d7+', 'Q6h6', 'Rad6+', 'R3xe6+', 'R1xe4+', 'Kxd7+', 'Qhb6+', 'Qae3', 'Qgc1', 'N6xg7', 'Q6f4+', 'Rbxe3+', 'Qdxc1', 'Rbb1+', 'Nbxa1', 'Rbe3+', 'Qba7+', 'Qfxf5', 'Qfxf4', 'Qfc5', 'Q5b6', 'Qcg2', 'Q1xg3+', 'Nbc1+', 'Ndc1+', 'Q6b3', 'Q3c3+', 'Q8g6', 'gxf1=R', 'Q2e6+', 'dxc1=B', 'N2xa3', 'Q8xg8+', 'Qaxf1', 'R1g3+', 'Q8c8+', 'Qgd7', 'R3xc4+', 'Q1a2+', 'Qdc6', 'Qcxd8', 'Qaxd1+', 'Qfxh3+', 'Q8d7', 'Q7xa4', 'Nfxd2+', 'Ndxc6+', 'R1xd5+', 'R5xg4+', 'Q8d3', 'Qda4', 'Q7xf2+', 'Raxc4+', 'Rexb4+', 'Q1b3', 'Qag2', 'Qcxg6', 'Q4c5+', 'Q4xf4+', 'R4b2+', 'Q8xf3+', 'Qfxg3', 'Q3h3+', 'Qag7+', 'N8a6+', 'Rcc7+', 'Rbb8+', 'N5xd6', 'Qch3', 'Qda2', 'Q1xb2', 'Qaxg6+', 'Rgxh4+', 'Rdxa1', 'R5xe3+', 'gxf1=B', 'R2xa4+', 'Qef1', 'N3b2', 'Qdc2', 'Rbxh6+', 'hxg1=R+', 'Naxb7+', 'Q4xg6+', 'R4xd3+', 'N6xc4+', 'Rgxc8+', 'Qbb5', 'Qdh6+', 'Qgxf5', 'Qexd6+', 'Qdxe5+', 'R7xf2+', 'Rgxb2+', 'Ndxe1', 'Q1e2', 'Q6xd4+', 'Rfd5+', 'N3xc2', 'Qbxa2+', 'Qgf2', 'Qgxd6+', 'N1xg2', 'R2xf6+', 'Qeh5', 'Q3xe3+', 'R5xb6+', 'Qgc5', 'Q7e5', 'Q2e5+', 'Q3b3+', 'Qexf7', 'Qfxh5+', 'R4xg7', 'R5xg6', 'Qfb3', 'Qch4+', 'Q7c2+', 'Qexf2', 'Kxh2+', 'Bbe3', 'Q4xh7+', 'R5xd2+', 'Qgxc1+', 'N8d6+', 'Qfa4+', 'Rhxd1+', 'Rfh7+', 'Nhxf7+', 'Q4xe3+', 'Qeg8', 'Q4e1', 'Qfxg2', 'Q1f3', 'Qbxc2+', 'Q2b2+', 'Ngxh4+', 'Qeb6', 'N7xg5+', 'Qeb2+', 'N6xh4', 'Neg7+', 'Rhc5+', 'Qdxh4+', 'Qhxf1', 'Qfxe3', 'R1xf7', 'Qbxe8', 'R5xg6+', 'Ncxb7+', 'Naxc4+', 'R8xe4+', 'R8xa7+', 'Qcd8', 'R1xf5+', 'Q6e7', 'Qexd5', 'Q2b7', 'Qbe7+', 'Nab7+', 'Kxc6+', 'Rexh3', 'Ngxe1', 'Qgxe3', 'Qhg5', 'Qed6', 'R4xg6+', 'Qef6', 'Qexc8+', 'Qdxc2+', 'Kxe7+', 'Q6d8', 'Qbg2', 'Qag7', 'N3xa2', 'Q1e1', 'Qaxa5', 'Q1e5+', 'R6xf4+', 'Q3b7+', 'R8xb7+', 'bxc1=N+', 'cxb1=N', 'Q4c1+', 'Q7d6', 'bxc1=B', 'Rdxb5+', 'Qhxf5+', 'Q7xa5+', 'R5xc3+', 'R1xd4+', 'R6xh4', 'Qbg3', 'Qdxe7+', 'Qda7', 'Q7a6', 'Qcxe6+', 'Qae6+', 'Qfxg8+', 'Qhxd8+', 'Ncxb2+', 'Nce1+', 'hxg8=N', 'Rgg2+', 'R7xd5+', 'Qcxa3', 'Qdxa4+', 'Q1c6', 'Q1xg5+', 'Q6xh6+', 'Rgb5+', 'Qfh6', 'Q6h2+', 'Qch3+', 'Qdh3', 'Q1xf2', 'Qbxd2+', 'R6c3+', 'fxg8=B+', 'Qcxd6', 'Qbg5+', 'R1xb6+', 'Qfxe3+', 'Qexg1+', 'Q6xg7+', 'N1xb3', 'bxa8=N', 'Kxb2+', 'Rbxh6', 'R3h5+', 'Rexh3+', 'Q8xc8', 'Q8xa6', 'Nhxg5+', 'Qeb2', 'Q4d3+', 'Qgd2', 'Qexf5', 'Bfg7', 'Bge6', 'Q5f7', 'Qfa8', 'Q6a7', 'R4xh7', 'Q1g4+', 'N5xd4+', 'hxg1=N+', 'Rfh3+', 'Rhb3+', 'Kxf8+', 'Qdg1', 'Q8xc6+', 'Q2xg2+', 'Qfxf1+', 'Q8xf7+', 'Q6xf1+', 'Nbxc5+', 'R3xc7', 'N4b6+', 'Qexg1', 'Q5d3+', 'Qgxf8+', 'Q2g5', 'Q5g6', 'Rbxg1', 'N4xf3+', 'Qbe1', 'Qec1', 'Q7a7', 'Qaxc7+', 'Qbxc8+', 'R7xd6+', 'Q8xd7+', 'Q1xf5', 'N7xf5', 'Qbf1', 'Q8xg7', 'Q1xg4+', 'Qbd8', 'Qhc8', 'N3xe4+', 'R1xb7', 'dxc8=B', 'Qea4', 'R8xa4+', 'bxa8=N+', 'Qaxc5+', 'Q8e5', 'Qexb6', 'R2xc5+', 'Rcxf4+', 'bxc8=B', 'Qbxb1+', 'Q8xc7+', 'N5xa6+', 'Qh8f6+', 'Naxc7+', 'Qhxc1+', 'Qaxg2+', 'Qhxc6+', 'Qgxe3+', 'Qexd7+', 'N1xf2+', 'Rgxh7+', 'Qgc2', 'Q8c3', 'Qgxh2+', 'Qbxg5+', 'Rbxg5', 'R7xc5+', 'Nfg7+', 'Rdxa8', 'R6xe2', 'Nec1+', 'Qcb5', 'Q4a5+', 'Qexf3+', 'Q5g4+', 'Qgxf6', 'Rgxc5+', 'Rcxb8+', 'Q6xd6+', 'Qexd2+', 'Rcxa6+', 'Qaxb3', 'Rgg1+', 'Qgd5', 'Naxb1', 'N2xb4', 'N1xh3', 'N3e2+', 'Qaxf3+', 'Nbxc3+', 'Q3b2', 'Q1c1', 'Q8b5+', 'Rfh2+', 'Rdxa6+', 'Q8a2+', 'Qed8+', 'Qfxd3+', 'Q6g4+', 'Qbh5+', 'Q6xg6+', 'Q8xb6', 'Qcxd3', 'Raxf4+', 'Q8a5', 'Qfxf6', 'Q5c5', 'Qfxb4+', 'Q4d4', 'Qac2', 'Rgxc4+', 'Q3e7+', 'Qaxd1', 'Raxh3+', 'Qdxf3', 'Qe8e5+', 'R5xe4+', 'Q6g6+', 'Rgc5+', 'R5xh2+', 'Qge1', 'Qdg2', 'Qbxb4+', 'Qcg8', 'Q8xc6', 'Rgc3+', 'Rcxh7+', 'Q4c7+', 'Q3c4', 'Q5e5+', 'Rdxh4+', 'Qexa4', 'Rgxh5+', 'R5xb2+', 'Rdxc4+', 'Q1b5+', 'R7xc4+', 'R8xa6+', 'R1xe3+', 'Qdxc3+', 'Qgxg8', 'Q1a6', 'Qba8', 'Q6b7', 'Qbxb3', 'Q8d2', 'Rfxa7+', 'Qgxd5', 'Qdxh7+', 'Qaxg4+', 'Qcxc3', 'Q6d4', 'Q1d2', 'Qexg5', 'Nfd1+', 'Qge5', 'Nhxg7+', 'R4xe7+', 'Q5f7+', 'N4g6+', 'R1xa6+', 'Qbxb3+', 'Ncxa2+', 'Q8h5+', 'Q7d4+', 'Q4xf2+', 'Qcxa2+', 'Qbxh6+', 'Qbxd1', 'Qcxd1+', 'Rbe6+', 'Q8h3', 'Q6h7', 'Qfe7', 'R8xd3+', 'R3xb5+', 'R6xc7+', 'Rexh5+', 'Qbxb7', 'N2xe3+', 'Qfe3', 'Q2xc2+', 'Nba6+', 'N6xe5+', 'Qca3', 'N1h2+', 'Qhxh2+', 'Qgd4', 'Q4g1+', 'N6b4+', 'Qdxc7', 'Kxa4+', 'Q4d2', 'Q3xf4+', 'exd8=N+', 'Q4b7', 'Q7c8', 'Q8xd7', 'Q1xb1+', 'Q8d5', 'Qdxd8', 'Qcxb3+', 'Q6xc3+', 'R5xa2+', 'Nca2+', 'R5xh2', 'Nbd1', 'Q6f6+', 'N1e2+', 'B4d5', 'B3c5', 'Bcd6+', 'Q2a3', 'N4a5+', 'Q6b2', 'Qb2c2', 'Q2b2', 'Qd2c2', 'Qc1d1', 'Qgxd4', 'Q3xd3+', 'Qcxc4+', 'Qag1', 'Qdb8', 'N1d3+', 'Qhxf6', 'Qexh5+', 'Nfxg7+', 'Qgh5', 'Q1h4', 'Rexh1', 'R2xd5+', 'Q4b4+', 'Qexe6', 'Q6xf6+', 'Qdf2', 'Rexh4+', 'R6xc5+', 'Qaxd7', 'Rdxb6+', 'R1xh2+', 'N2xg4+', 'Q7d8+', 'Rae5+', 'Qgxb3+', 'Rdxc3+', 'Q1d6+', 'R4xc5+', 'R2a7+', 'Q7h5+', 'R2xd4+', 'Q3a2+', 'Qag6', 'N4xg5+', 'Q5a6+', 'Q7xb7+', 'Rgxa8', 'Q4b7+', 'Q6g2+', 'Qgf5', 'N5xg4+', 'R6xg2', 'R5xf4+', 'Qcxh2+', 'Rcxa2+', 'R4xg5+', 'Q6e4+', 'Naxb5+', 'Qag3+', 'N4a6+', 'Qfc2', 'Qaxg2', 'Q2g5+', 'N3xe2', 'Qaxf6+', 'Qdxe8', 'dxe8=B+', 'Q6xf6', 'Qbe7', 'Q2e5', 'Qbxa3', 'Q1b7+', 'R6xe4+', 'Qdc1', 'R1xd7+', 'Qaxg1+', 'Qbxd7', 'Q5h7+', 'R3xg6+', 'Q1xc7', 'Q8d8', 'Qexe5+', 'Qcxe4', 'R5xb7', 'Rcc5+', 'Nbxd8', 'Qfxe4', 'Qfxb2+', 'Rff6+', 'Qhxe5', 'Kxe1+', 'Qec4', 'N5xe6+', 'Q2h5+', 'N4xh3+', 'Q8c6', 'Ngf1+', 'Qea5', 'Qha2+', 'Q1xa6', 'Qexc4', 'R3xc6+', 'Nbxc4+', 'Reb5+', 'R6xc2+', 'exd1=B', 'Qexd3+', 'Qfxd5', 'Qde3', 'Nfxe8+', 'Qcxb4+', 'R8xa2+', 'Qhxh3', 'fxe1=B+', 'Qgh4+', 'Ndxb7+', 'Ree2+', 'Q3g5', 'N3a5+', 'Qhc6', 'Rhxc8+', 'N2xe3', 'Q5xe6+', 'Q1xd6+', 'Q8xg7+', 'Rexg1', 'Qgd6', 'Qaxb8', 'Qbxf1+', 'Qeh7', 'Qfxa8', 'Qcxe3', 'Q8e6+', 'N8g6+', 'Qeb5', 'Q4h8+', 'Qfxe7', 'Kxe8+', 'Qbxf5', 'Qgxe1+', 'Q1b1', 'Ncxa3+', 'Q3f3', 'Q6a3', 'Qfxg7', 'Qaxb4+', 'R5xd4+', 'Qhxa7+', 'Qgxf2', 'Rfxb5+', 'Qexh7+', 'Q3b2+', 'Q8h2+', 'N1xc2', 'Qha6', 'axb1=B+', 'Q3g7+', 'R7xh4+', 'Nexd8+', 'Q4f1+', 'Q6e6', 'R1xh5+', 'Nhf8+', 'Qhxd8', 'Ka8+', 'Rec3+', 'N6xh7', 'Qaxe3+', 'Qfxg4', 'Q2xb3+', 'B4e5', 'Bce6', 'Bde5+', 'Bed5+', 'Q1b4', 'R1xb2+', 'Q4c3', 'Q2a7+', 'N6h4', 'dxe8=B', 'Q4d5+', 'Q5g8+', 'Rfxc3+', 'Q1h6', 'Qbh1', 'Q1h5+', 'Q7h4', 'Q4xa5', 'Qgxf4', 'Qfxd4', 'Q5xe2+', 'cxd8=N+', 'Nab1+', 'Qbxc5', 'Q5g1+', 'Bexf5', 'Bae5+', 'Bbe4', 'Bde4+', 'Bgf3', 'Qfxc2+', 'Q4d3', 'Q6f8', 'Qgxb7+', 'R8xh4+', 'Qgxh1', 'Kxg8+', 'Qexe2+', 'Q2f3+', 'Q3g7', 'Ka1+', 'Rgxc1+', 'N4c6+', 'Qhxf2+', 'Qba2+', 'Qhxc8+', 'Q4xh2', 'Qdxc6', 'Q8e7', 'Qgxg4', 'R5xg2', 'Q3d2+', 'Q8xe2', 'Q8c7', 'Qaxc1', 'Qdxf4', 'Q7b5+', 'Q1f5+', 'Q1g6+', 'Q6xg7', 'Qbxh7', 'Qaxc2+', 'Rhb5+', 'Qbd3', 'R7xh4', 'Qhxf7', 'R4xh3', 'Kxg2+', 'Qbxf2', 'Q8xe7', 'Nbxd2+', 'Q5xg3+', 'Q6xd7', 'Bhg4', 'Bgf5+', 'N6xf4+', 'Rbxg3+', 'Q2d3', 'Q6g8+', 'N5xe4+', 'Raxh1', 'Qgxf5+', 'Qaxb5+', 'Rhxb3+', 'Qhb3', 'gxh8=N', 'Nca8', 'Q7xg6+', 'Q3xf2+', 'Qbf3', 'Qaxc8', 'Q7xd5', 'Q5d2', 'Q7e2+', 'gxh8=R+', 'R4xe5+', 'Rfxa3+', 'Kxd2+', 'Qexe3+', 'Q8xh7+', 'N3xc4+', 'Qbxc3+', 'Q3b4+', 'Q4xg1+', 'Q5f1', 'N5xd6+', 'Qca4', 'Q6a3+', 'Qbxc1', 'Q3d1', 'Qdxc5', 'Rgxh2+', 'Q2xc4+', 'dxc1=B+', 'Qbg8', 'Q7e8+', 'Q7f7+', 'exf8=R', 'Q6f5+', 'Qgxe7+', 'Qexf6', 'Qgf7', 'Q7e7', 'N6xb7', 'Q6b5+', 'N1xf3+', 'Qab5', 'Q2e3+', 'Qfg4', 'Q7g4', 'Qhxf3', 'R2xe5+', 'Qae2', 'Q6a6+', 'Qbh7', 'Q1d3', 'Q5xb5+', 'Q1xd1+', 'Q4g6+', 'Qcxb3', 'Qaxa4+', 'Q8e4', 'Qeg6', 'Q4xe6', 'Qaxh7+', 'Q4a4+', 'Rcxh8', 'Rgxc3+', 'Qdxb1', 'Q8b3', 'Q5d3', 'Qhxb8', 'fxe1=R', 'R5xa3+', 'Qga8', 'Qdxc4+', 'Rgxb5+', 'Qgxh2', 'Qbxe1', 'Qdxe3+', 'N3xd4+', 'Qag4', 'Q7e7+', 'Q6d8+', 'Qfb6+', 'N6xe7', 'R4xf3+', 'N7xa6+', 'Q1d1', 'R7xf4+', 'Q5xa3', 'Rdxh5+', 'Q6g3+', 'R6xa5+', 'Qdxf1', 'N3xf4+', 'Rbxh3+', 'Q2g4+', 'Q3d4', 'Q4a1+', 'R2xg6', 'Q5xf6+', 'Q6xh2+', 'Qhe1', 'Nef1+', 'Q8a5+', 'Qdg5', 'Q3a5+', 'N3d2+', 'Rexb1+', 'R2xg4+', 'Qab4', 'Q4e7+', 'N1xf2', 'Qcxa7+', 'Qexc5', 'N5b3', 'exd1=N+', 'Qaxb2', 'Q1c6+', 'Q6b2+', 'Q3h1+', 'Qhxc3+', 'Q3xd2', 'Ngxh8', 'N8c6+', 'Qde1', 'cxd1=B', 'Qgb4', 'Rgc6+', 'Qbc4', 'Qbe3', 'Q3c2', 'Q1xe5+', 'Qbg4', 'Nce8+', 'Qbxd6', 'Rbxh1', 'Q7b2+', 'Q1d4', 'Raxh2+', 'Qfxf2', 'Q3e4', 'Rfb3+', 'Ndxe8+', 'Q8xd6', 'Qdxf6', 'Qbh3+', 'Qba3', 'R2xf5+', 'Ndf8+', 'Qeg5', 'N8e6+', 'Qcg5', 'R3xg2+', 'Q7c8+', 'Q6g7', 'Q7h8', 'Q4g6', 'N1a3+', 'N6xg4+', 'Q4d7', 'Qcxh4+', 'Qda8', 'N2xd3', 'R2a5+', 'Qaxb6', 'Kxc7+', 'Q4xb6+', 'Rbb5+', 'Q7a7+', 'Q3f1+', 'Q4xb3+', 'Q1xe2', 'Qga5+', 'Q3d6', 'Qbxf5+', 'Q2d7+', 'N6e7+', 'Kxh7+', 'Q5xg5+', 'Q8xc5', 'Q7e4+', 'Qbxb8', 'Qdh5', 'Q8h7+', 'Rfxa6+', 'Q6c2', 'Q7a6+', 'Qdh8', 'Q3xg3+', 'Q4d6+', 'R8xg5+', 'Qdxh5', 'Qhg4', 'cxb8=R+', 'Qcb1+', 'R2xa5+', 'Q6xa4+', 'Rcxa4+', 'Q8xe5+', 'Qhxa8', 'Q8xf5', 'R3xg7+', 'Q3xe3', 'Rd1#', 'Qa1#', 'Ra1#', 'Qg2#', 'Qxf2#', 'Qe8#', 'Qxg7#', 'Rdf2#', 'Qxe7#', 'Qxc7#', 'Rh4#', 'Qxg3#', 'Qxe1#', 'Qh2#', 'Qb5#', 'Qh6#', 'Qg5#', 'Rxe8#', 'Ra8#', 'Qg7#', 'Qa7#', 'Qd8#', 'Qa6#', 'Rb8#', 'Qa8#', 'Qd1#', 'Qxf8#', 'Qh3#', 'Qxh7#', 'Rxf8#', 'Qxb7#', 'Rxh2#', 'Qh4#', 'Rb1#', 'Nxh7#', 'Be4#', 'Qh7#', 'Rh6#', 'Rxb1#', 'Qb2#', 'Qd7#', 'Qg8#', 'Qbb3#', 'Qa5#', 'Qxe8#', 'Qf7#', 'Bf8#', 'Qh1#', 'Qf2#', 'Rc8#', 'Rxe1#', 'Qxg2#', 'Qc8#', 'R8d3#', 'Qc7#', 'Qg3#', 'Qh8#', 'Qe2#', 'Re8#', 'Nf3#', 'Qxf4#', 'Rdb7#', 'Qe5#', 'Rg1#', 'Rh1#', 'Bd6#', 'Rxd1#', 'Rd8#', 'Qxf7#', 'Bxf5#', 'Rh8#', 'Qf1#', 'Rgf8#', 'Bc5#', 'Qh5#', 'Qxc2#', 'R1e6#', 'Qxe2#', 'Qxd8#', 'Qg4#', 'Bxe6#', 'Qxh6#', 'Qxa7#', 'e1=Q#', 'h5#', 'Qb7#', 'Nh3#', 'Qxb2#', 'd1=R#', 'Qxe5#', 'Qxf3#', 'Qxh3#', 'Bf5#', 'Qxb4#', 'Rg3#', 'Bg7#', 'Qd2#', 'R7h2#', 'g3#', 'Bg4#', 'Rxa2#', 'axb5#', 'Qf8#', 'Rc1#', 'Bc8#', 'Rxh7#', 'Qe6#', 'Qc2#', 'Qg6#', 'fxe1=Q#', 'Rh5#', 'Bxf4#', 'Qdg2#', 'Qc4#', 'a8=Q#', 'Re7#', 'Qxc1#', 'Rxe4#', 'cxd1=Q#', 'Qdd6#', 'Ba3#', 'Qxd6#', 'Bxc5#', 'Rd7#', 'Qxa2#', 'Qb8#', 'f1=Q#', 'Qxd1#', 'Qeg6#', 'Qb4#', 'Qde5#', 'Qxg5#', 'Rg5#', 'Rc7#', 'Ra3#', 'Qxf1#', 'Bd3#', 'Qxg8#', 'Rf1#', 'hxg7#', 'Qf5#', 'Qxh2#', 'exf6#', 'Rxf2#', 'Nc4#', 'Qc5#', 'Qxd7#', 'Bxb5#', 'Qxa3#', 'Re1#', 'Rxf5#', 'Qxh5#', 'Rh2#', 'Rxf1#', 'Bxg6#', 'Nxc3#', 'Raxa7#', 'Nh6#', 'Bb6#', 'Qg1#', 'Rg2#', 'Rc5#', 'Rxc1#', 'Qd3#', 'Rxd8#', 'h8=Q#', 'Qe1#', 'Bxd5#', 'Nf7#', 'Qa2#', 'Qcc2#', 'Nf6#', 'Re2#', 'g5#', 'Qe7#', 'Rb6#', 'Ne2#', 'Qxg6#', 'g1=Q#', 'Rxa7#', 'Qb3#', 'Rxh3#', 'Rxf7#', 'Nc7#', 'Rh3#', 'Rg6#', 'Qxh4#', 'Rf8#', 'Qc6#', 'Rhf7#', 'Qf6#', 'g8=Q#', 'h4#', 'Ra5#', 'Ng3#', 'Qe4#', 'd1=Q#', 'Qxe6#', 'Bb4#', 'c1=Q#', 'Qce6#', 'Ra7#', 'Qb1#', 'Qxh8#', 'Nd5#', 'Nd3#', 'Bf3#', 'Rgb2#', 'Qxc5#', 'Bxg2#', 'Q3g6#', 'Nxf3#', 'Rxe6#', 'Bf1#', 'Qc3#', 'Bxf7#', 'Qf4#', 'Ne4#', 'Rxg5#', 'Bf6#', 'Qgg6#', 'Qee1#', 'Nd7#', 'Bd4#', 'Rg7#', 'e5#', 'exf1=Q#', 'Bh6#', 'Qxg4#', 'Rbf7#', 'Ne7#', 'Rxh6#', 'Bxf3#', 'Na7#', 'Rf7#', 'Qe3#', 'c5#', 'Qxf6#', 'Be3#', 'Bxd6#', 'fxg5#', 'Bxg7#', 'Re4#', 'Qa4#', 'Nc3#', 'Rh7#', 'Rxc4#', 'Rxh5#', 'Rg8#', 'Rf3#', 'Nxg3#', 'Qxb8#', 'Rxb2#', 'Ne3#', 'Ra6#', 'dxc1=Q#', 'Rxg3#', 'Rxc6#', 'Rxc8#', 'Rd3#', 'Rxh4#', 'Bxd2#', 'Rb5#', 'Qf3#', 'a5#', 'Rb7#', 'Qxd2#', 'Qxc8#', 'Rhd8#', 'Nh2#', 'Qc1#', 'Nh8#', 'Qxa8#', 'Rhxh2#', 'Rd4#', 'Qd4#', 'Nxg6#', 'Rfc2#', 'Rag2#', 'R3f6#', 'g6#', 'Qhe4#', 'Bf4#', 'Bxa6#', 'Qxb5#', 'fxg7#', 'Ne5#', 'Rexe1#', 'Ra2#', 'Bc4#', 'Rxd4#', 'Rf5#', 'Qxd5#', 'Bf7#', 'Rxb6#', 'Rdg7#', 'Rf6#', 'Qhxf8#', 'Nxe3#', 'Nc5#', 'Nc2#', 'Nxf2#', 'Rd5#', 'Bf2#', 'dxe1=Q#', 'h6#', 'Rxb8#', 'bxc3#', 'b4#', 'Rxd6#', 'Bg5#', 'b8=Q#', 'Nf5#', 'd3#', 'Bg2#', 'Rd6#', 'Bb5#', 'Qch2#', 'Re3#', 'R1d6#', 'Bxe5#', 'Rexe3#', 'Qxh1#', 'Nxe6#', 'Qxb1#', 'Rxg7#', 'h1=Q#', 'Nd2#', 'Rec2#', 'R8h2#', 'Qaxa6#', 'Bb2#', 'Qca1#', 'Rexf8#', 'Rxg1#', 'Rcg2#', 'Nxf7#', 'g2#', 'Qxc4#', 'Be6#', 'Qca8#', 'Qee8#', 'Nb4#', 'Bxc3#', 'Rc6#', 'Qd6#', 'Rgc7#', 'b8=R#', 'Qxa1#', 'Rc4#', 'Rcf7#', 'g4#', 'Qxe4#', 'Qfh8#', 'Rxf3#', 'R8e5#', 'Rdxd8#', 'Bc3#', 'Rde8#', 'Qdc7#', 'Nxf4#', 'Qd5#', 'Nxd7#', 'Qb6#', 'Bxc6#', 'g7#', 'Nf2#', 'Re6#', 'Qcc6#', 'Rxe3#', 'Rxe2#', 'Qxc6#', 'Rxa3#', 'Qxc3#', 'Nxc7#', 'Qcc1#', 'Rdb6#', 'e8=Q#', 'Rcg7#', 'Ng5#', 'Qcb5#', 'Qxa6#', 'Reg4#', 'Rgxg2#', 'Rfxf1#', 'Rdg1#', 'h3#', 'Qeg3#', 'Qxe3#', 'hxg5#', 'e4#', 'Bxf2#', 'fxg4#', 'e7#', 'Be8#', 'Rxa5#', 'Bc6#', 'Qbf6#', 'Bd7#', 'Rxc3#', 'f4#', 'Qag4#', 'Ba6#', 'R8f2#', 'c4#', 'Rxh1#', 'Rfxd2#', 'Nh5#', 'Ne6#', 'Qbxb2#', 'Be2#', 'Rgb7#', 'Bxf6#', 'Qac8#', 'a1=Q#', 'h2#', 'Qgg3#', 'Qxg1#', 'Bxe4#', 'Rxd7#', 'Rexh6#', 'Rag3#', 'Qxd4#', 'd8=Q#', 'Rfxg8#', 'cxd5#', 'Qxf5#', 'Re5#', 'Rce2#', 'Be7#', 'Ra4#', 'Rfd1#', 'Rxh8#', 'a6#', 'Nd6#', 'Be1#', 'Bh4#', 'Rf2#', 'Qxa5#', 'R7e3#', 'Rhg8#', 'b2#', 'bxc8=R#', 'Ng4#', 'Rg4#', 'R1b7#', 'Rf4#', 'Qxd3#', 'Rxa8#', 'Rd2#', 'dxe8=Q#', 'R1h7#', 'exf8=Q#', 'hxg4#', 'Bxc1#', 'Nxc6#', 'Nc6#', 'R7c3#', 'Rxe5#', 'f7#', 'Qa3#', 'Rdxc1#', 'Bh5#', 'Qbf2#', 'Ng6#', 'Rc2#', 'Bxg3#', 'Nb5#', 'Rbg7#', 'Qfh2#', 'Qef5#', 'Qgd5#', 'Qeg2#', 'gxf8=Q#', 'a1=R#', 'Qca5#', 'exd2#', 'Bxe3#', 'Qeg7#', 'b5#', 'Nxb6#', 'Qaf6#', 'Rb2#', 'Qcc8#', 'Nf4#', 'Be5#', 'Rcf2#', 'cxb4#', 'Raf1#', 'Na4#', 'h8=R#', 'Rec7#', 'Rxe7#', 'Bd2#', 'Rxf4#', 'Qdxg6#', 'Rdxg6#', 'Qhg7#', 'b6#', 'Rb3#', 'Qef8#', 'Bg8#', 'f5#', 'Bxb2#', 'Rhf3#', 'Rag7#', 'Bh3#', 'Nb8#', 'Bg6#', 'Bxg4#', 'Rxg6#', 'f6#', 'exd8=Q#', 'f8=Q#', 'Qxb3#', 'a8=R#', 'Qaa4#', 'Rxc7#', 'Reg7#', 'Nxe7#', 'Ng7#', 'Rxd2#', 'Qeh5#', 'Qca2#', 'c7#', 'Raxc8#', 'Bg3#', 'Bd5#', 'Rc3#', 'Bd8#', 'Rxf6#', 'Qcc4#', 'Qaa8#', 'Nxh2#', 'fxe8=Q#', 'Nh4#', 'Rad2#', 'R8b2#', 'Qbxg6#', 'Nxe5#', 'c3#', 'exf2#', 'Bc7#', 'Rhd7#', 'c2#', 'Rhxh7#', 'Qgg7#', 'Nxd4#', 'f2#', 'Bxc7#', 'Qdc5#', 'Bb7#', 'Rb4#', 'R1c6#', 'Qea2#', 'Rhe7#', 'fxg8=Q#', 'Rbg2#', 'dxe8=R#', 'Qfg3#', 'dxc4#', 'Qbd3#', 'Ne8#', 'Bxb6#', 'Rge8#', 'b3#', 'Na2#', 'Bxh3#', 'Nxh6#', 'Nxc2#', 'R8d5#', 'Qfh7#', 'Rgxf6#', 'Rhxh5#', 'Qhf6#', 'R1e7#', 'Reb2#', 'Qgg5#', 'R1g7#', 'Qdd8#', 'Nb3#', 'Qdg5#', 'Bxd3#', 'Nfd6#', 'R8f3#', 'Rxb7#', 'Bc1#', 'Qca4#', 'Qfc1#', 'Nxf6#', 'Ng2#', 'R6e2#', 'Bxh5#', 'd8=R#', 'Bxg5#', 'Qaa1#', 'R8e2#', 'Rxd3#', 'Rxd5#', 'Rdg2#', 'R1e4#', 'Qbf8#', 'Rcg8#', 'fxg3#', 'Rxc5#', 'R8f6#', 'Qxb6#', 'R1a7#', 'Q3h5#', 'O-O-O#', 'Qfg7#', 'Nxd6#', 'Nxf5#', 'R4c3#', 'a4#', 'Rfd7#', 'Qcxe8#', 'Q5e2#', 'Q4b3#', 'Bxh7#', 'hxg2#', 'Rxc2#', 'Qab1#', 'Nxe2#', 'Rxa1#', 'Rexd1#', 'Qaf8#', 'b1=Q#', 'Nxg2#', 'Rfe1#', 'Qdxh8#', 'Qee7#', 'Bxd1#', 'Rfxe8#', 'Qed5#', 'Nexf7#', 'Nb2#', 'Q5h8#', 'Qbd5#', 'Qdh1#', 'Nb6#', 'Qdd1#', 'R1d4#', 'Rxb5#', 'Nxd5#', 'R8g2#', 'c8=Q#', 'Bxd7#', 'Bb3#', 'Qag8#', 'Qaxa2#', 'R8d6#', 'R7d2#', 'gxf5#', 'Nxa2#', 'd2#', 'Rxa6#', 'Rdxf1#', 'Nxa7#', 'Qff6#', 'R6f3#', 'N6d5#', 'Nxh3#', 'Nd4#', 'R7a2#', 'Rxg8#', 'Bc2#', 'Qeb4#', 'gxf6#', 'Qbf3#', 'R1b6#', 'Rxg4#', 'Na3#', 'Na6#', 'Rbg4#', 'Q3g3#', 'Ned2#', 'Qdc8#', 'Ba5#', 'Qff7#', 'Qae8#', 'Qgg8#', 'f3#', 'Qhg6#', 'Ng1#', 'Rac7#', 'Qbf5#', 'Rbg1#', 'Rbxd2#', 'exf7#', 'Nxh4#', 'Nh7#', 'R8g3#', 'g8=R#', 'Ng8#', 'Ba4#', 'Nf8#', 'Nxa5#', 'Rgc2#', 'axb1=Q#', 'Nxc5#', 'Qaxc6#', 'Qfh1#', 'Qbb8#', 'R8d2#', 'Rxg2#', 'Bh2#', 'h7#', 'Rbe2#', 'Rcb7#', 'Bxe7#', 'Qbxf7#', 'gxf2#', 'Qdh8#', 'Qch5#', 'Qbb4#', 'R6f7#', 'Ref2#', 'Bh7#', 'fxg1=Q#', 'Qfg2#', 'Rgxe5#', 'R8g4#', 'Qaxe1#', 'Qfxc1#', 'Q1g3#', 'Qfe3#', 'Qxa4#', 'Rad1#', 'h1=R#', 'Bxc2#', 'Rhc2#', 'c6#', 'hxg1=Q#', 'e8=B#', 'Qgf5#', 'Qdg8#', 'f8=R#', 'Qdd7#', 'Rhd2#', 'R1e5#', 'Nb7#', 'Rdxd7#', 'a3#', 'Qac1#', 'R1e3#', 'Qfe4#', 'Rcxd8#', 'Q2d4#', 'Nxe4#', 'Rbd7#', 'Qff8#', 'Rge2#', 'bxc8=Q#', 'Q3f5#', 'R4h7#', 'Raxa3#', 'Nc8#', 'Qeb1#', 'Bxb7#', 'R8a3#', 'Rdxe1#', 'Qbb1#', 'Bxh6#', 'Qec5#', 'Rfe3#', 'd5#', 'Qdc2#', 'Bxb3#', 'Kf6#', 'd6#', 'Qgg4#', 'e8=R#', 'Qdxh5#', 'Q8h8#', 'Qdd5#', 'R3f2#', 'Qgf3#', 'Qhh1#', 'Rae1#', 'Rexg1#', 'Qcxg5#', 'Red2#', 'Rhe1#', 'R8a2#', 'Rbd8#', 'Bxc4#', 'dxc7#', 'Qef6#', 'Qac5#', 'Qhd2#', 'R4e7#', 'R4a7#', 'e1=R#', 'R1d7#', 'Q3b5#', 'Rfxf5#', 'hxg6#', 'Qeh6#', 'Raf7#', 'Rae8#', 'cxb1=Q#', 'Rfe8#', 'bxc1=Q#', 'R7h3#', 'Rxb4#', 'Rae6#', 'Q5g3#', 'Qcg4#', 'Q2f7#', 'R3e7#', 'Rexd5#', 'R6h7#', 'b7#', 'Rbxb8#', 'Qca7#', 'Rbxc1#', 'Qfd1#', 'Rexg2#', 'hxg3#', 'Q1a4#', 'Qhd4#', 'Qag1#', 'R8g5#', 'Rce7#', 'Qef3#', 'Qbd1#', 'd4#', 'Q3d1#', 'Ref8#', 'R1f7#', 'cxb7#', 'Rcf4#', 'axb6#', 'Qad5#', 'Qdb8#', 'Rhc7#', 'Rexe8#', 'Qah5#', 'Qbd8#', 'Qaxe7#', 'Qhc1#', 'a7#', 'Qff1#', 'Qea4#', 'Qaf5#', 'Qgg1#', 'Q7xg8#', 'Rfc1#', 'hxg8=Q#', 'Na5#', 'exf5#', 'Rdf7#', 'Rfd2#', 'Rexe6#', 'Q4f6#', 'R8c2#', 'Qfa3#', 'e6#', 'Rxb3#', 'exf1=R#', 'Rbf2#', 'Q3xd4#', 'Qdb3#', 'R3f4#', 'Qbg8#', 'Qfh5#', 'Bxf1#', 'R1f6#', 'R3a7#', 'Rexe2#', 'Kg1#', 'Qac4#', 'Qec7#', 'Rfxf2#', 'Rag1#', 'Qbb5#', 'Bxc8#', 'Rfe2#', 'Qfd4#', 'Nxd2#', 'Nxg5#', 'Rfb7#', 'Rexf1#', 'Q3g5#', 'Qdb2#', 'Qhh6#', 'Nxd3#', 'Qff3#', 'R1d5#', 'Nf1#', 'Qge4#', 'Ba2#', 'Qfg4#', 'Qaxb7#', 'axb2#', 'R7f2#', 'Qaxa7#', 'Qhxg5#', 'R8e4#', 'Qeh1#', 'Nb1#', 'Q3xh7#', 'Qdxg1#', 'Rbxd4#', 'Qda7#', 'Rgxg1#', 'Qfxg6#', 'R2d5#', 'R3g6#', 'R4g7#', 'R7h5#', 'Nxb3#', 'd7#', 'exf4#', 'Qdg4#', 'Rac8#', 'Rgxf8#', 'Qeh8#', 'Qee4#', 'exd1=Q#', 'Nxg7#', 'Rdg4#', 'Rhxh4#', 'Qbxg1#', 'Qge3#', 'N3e4#', 'Q8c7#', 'Qbc3#', 'R2h7#', 'Rbg6#', 'Nxb2#', 'Qab7#', 'b1=R#', 'Reb7#', 'Qcxa3#', 'Qca3#', 'Raxf6#', 'e3#', 'axb4#', 'Raxe8#', 'Nxb4#', 'Q8f8#', 'Qfh6#', 'cxd8=Q#', 'Rhb2#', 'Qbd6#', 'R8e3#', 'dxc8=Q#', 'Rde2#', 'Rcf8#', 'dxc3#', 'Qfxf2#', 'e2#', 'Rfxf7#', 'Rdxf6#', 'Qcf5#', 'Qab6#', 'Bxd8#', 'Qhd8#', 'Nxb8#', 'Ndc5#', 'Rdc7#', 'Rhxf7#', 'Qcb6#', 'Qbc2#', 'R3h7#', 'Qde8#', 'R4d2#', 'gxf1=Q#', 'Rgxe1#', 'Qgxc5#', 'Q2f4#', 'Rcxf3#', 'Q1f3#', 'Nxc4#', 'Qbh8#', 'Nce7#', 'Raxd2#', 'Bxa3#', 'R8a7#', 'Rbxf1#', 'gxf3#', 'Qfxe4#', 'Reg2#', 'R5f4#', 'Ngh7#', 'Rcf3#', 'gxh2#', 'Q1b2#', 'Bg1#', 'gxh6#', 'R8h3#', 'Qdf8#', 'bxa8=Q#', 'Qbg1#', 'Nd1#', 'Bxd4#', 'Rxa4#', 'Qcf3#', 'Bxg1#', 'Qfg5#', 'R7d5#', 'Reg6#', 'Rexc1#', 'Rexd8#', 'Rexg7#', 'Kf3#', 'R1a6#', 'Qca6#', 'cxb8=Q#', 'Qfxg7#', 'Qdxf7#', 'Nhg3#', 'Qea8#', 'Qcg5#', 'Qcf6#', 'dxe5#', 'Qcc3#', 'Qda8#', 'Qbb6#', 'Qee2#', 'Rhc8#', 'Qfg6#', 'Qexe8#', 'R8c3#', 'Qcb7#', 'Rfxd7#', 'Kd7#', 'Qhb7#', 'Rfc7#', 'Kg7#', 'Rdg6#', 'Qga7#', 'R1xg7#', 'Kc7#', 'Qhh4#', 'Q8c6#', 'Q1xc2#', 'g1=R#', 'R8b7#', 'Kb3#', 'Rac2#', 'Q1g1#', 'R8h5#', 'R4c5#', 'Qhxg2#', 'fxe4#', 'R3g2#', 'Qhd5#', 'R2g7#', 'Nxd8#', 'Q5a5#', 'Bd1#', 'Rfxh3#', 'Qeg8#', 'Rcxf7#', 'Qdf6#', 'Qef7#', 'R1f5#', 'Rae7#', 'cxb6#', 'Rdb8#', 'Qbe4#', 'N5c3#', 'Rfxg2#', 'Qeg4#', 'R8c4#', 'R2e5#', 'Qbxg2#', 'R5g6#', 'Qdxf6#', 'Rbd2#', 'Qfb5#', 'Qda4#', 'Reb8#', 'Qgb7#', 'Q4f2#', 'Rdxe8#', 'Qfc5#', 'Qhh8#', 'Rcd7#', 'Rec8#', 'R1c7#', 'Qbh1#', 'Qec6#', 'Rcb2#', 'Rhe2#', 'R5a2#', 'a2#', 'Qee3#', 'Qdg6#', 'Qhf1#', 'Qhxh3#', 'R2d6#', 'Nxa6#', 'Bb8#', 'Qexb5#', 'Qexg5#', 'gxf4#', 'R3xe2#', 'Rbxb2#', 'Nge6#', 'Qac3#', 'Ngf3#', 'Rhg4#', 'Qef4#', 'Rag8#', 'axb3#', 'Rgd2#', 'Qec3#', 'Qdd3#', 'Qac7#', 'N4g3#', 'Qcxf8#', 'Ke7#', 'Rbe7#', 'Q2b4#', 'exf3#', 'Qbxf8#', 'Rbxf7#', 'Qed3#', 'Nxb7#', 'Rfe7#', 'Rae2#', 'Ndf2#', 'R4g6#', 'Qdf4#', 'Qfh3#', 'Rfb2#', 'Qah4#', 'R7c2#', 'Qdf7#', 'Q8xb7#', 'Qgxg6#', 'Qfxf6#', 'Qgxg8#', 'c1=R#', 'R5d4#', 'Qaxg5#', 'Qbxb7#', 'Qed2#', 'Rfg7#', 'Qea5#', 'Bxa2#', 'Kf2#', 'Qbb2#', 'f1=R#', 'Qhg2#', 'axb8=Q#', 'exf8=R#', 'Qde2#', 'gxh1=Q#', 'Rgd1#', 'Rhg1#', 'Q3g2#', 'fxg6#', 'Qaxd7#', 'Q5f3#', 'Rbxe2#', 'Rdxe7#', 'Qcd8#', 'Qcg7#', 'Qcxg7#', 'Qfa7#', 'Bxh4#', 'R8b3#', 'Q3f6#', 'Rcxc8#', 'Q8xe8#', 'bxc4#', 'Rbxd1#', 'Rfxb6#', 'Rhxg1#', 'R6d3#', 'Q4f3#', 'fxg2#', 'Rhf1#', 'Rbg8#', 'R1xh7#', 'Rhf2#', 'Qba3#', 'Bxb4#', 'R2g6#', 'Raf2#', 'Rbxf6#', 'Qfe1#', 'Qhh5#', 'Qff5#', 'exd7#', 'Qeb6#', 'Qhxg7#', 'Qhd3#', 'R6h2#', 'Rcxf8#', 'Nhf7#', 'Qgg2#', 'Rfg8#', 'Qce8#', 'Rfxf8#', 'R3g7#', 'Qfxc8#', 'Qbg6#', 'R5a3#', 'Nc1#', 'Qfh4#', 'Qde6#', 'Q7h6#', 'R7xg6#', 'Qcf4#', 'Raxb8#', 'Nxg4#', 'Qcf1#', 'Qhf7#', 'Rfg2#', 'Rcg1#', 'Rbxb7#', 'Qfd8#', 'dxe1=R#', 'Rcxe1#', 'Qhb8#', 'Qhxe8#', 'Qdxe8#', 'Qgf6#', 'Qcxe2#', 'Rfxc2#', 'Nxh8#', 'R5e7#', 'gxh4#', 'Qbd7#', 'Qcf8#', 'Rbc5#', 'Q1d3#', 'Bxe2#', 'R3f7#', 'Qac6#', 'Raxa6#', 'Rfb1#', 'Rgxg6#', 'Qdxg7#', 'Qda6#', 'Qda1#', 'Qeh7#', 'Ndf4#', 'Rbxc8#', 'Rdxd1#', 'Q1e4#', 'Rac1#', 'Rff1#', 'Rbxe1#', 'Qbxe5#', 'Qhe8#', 'Qdg7#', 'Raxe3#', 'Rdxc8#', 'R7g2#', 'R8d4#', 'Qbxe8#', 'R6g5#', 'Kd4#', 'Qcg8#', 'Bxh2#', 'R3d5#', 'Qff4#', 'Raxb1#', 'Qff2#', 'bxa7#', 'cxd4#', 'Qfc7#', 'Q5c3#', 'Rge7#', 'Q1g2#', 'Qfe7#', 'Qcc7#', 'Rgf4#', 'Qed4#', 'Rcxe8#', 'Qgc5#', 'Qef1#', 'Qdg1#', 'Qbxg8#', 'Qef2#', 'R5h2#', 'Qdh2#', 'Nef6#', 'R6g2#', 'Qfxa3#', 'Reb1#', 'Qee6#', 'Qeh4#', 'Rbd1#', 'Rfg3#', 'Rbxd7#', 'Q8b4#', 'Red7#', 'Qdb1#', 'Qea7#', 'Rdg8#', 'Rexc8#', 'Ndc3#', 'axb7#', 'Qdh5#', 'Qbb7#', 'Qhe1#', 'Qdxd3#', 'R8c5#', 'exd4#', 'Rdxd3#', 'dxe3#', 'Rdxa3#', 'Qfxf8#', 'Qed1#', 'R2c7#', 'Q7h7#', 'Q2f5#', 'Bxa7#', 'Rgxf1#', 'Q4b6#', 'Ke2#', 'Qcb4#', 'R1g6#', 'Rdxd2#', 'Qcxb2#', 'Nhg5#', 'Rhxd8#', 'R4e3#', 'Qexd8#', 'Q5c6#', 'Q1h4#', 'Nba4#', 'Nxh1#', 'Qag2#', 'R7xh6#', 'Qch1#', 'Rexh2#', 'Rhf6#', 'Q1d7#', 'Qhh3#', 'Qeg1#', 'Qbe8#', 'Q3a3#', 'Rab1#', 'Bxf8#', 'Qfc2#', 'Rbxd8#', 'R8e6#', 'Rgxg7#', 'Qgxg2#', 'Qfc8#', 'Qda2#', 'Qae2#', 'Qah8#', 'Qhh2#', 'R8b4#', 'Q8a6#', 'Qcc5#', 'Qaa7#', 'Rgd7#', 'Qab8#', 'Q1xg2#', 'Nec4#', 'Rfd8#', 'Qec2#', 'Ngf7#', 'fxe6#', 'Qhf2#', 'Qexa6#', 'Rcxf1#', 'Rdd8#', 'Qce3#', 'Q4f5#', 'R7e6#', 'Qhf3#', 'Qbg4#', 'Q5g6#', 'Qcxe7#', 'R4d3#', 'Qcd3#', 'Q5h2#', 'Ba7#', 'Rdb2#', 'R5e6#', 'gxh8=Q#', 'Qfg8#', 'cxb5#', 'Qaxg7#', 'Qbxd8#', 'Q5xc6#', 'Qcb3#', 'Rdb1#', 'dxe6#', 'Rexf3#', 'Qdd2#', 'Qea3#', 'hxg8=R#', 'Nde4#', 'Qcf2#', 'Q4xg5#', 'Qgc1#', 'Qgxd4#', 'R6c7#', 'Qbf1#', 'Nxa1#', 'Raxa2#', 'Rde7#', 'R7f5#', 'Qah1#', 'Raxd8#', 'R8xa4#', 'Qcxb7#', 'Qha7#', 'Qaxc5#', 'Nef7#', 'Qcxe5#', 'Qbc1#', 'Qaf7#', 'Rhb6#', 'R8h4#', 'R3b7#', 'R5c7#', 'Qgb6#', 'Qdxb4#', 'Q8d5#', 'Qhxc1#', 'Qaxg2#', 'Rcxd5#', 'Qce1#', 'N3d5#', 'R3c6#', 'Qhxc6#', 'Qgxe3#', 'Qbxb8#', 'Qcxf7#', 'Qgxh2#', 'Qab2#', 'Rfxf3#', 'Q5e6#', 'Rgf7#', 'Qhxf1#', 'R5g2#', 'Qhxg1#', 'Qaf1#', 'Q4a5#', 'c8=R#', 'N7f6#', 'Qexf3#', 'Qgb2#', 'Rfb8#', 'R1h6#', 'bxc5#', 'R7h6#', 'Qdb5#', 'R2g5#', 'Rcxb8#', 'Qhd1#', 'Qha8#', 'Rhc4#', 'Rdxf8#', 'R4f3#', 'Qeb5#', 'Qbxd3#', 'Rfxh2#', 'Q8a2#', 'Qbxd4#', 'Raxf1#', 'Q8c8#', 'R3b5#', 'Qbe1#', 'R5e4#', 'Rad7#', 'R8f4#', 'Bxe8#', 'Rhxg8#', 'cxb3#', 'Qhg3#', 'Nbxa7#', 'Qeb3#', 'Qbc4#', 'Q1e1#', 'Qexb7#', 'Q8g7#', 'Rfxc7#', 'Rhxh6#', 'Rexf2#', 'Kf7#', 'Rec5#', 'Ndf6#', 'R7d4#', 'Qdf1#', 'Rhg7#', 'Ne1#', 'Qgd3#', 'R1b5#', 'Raxe6#', 'Nfd5#', 'Qbh6#', 'Qed6#', 'Rde1#', 'Qaf2#', 'Rdxd6#', 'R4e6#', 'Qfxg2#', 'Qfxg3#', 'Nxf8#', 'Qdxc1#', 'Kh2#', 'Qdxd1#', 'Qhg4#', 'Rdf4#', 'Qcb2#', 'R8f5#', 'exd6#', 'Reg1#', 'Rhxd1#', 'Q1a1#', 'exd5#', 'Q4c7#', 'gxh7#', 'a8=N#', 'Q8xe7#', 'Q1b5#', 'R1c3#', 'Qdg3#', 'Rbxe8#', 'Bb1#', 'Qfc6#', 'Qdxc3#', 'Rcd2#', 'Ngxh7#', 'Qaa5#', 'Rge1#', 'Rfxa7#', 'Rcd8#', 'R2f5#', 'Qhh7#', 'Rfg4#', 'Rgb6#', 'Qaxg4#', 'Ree1#', 'R4c7#', 'Qae1#', 'O-O#', 'bxc6#', 'Qdh7#', 'R4xe7#', 'Q5f7#', 'Bxb8#', 'Q8h5#', 'Nxg8#', 'Qhg8#', 'Raxa5#', 'Rexh7#', 'Qbxh6#', 'R5f6#', 'Q5e4#', 'Qcxd1#', 'Qch6#', 'Qcd7#', 'Qfa6#', 'Qdxf1#', 'R8xd3#', 'Qdxb2#', 'Nbc7#', 'Rhxf2#', 'Rgxg8#', 'Qgxg7#', 'Rfxg6#', 'Qgd4#', 'Qexf2#', 'Qgc4#', 'Qexg6#', 'Reb3#', 'R6f2#', 'Qfb1#', 'bxa1=Q#', 'Ref7#', 'Qde3#', 'Qab3#', 'Rag4#', 'Qdf2#', 'Qhxh2#', 'Q8xh2#', 'Q4g1#', 'N5xg6#', 'Qexd6#', 'Rhxf1#', 'Qad6#', 'Qfxf3#', 'Q3xf4#', 'Qad1#', 'Qexg2#', 'Rexe7#', 'R5f3#', 'R7f3#', 'Qcxb3#', 'Rhxd7#', 'Q1e3#', 'Qbg7#', 'Q6f6#', 'Qgf7#', 'R4f6#', 'Qfe6#', 'Rexe4#', 'Q8d6#', 'Qhxe6#', 'Rab8#', 'Qad4#', 'Qdd4#', 'Qfb3#', 'Rdf3#', 'Nxf1#', 'Bxe1#', 'Qdxc2#', 'R4e5#', 'Qcxc4#', 'Nexf2#', 'Rfxb2#', 'Qhc3#', 'Q7g6#', 'Qcxf6#', 'Nce4#', 'Qexh5#', 'Qbe3#', 'Qga8#', 'Qfe5#', 'Qha1#', 'Bxa5#', 'Q6xf6#', 'Qcd4#', 'Qdxg3#', 'Qfb2#', 'dxc5#', 'Qfc3#', 'Qbd4#', 'Rexh4#', 'dxc8=R#'])\n"
     ]
    }
   ],
   "source": [
    "print(vocab.move_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 3.164531381011009 | Training Accuracy: 0.06498046875\n",
      "Epoch 1, Batch: 2000| Training Loss: 1.965106923520565 | Training Accuracy: 0.075517578125\n",
      "Epoch 1, Batch: 3000| Training Loss: 1.4816381523311137 | Training Accuracy: 0.08402864583333333\n",
      "Epoch 1, Batch: 4000| Training Loss: 1.2242640992924572 | Training Accuracy: 0.092001953125\n",
      "Epoch 1, Batch: 5000| Training Loss: 1.0634873887717724 | Training Accuracy: 0.09899296875\n",
      "Epoch 1, Batch: 6000| Training Loss: 0.9529955608894428 | Training Accuracy: 0.10500911458333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[16], line 143\u001b[0m, in \u001b[0;36mtrain_decoder\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay)\u001b[0m\n\u001b[1;32m    140\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# For logging purposes\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m    145\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_decoder(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('training_results/transformer-exp-3-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_images/seq-transformer-exp-4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenrt 5 (512 dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39521912\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=3,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 512\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 1e-4\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.8 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 3.3118682713508605 | Training Accuracy: 0.064578125\n",
      "Epoch 1, Batch: 2000| Training Loss: 2.1279233896136285 | Training Accuracy: 0.07708984375\n",
      "Epoch 1, Batch: 3000| Training Loss: 1.592337130745252 | Training Accuracy: 0.08634895833333334\n",
      "Epoch 1, Batch: 4000| Training Loss: 1.3039989030361177 | Training Accuracy: 0.095490234375\n",
      "Epoch 1, Batch: 5000| Training Loss: 1.1243414255440236 | Training Accuracy: 0.1036328125\n",
      "Epoch 1, Batch: 6000| Training Loss: 1.0012873452206452 | Training Accuracy: 0.11042317708333334\n",
      "Epoch 1, Batch: 7000| Training Loss: 0.9115567641598837 | Training Accuracy: 0.11628013392857142\n",
      "Epoch 1, Batch: 8000| Training Loss: 0.8430284378342331 | Training Accuracy: 0.12156640625\n",
      "Epoch 1, Batch: 9000| Training Loss: 0.7888814149432712 | Training Accuracy: 0.1261935763888889\n",
      "Epoch 1, Batch: 10000| Training Loss: 0.7449112012445926 | Training Accuracy: 0.1305109375\n",
      "Epoch 1, Batch: 11000| Training Loss: 0.7084411645612934 | Training Accuracy: 0.13442720170454545\n",
      "Epoch 1, Batch: 12000| Training Loss: 0.6775954652850826 | Training Accuracy: 0.13788020833333334\n",
      "Epoch 1, Batch: 13000| Training Loss: 0.651240722990953 | Training Accuracy: 0.14111298076923076\n",
      "Epoch 1, Batch: 14000| Training Loss: 0.6283683028434004 | Training Accuracy: 0.14419921875\n",
      "Epoch 1, Batch: 15000| Training Loss: 0.608372875525554 | Training Accuracy: 0.1469546875\n",
      "Epoch 1, Batch: 16000| Training Loss: 0.5906625918634236 | Training Accuracy: 0.14944140625\n",
      "Epoch 1, Batch: 17000| Training Loss: 0.5748408087106313 | Training Accuracy: 0.15191130514705883\n",
      "Epoch 1, Batch: 18000| Training Loss: 0.5606272215644519 | Training Accuracy: 0.15431640625\n",
      "Epoch 1, Batch: 19000| Training Loss: 0.5478262218064384 | Training Accuracy: 0.1565067845394737\n",
      "Epoch 1, Batch: 20000| Training Loss: 0.5361875135540962 | Training Accuracy: 0.158558984375\n",
      "Epoch 1, Batch: 21000| Training Loss: 0.5255392406682173 | Training Accuracy: 0.16052176339285715\n",
      "Epoch 1, Batch: 22000| Training Loss: 0.5157546990581534 | Training Accuracy: 0.1624131747159091\n",
      "Epoch 1, Batch: 23000| Training Loss: 0.5067715477515822 | Training Accuracy: 0.16418800951086956\n",
      "Epoch 1, Batch: 24000| Training Loss: 0.4984524842947721 | Training Accuracy: 0.16591048177083334\n",
      "Epoch 1, Batch: 25000| Training Loss: 0.4907316128766537 | Training Accuracy: 0.16750734375\n",
      "Epoch 1, Batch: 26000| Training Loss: 0.4835650912580582 | Training Accuracy: 0.16906956129807693\n",
      "Epoch 1, Batch: 27000| Training Loss: 0.476866201106045 | Training Accuracy: 0.17054224537037038\n",
      "Epoch 1, Batch: 28000| Training Loss: 0.47061259910570724 | Training Accuracy: 0.17193233816964285\n",
      "Epoch 1, Batch: 29000| Training Loss: 0.4647041785172347 | Training Accuracy: 0.17337594288793104\n",
      "Epoch 1, Batch: 30000| Training Loss: 0.4591685735603174 | Training Accuracy: 0.17472916666666666\n",
      "Epoch 1, Batch: 31000| Training Loss: 0.453947449099633 | Training Accuracy: 0.17599029737903227\n",
      "Epoch 1, Batch: 32000| Training Loss: 0.4490194661971182 | Training Accuracy: 0.177241943359375\n",
      "Epoch 1, Batch: 33000| Training Loss: 0.44435442228570127 | Training Accuracy: 0.17844093276515152\n",
      "Epoch 1, Batch: 34000| Training Loss: 0.4398977817077847 | Training Accuracy: 0.17964878216911764\n",
      "Epoch 1, Batch: 35000| Training Loss: 0.43572176045264516 | Training Accuracy: 0.18074174107142857\n",
      "Epoch 1, Batch: 36000| Training Loss: 0.4317115848718418 | Training Accuracy: 0.18183441840277778\n",
      "Epoch 1, Batch: 37000| Training Loss: 0.4279040637507632 | Training Accuracy: 0.18289959881756757\n",
      "Epoch 1, Batch: 38000| Training Loss: 0.4242777638874556 | Training Accuracy: 0.183953125\n",
      "Epoch 1, Batch: 39000| Training Loss: 0.4208056623400786 | Training Accuracy: 0.1849601362179487\n",
      "Epoch 1, Batch: 40000| Training Loss: 0.4174985802832991 | Training Accuracy: 0.18593623046875\n",
      "Epoch 1, Batch: 41000| Training Loss: 0.4143100646192708 | Training Accuracy: 0.18687147484756098\n",
      "Epoch 1, Batch: 42000| Training Loss: 0.411264076121861 | Training Accuracy: 0.18779892113095237\n",
      "Epoch 1, Batch: 43000| Training Loss: 0.4083297820995713 | Training Accuracy: 0.18870257994186046\n",
      "Epoch 1, Batch: 44000| Training Loss: 0.40549884786626156 | Training Accuracy: 0.18956001420454546\n",
      "Epoch 1, Batch: 45000| Training Loss: 0.4027865356316169 | Training Accuracy: 0.19043072916666667\n",
      "Epoch 1, Batch: 46000| Training Loss: 0.40017023637210547 | Training Accuracy: 0.19127088994565217\n",
      "Epoch 1, Training Loss: 0.39903408464657864, Validation Error: 75.17783143537704, Validation Top-3 Accuracy: 0.0, Training Error: 80.83676771135785\n",
      "Epoch 2, Batch: 1000| Training Loss: 0.2773651039749384 | Training Accuracy: 0.23379296875\n",
      "Epoch 2, Batch: 2000| Training Loss: 0.27698012014478446 | Training Accuracy: 0.23460546875\n",
      "Epoch 2, Batch: 3000| Training Loss: 0.2766060487429301 | Training Accuracy: 0.23517708333333334\n",
      "Epoch 2, Batch: 4000| Training Loss: 0.276256703030318 | Training Accuracy: 0.235873046875\n",
      "Epoch 2, Batch: 5000| Training Loss: 0.2759286107629538 | Training Accuracy: 0.23612109375\n",
      "Epoch 2, Batch: 6000| Training Loss: 0.27562648767481246 | Training Accuracy: 0.23624934895833333\n",
      "Epoch 2, Batch: 7000| Training Loss: 0.2752938643246889 | Training Accuracy: 0.2367265625\n",
      "Epoch 2, Batch: 8000| Training Loss: 0.274966062201187 | Training Accuracy: 0.23700537109375\n",
      "Epoch 2, Batch: 9000| Training Loss: 0.27469703261885375 | Training Accuracy: 0.23715711805555556\n",
      "Epoch 2, Batch: 10000| Training Loss: 0.27443650519102814 | Training Accuracy: 0.237458984375\n",
      "Epoch 2, Batch: 11000| Training Loss: 0.27411819676106625 | Training Accuracy: 0.23778160511363636\n",
      "Epoch 2, Batch: 12000| Training Loss: 0.2738215958500902 | Training Accuracy: 0.23805891927083334\n",
      "Epoch 2, Batch: 13000| Training Loss: 0.27355658007699707 | Training Accuracy: 0.23830709134615385\n",
      "Epoch 2, Batch: 14000| Training Loss: 0.2733187439367175 | Training Accuracy: 0.23855301339285714\n",
      "Epoch 2, Batch: 15000| Training Loss: 0.27304332147041954 | Training Accuracy: 0.23883984375\n",
      "Epoch 2, Batch: 16000| Training Loss: 0.2727581949364394 | Training Accuracy: 0.23912841796875\n",
      "Epoch 2, Batch: 17000| Training Loss: 0.2724758965145139 | Training Accuracy: 0.23941015625\n",
      "Epoch 2, Batch: 18000| Training Loss: 0.27222661374840473 | Training Accuracy: 0.23964952256944444\n",
      "Epoch 2, Batch: 19000| Training Loss: 0.2719570863325345 | Training Accuracy: 0.23999033717105264\n",
      "Epoch 2, Batch: 20000| Training Loss: 0.27166831294596194 | Training Accuracy: 0.2403626953125\n",
      "Epoch 2, Batch: 21000| Training Loss: 0.27144183932670524 | Training Accuracy: 0.24063020833333335\n",
      "Epoch 2, Batch: 22000| Training Loss: 0.2712066570906477 | Training Accuracy: 0.24089311079545456\n",
      "Epoch 2, Batch: 23000| Training Loss: 0.2709839829277733 | Training Accuracy: 0.2411477581521739\n",
      "Epoch 2, Batch: 24000| Training Loss: 0.2707358979483446 | Training Accuracy: 0.24145100911458334\n",
      "Epoch 2, Batch: 25000| Training Loss: 0.27051417509675024 | Training Accuracy: 0.24166140625\n",
      "Epoch 2, Batch: 26000| Training Loss: 0.2702811240112552 | Training Accuracy: 0.24197205528846155\n",
      "Epoch 2, Batch: 27000| Training Loss: 0.27003301307338257 | Training Accuracy: 0.24224927662037038\n",
      "Epoch 2, Batch: 28000| Training Loss: 0.2698056433812848 | Training Accuracy: 0.2424926060267857\n",
      "Epoch 2, Batch: 29000| Training Loss: 0.26958532256710116 | Training Accuracy: 0.24270083512931034\n",
      "Epoch 2, Batch: 30000| Training Loss: 0.26934951794395845 | Training Accuracy: 0.24296705729166668\n",
      "Epoch 2, Batch: 31000| Training Loss: 0.26912577519830194 | Training Accuracy: 0.2432195060483871\n",
      "Epoch 2, Batch: 32000| Training Loss: 0.2688959095566533 | Training Accuracy: 0.2434471435546875\n",
      "Epoch 2, Batch: 33000| Training Loss: 0.2686765370666981 | Training Accuracy: 0.24374846117424243\n",
      "Epoch 2, Batch: 34000| Training Loss: 0.26848422751488055 | Training Accuracy: 0.2439563419117647\n",
      "Epoch 2, Batch: 35000| Training Loss: 0.2682853005771126 | Training Accuracy: 0.24418236607142857\n",
      "Epoch 2, Batch: 36000| Training Loss: 0.26806925196862885 | Training Accuracy: 0.2444150390625\n",
      "Epoch 2, Batch: 37000| Training Loss: 0.2678722331922602 | Training Accuracy: 0.24462246621621622\n",
      "Epoch 2, Batch: 38000| Training Loss: 0.2676729907409141 | Training Accuracy: 0.2448548519736842\n",
      "Epoch 2, Batch: 39000| Training Loss: 0.26747104200338706 | Training Accuracy: 0.24512339743589742\n",
      "Epoch 2, Batch: 40000| Training Loss: 0.2672778565403074 | Training Accuracy: 0.2453294921875\n",
      "Epoch 2, Batch: 41000| Training Loss: 0.26708018194938576 | Training Accuracy: 0.24555259146341463\n",
      "Epoch 2, Batch: 42000| Training Loss: 0.26688498027303387 | Training Accuracy: 0.24577036830357143\n",
      "Epoch 2, Batch: 43000| Training Loss: 0.2666780734800322 | Training Accuracy: 0.24602025799418606\n",
      "Epoch 2, Batch: 44000| Training Loss: 0.26648850337686864 | Training Accuracy: 0.24624174360795453\n",
      "Epoch 2, Batch: 45000| Training Loss: 0.26629202523761325 | Training Accuracy: 0.24647604166666667\n",
      "Epoch 2, Batch: 46000| Training Loss: 0.2660941894786513 | Training Accuracy: 0.2466896229619565\n",
      "Epoch 2, Training Loss: 0.26600107914953625, Validation Error: 72.3904460469214, Validation Top-3 Accuracy: 0.0, Training Error: 75.31959772743213\n",
      "Epoch 3, Batch: 1000| Training Loss: 0.2528854096084833 | Training Accuracy: 0.2612109375\n",
      "Epoch 3, Batch: 2000| Training Loss: 0.2528936255574226 | Training Accuracy: 0.2616171875\n",
      "Epoch 3, Batch: 3000| Training Loss: 0.2525084359596173 | Training Accuracy: 0.26195963541666667\n",
      "Epoch 3, Batch: 4000| Training Loss: 0.25218564094975593 | Training Accuracy: 0.262421875\n",
      "Epoch 3, Batch: 5000| Training Loss: 0.251980495506525 | Training Accuracy: 0.26290390625\n",
      "Epoch 3, Batch: 6000| Training Loss: 0.2519686895683408 | Training Accuracy: 0.2628815104166667\n",
      "Epoch 3, Batch: 7000| Training Loss: 0.25173322792351244 | Training Accuracy: 0.2631573660714286\n",
      "Epoch 3, Batch: 8000| Training Loss: 0.2515626234170049 | Training Accuracy: 0.26328515625\n",
      "Epoch 3, Batch: 9000| Training Loss: 0.2514568259666363 | Training Accuracy: 0.26329557291666666\n",
      "Epoch 3, Batch: 10000| Training Loss: 0.2513590115621686 | Training Accuracy: 0.26343125\n",
      "Epoch 3, Batch: 11000| Training Loss: 0.2512230664315549 | Training Accuracy: 0.2635788352272727\n",
      "Epoch 3, Batch: 12000| Training Loss: 0.251056555001686 | Training Accuracy: 0.26371419270833335\n",
      "Epoch 3, Batch: 13000| Training Loss: 0.25098435458999413 | Training Accuracy: 0.26383864182692307\n",
      "Epoch 3, Batch: 14000| Training Loss: 0.2508695305128183 | Training Accuracy: 0.26399358258928574\n",
      "Epoch 3, Batch: 15000| Training Loss: 0.2507409484118223 | Training Accuracy: 0.26409635416666666\n",
      "Epoch 3, Batch: 16000| Training Loss: 0.25061328380089254 | Training Accuracy: 0.264215087890625\n",
      "Epoch 3, Batch: 17000| Training Loss: 0.25048615490513687 | Training Accuracy: 0.2643768382352941\n",
      "Epoch 3, Batch: 18000| Training Loss: 0.250370372099181 | Training Accuracy: 0.2645275607638889\n",
      "Epoch 3, Batch: 19000| Training Loss: 0.25025918568513894 | Training Accuracy: 0.26472635690789476\n",
      "Epoch 3, Batch: 20000| Training Loss: 0.25016764885783194 | Training Accuracy: 0.264851171875\n",
      "Epoch 3, Batch: 21000| Training Loss: 0.2500619192840088 | Training Accuracy: 0.2649551711309524\n",
      "Epoch 3, Batch: 22000| Training Loss: 0.24994986605982888 | Training Accuracy: 0.26508842329545457\n",
      "Epoch 3, Batch: 23000| Training Loss: 0.24980327611578548 | Training Accuracy: 0.2653033288043478\n",
      "Epoch 3, Batch: 24000| Training Loss: 0.24969042663648724 | Training Accuracy: 0.265466796875\n",
      "Epoch 3, Batch: 25000| Training Loss: 0.24958900342285634 | Training Accuracy: 0.26561609375\n",
      "Epoch 3, Batch: 26000| Training Loss: 0.24946829358774883 | Training Accuracy: 0.26575646033653844\n",
      "Epoch 3, Batch: 27000| Training Loss: 0.24936707445482412 | Training Accuracy: 0.265877025462963\n",
      "Epoch 3, Batch: 28000| Training Loss: 0.24926489778341993 | Training Accuracy: 0.26600864955357145\n",
      "Epoch 3, Batch: 29000| Training Loss: 0.24916840989168348 | Training Accuracy: 0.2660848599137931\n",
      "Epoch 3, Batch: 30000| Training Loss: 0.24905184503644706 | Training Accuracy: 0.266233203125\n",
      "Epoch 3, Batch: 31000| Training Loss: 0.24895417909997125 | Training Accuracy: 0.26636554939516127\n",
      "Epoch 3, Batch: 32000| Training Loss: 0.2488640922657214 | Training Accuracy: 0.266470703125\n",
      "Epoch 3, Batch: 33000| Training Loss: 0.24877491122032658 | Training Accuracy: 0.2665764678030303\n",
      "Epoch 3, Batch: 34000| Training Loss: 0.24868206168623533 | Training Accuracy: 0.266689453125\n",
      "Epoch 3, Batch: 35000| Training Loss: 0.24857522403895854 | Training Accuracy: 0.2668535714285714\n",
      "Epoch 3, Batch: 36000| Training Loss: 0.2484660921730101 | Training Accuracy: 0.26697081163194447\n",
      "Epoch 3, Batch: 37000| Training Loss: 0.2483583408529694 | Training Accuracy: 0.2670994510135135\n",
      "Epoch 3, Batch: 38000| Training Loss: 0.2482640461498185 | Training Accuracy: 0.26722337582236844\n",
      "Epoch 3, Batch: 39000| Training Loss: 0.24816832987582071 | Training Accuracy: 0.26737059294871796\n",
      "Epoch 3, Batch: 40000| Training Loss: 0.24806412885785104 | Training Accuracy: 0.2674939453125\n",
      "Epoch 3, Batch: 41000| Training Loss: 0.2479664444349161 | Training Accuracy: 0.26761880716463415\n",
      "Epoch 3, Batch: 42000| Training Loss: 0.24786492942947716 | Training Accuracy: 0.26772526041666667\n",
      "Epoch 3, Batch: 43000| Training Loss: 0.24775479364741682 | Training Accuracy: 0.26783929869186046\n",
      "Epoch 3, Batch: 44000| Training Loss: 0.24765734221718527 | Training Accuracy: 0.26795472301136364\n",
      "Epoch 3, Batch: 45000| Training Loss: 0.2475616542223427 | Training Accuracy: 0.26807578125\n",
      "Epoch 3, Batch: 46000| Training Loss: 0.24747338666702096 | Training Accuracy: 0.26818325407608695\n",
      "Epoch 3, Training Loss: 0.24742142385282864, Validation Error: 70.58113355303945, Validation Top-3 Accuracy: 0.0, Training Error: 73.17480671584542\n",
      "Epoch 4, Batch: 1000| Training Loss: 0.23861803895235062 | Training Accuracy: 0.280671875\n",
      "Epoch 4, Batch: 2000| Training Loss: 0.23866606801003218 | Training Accuracy: 0.279796875\n",
      "Epoch 4, Batch: 3000| Training Loss: 0.23869472733139993 | Training Accuracy: 0.2790403645833333\n",
      "Epoch 4, Batch: 4000| Training Loss: 0.2387636906914413 | Training Accuracy: 0.279365234375\n",
      "Epoch 4, Batch: 5000| Training Loss: 0.23866804239451886 | Training Accuracy: 0.27973515625\n",
      "Epoch 4, Batch: 6000| Training Loss: 0.23851680152863264 | Training Accuracy: 0.2799108072916667\n",
      "Epoch 4, Batch: 7000| Training Loss: 0.23840545755411896 | Training Accuracy: 0.28009095982142856\n",
      "Epoch 4, Batch: 8000| Training Loss: 0.23831010372005404 | Training Accuracy: 0.280111328125\n",
      "Epoch 4, Batch: 9000| Training Loss: 0.23816020079784922 | Training Accuracy: 0.28031380208333334\n",
      "Epoch 4, Batch: 10000| Training Loss: 0.23809849883019923 | Training Accuracy: 0.280278125\n",
      "Epoch 4, Batch: 11000| Training Loss: 0.23803227904168042 | Training Accuracy: 0.2803689630681818\n",
      "Epoch 4, Batch: 12000| Training Loss: 0.23795949551090598 | Training Accuracy: 0.2803707682291667\n",
      "Epoch 4, Batch: 13000| Training Loss: 0.23789952452824667 | Training Accuracy: 0.28043359375\n",
      "Epoch 4, Batch: 14000| Training Loss: 0.23784280963987112 | Training Accuracy: 0.2805083705357143\n",
      "Epoch 4, Batch: 15000| Training Loss: 0.23780785718262196 | Training Accuracy: 0.28048203125\n",
      "Epoch 4, Batch: 16000| Training Loss: 0.23778721044305712 | Training Accuracy: 0.28054296875\n",
      "Epoch 4, Batch: 17000| Training Loss: 0.23770282288421604 | Training Accuracy: 0.28062017463235295\n",
      "Epoch 4, Batch: 18000| Training Loss: 0.23763729314919976 | Training Accuracy: 0.2806931423611111\n",
      "Epoch 4, Batch: 19000| Training Loss: 0.2375472323031802 | Training Accuracy: 0.2808112664473684\n",
      "Epoch 4, Batch: 20000| Training Loss: 0.2374776607118547 | Training Accuracy: 0.28095625\n",
      "Epoch 4, Batch: 21000| Training Loss: 0.2374063366005818 | Training Accuracy: 0.28101655505952383\n",
      "Epoch 4, Batch: 22000| Training Loss: 0.23734256829930978 | Training Accuracy: 0.281130859375\n",
      "Epoch 4, Batch: 23000| Training Loss: 0.2372895672360192 | Training Accuracy: 0.2812525475543478\n",
      "Epoch 4, Batch: 24000| Training Loss: 0.23723381448164582 | Training Accuracy: 0.28133544921875\n",
      "Epoch 4, Batch: 25000| Training Loss: 0.23718707419753074 | Training Accuracy: 0.28136140625\n",
      "Epoch 4, Batch: 26000| Training Loss: 0.23711281146854163 | Training Accuracy: 0.28143419471153847\n",
      "Epoch 4, Batch: 27000| Training Loss: 0.23706499402776912 | Training Accuracy: 0.2814486400462963\n",
      "Epoch 4, Batch: 28000| Training Loss: 0.23699609249510936 | Training Accuracy: 0.28153864397321426\n",
      "Epoch 4, Batch: 29000| Training Loss: 0.23691379850126545 | Training Accuracy: 0.281678744612069\n",
      "Epoch 4, Batch: 30000| Training Loss: 0.2368675858810544 | Training Accuracy: 0.28173671875\n",
      "Epoch 4, Batch: 31000| Training Loss: 0.23681342668398733 | Training Accuracy: 0.28178553427419356\n",
      "Epoch 4, Batch: 32000| Training Loss: 0.23674094066442922 | Training Accuracy: 0.2818426513671875\n",
      "Epoch 4, Batch: 33000| Training Loss: 0.2366890855110956 | Training Accuracy: 0.28191903409090907\n",
      "Epoch 4, Batch: 34000| Training Loss: 0.2366258300818941 | Training Accuracy: 0.2819679457720588\n",
      "Epoch 4, Batch: 35000| Training Loss: 0.23659345496211734 | Training Accuracy: 0.2819930803571429\n",
      "Epoch 4, Batch: 36000| Training Loss: 0.23653602696996595 | Training Accuracy: 0.28206705729166665\n",
      "Epoch 4, Batch: 37000| Training Loss: 0.23648994943257926 | Training Accuracy: 0.2821410472972973\n",
      "Epoch 4, Batch: 38000| Training Loss: 0.2364356935302678 | Training Accuracy: 0.2822177220394737\n",
      "Epoch 4, Batch: 39000| Training Loss: 0.2363720774077452 | Training Accuracy: 0.28229296875\n",
      "Epoch 4, Batch: 40000| Training Loss: 0.23631951416544617 | Training Accuracy: 0.28231591796875\n",
      "Epoch 4, Batch: 41000| Training Loss: 0.23626513296656493 | Training Accuracy: 0.28242435213414635\n",
      "Epoch 4, Batch: 42000| Training Loss: 0.23620511069255215 | Training Accuracy: 0.28252269345238096\n",
      "Epoch 4, Batch: 43000| Training Loss: 0.23615642901250097 | Training Accuracy: 0.2825750363372093\n",
      "Epoch 4, Batch: 44000| Training Loss: 0.23610083356025544 | Training Accuracy: 0.28264914772727273\n",
      "Epoch 4, Batch: 45000| Training Loss: 0.23603955898649162 | Training Accuracy: 0.2827371527777778\n",
      "Epoch 4, Batch: 46000| Training Loss: 0.23597322568038234 | Training Accuracy: 0.28282362432065217\n",
      "Epoch 4, Training Loss: 0.23594896002689655, Validation Error: 69.25310797141645, Validation Top-3 Accuracy: 0.0, Training Error: 71.71358910346125\n",
      "Epoch 5, Batch: 1000| Training Loss: 0.22976380167901517 | Training Accuracy: 0.2919296875\n",
      "Epoch 5, Batch: 2000| Training Loss: 0.22956183184683324 | Training Accuracy: 0.291875\n",
      "Epoch 5, Batch: 3000| Training Loss: 0.22958329378068448 | Training Accuracy: 0.29177473958333333\n",
      "Epoch 5, Batch: 4000| Training Loss: 0.229456232201308 | Training Accuracy: 0.291787109375\n",
      "Epoch 5, Batch: 5000| Training Loss: 0.22937000287473203 | Training Accuracy: 0.29174765625\n",
      "Epoch 5, Batch: 6000| Training Loss: 0.22919747013847033 | Training Accuracy: 0.29203515625\n",
      "Epoch 5, Batch: 7000| Training Loss: 0.2291247781195811 | Training Accuracy: 0.29223828125\n",
      "Epoch 5, Batch: 8000| Training Loss: 0.2291212564893067 | Training Accuracy: 0.2921591796875\n",
      "Epoch 5, Batch: 9000| Training Loss: 0.22909984134634337 | Training Accuracy: 0.29214105902777776\n",
      "Epoch 5, Batch: 10000| Training Loss: 0.2290421780720353 | Training Accuracy: 0.2922265625\n",
      "Epoch 5, Batch: 11000| Training Loss: 0.22901353676752612 | Training Accuracy: 0.29228870738636364\n",
      "Epoch 5, Batch: 12000| Training Loss: 0.2289776697245737 | Training Accuracy: 0.29239029947916667\n",
      "Epoch 5, Batch: 13000| Training Loss: 0.22896159447271092 | Training Accuracy: 0.29247776442307694\n",
      "Epoch 5, Batch: 14000| Training Loss: 0.2289580305750881 | Training Accuracy: 0.2923803013392857\n",
      "Epoch 5, Batch: 15000| Training Loss: 0.2289162073045969 | Training Accuracy: 0.29244036458333333\n",
      "Epoch 5, Batch: 16000| Training Loss: 0.22887992257252335 | Training Accuracy: 0.292552734375\n",
      "Epoch 5, Batch: 17000| Training Loss: 0.22885996683467838 | Training Accuracy: 0.2926256893382353\n",
      "Epoch 5, Batch: 18000| Training Loss: 0.2288070670126213 | Training Accuracy: 0.29272265625\n",
      "Epoch 5, Batch: 19000| Training Loss: 0.2287623739948398 | Training Accuracy: 0.29271833881578946\n",
      "Epoch 5, Batch: 20000| Training Loss: 0.22872513228207828 | Training Accuracy: 0.2927755859375\n",
      "Epoch 5, Batch: 21000| Training Loss: 0.2286886363497802 | Training Accuracy: 0.29280970982142857\n",
      "Epoch 5, Batch: 22000| Training Loss: 0.22866666074706749 | Training Accuracy: 0.2928096590909091\n",
      "Epoch 5, Batch: 23000| Training Loss: 0.22863433234069658 | Training Accuracy: 0.2928447690217391\n",
      "Epoch 5, Batch: 24000| Training Loss: 0.2286061364337802 | Training Accuracy: 0.2928626302083333\n",
      "Epoch 5, Batch: 25000| Training Loss: 0.22858096324086188 | Training Accuracy: 0.29287546875\n",
      "Epoch 5, Batch: 26000| Training Loss: 0.22856442674421348 | Training Accuracy: 0.29287950721153844\n",
      "Epoch 5, Batch: 27000| Training Loss: 0.22853598457905983 | Training Accuracy: 0.2929375\n",
      "Epoch 5, Batch: 28000| Training Loss: 0.22848118446873766 | Training Accuracy: 0.29299790736607145\n",
      "Epoch 5, Batch: 29000| Training Loss: 0.22843247224082208 | Training Accuracy: 0.29306694504310343\n",
      "Epoch 5, Batch: 30000| Training Loss: 0.22840051411688328 | Training Accuracy: 0.2930903645833333\n",
      "Epoch 5, Batch: 31000| Training Loss: 0.2283674021401713 | Training Accuracy: 0.29311378528225807\n",
      "Epoch 5, Batch: 32000| Training Loss: 0.22834115571156144 | Training Accuracy: 0.293138671875\n",
      "Epoch 5, Batch: 33000| Training Loss: 0.2282934749419942 | Training Accuracy: 0.2932233664772727\n",
      "Epoch 5, Batch: 34000| Training Loss: 0.22827320392210693 | Training Accuracy: 0.29324000459558824\n",
      "Epoch 5, Batch: 35000| Training Loss: 0.2282428731381893 | Training Accuracy: 0.2932972098214286\n",
      "Epoch 5, Batch: 36000| Training Loss: 0.2282096631816692 | Training Accuracy: 0.29335514322916667\n",
      "Epoch 5, Batch: 37000| Training Loss: 0.22818236029188374 | Training Accuracy: 0.2933879856418919\n",
      "Epoch 5, Batch: 38000| Training Loss: 0.22814480672502205 | Training Accuracy: 0.2934221833881579\n",
      "Epoch 5, Batch: 39000| Training Loss: 0.2281158716789423 | Training Accuracy: 0.2934133613782051\n",
      "Epoch 5, Batch: 40000| Training Loss: 0.2280656990658492 | Training Accuracy: 0.29348935546875\n",
      "Epoch 5, Batch: 41000| Training Loss: 0.2280386893978206 | Training Accuracy: 0.2935242949695122\n",
      "Epoch 5, Batch: 42000| Training Loss: 0.22799710130443177 | Training Accuracy: 0.2935506882440476\n",
      "Epoch 5, Batch: 43000| Training Loss: 0.22796531713078189 | Training Accuracy: 0.29357194767441863\n",
      "Epoch 5, Batch: 44000| Training Loss: 0.2279396985986016 | Training Accuracy: 0.29358939985795457\n",
      "Epoch 5, Batch: 45000| Training Loss: 0.2278994897633791 | Training Accuracy: 0.29363359375\n",
      "Epoch 5, Batch: 46000| Training Loss: 0.2278597852613615 | Training Accuracy: 0.2936859714673913\n",
      "Epoch 5, Training Loss: 0.22784120725706397, Validation Error: 68.43777531242863, Validation Top-3 Accuracy: 0.0, Training Error: 70.62804083948237\n",
      "Epoch 6, Batch: 1000| Training Loss: 0.22232275146245956 | Training Accuracy: 0.30286328125\n",
      "Epoch 6, Batch: 2000| Training Loss: 0.22239829083532095 | Training Accuracy: 0.302134765625\n",
      "Epoch 6, Batch: 3000| Training Loss: 0.22231014946103095 | Training Accuracy: 0.30216015625\n",
      "Epoch 6, Batch: 4000| Training Loss: 0.22237505839020014 | Training Accuracy: 0.3019111328125\n",
      "Epoch 6, Batch: 5000| Training Loss: 0.22229253637492657 | Training Accuracy: 0.302\n",
      "Epoch 6, Batch: 6000| Training Loss: 0.2222522908076644 | Training Accuracy: 0.30195182291666667\n",
      "Epoch 6, Batch: 7000| Training Loss: 0.22223787944231715 | Training Accuracy: 0.30189397321428574\n",
      "Epoch 6, Batch: 8000| Training Loss: 0.22225915420241654 | Training Accuracy: 0.30194091796875\n",
      "Epoch 6, Batch: 9000| Training Loss: 0.22230832767486572 | Training Accuracy: 0.3016875\n",
      "Epoch 6, Batch: 10000| Training Loss: 0.22234229599535466 | Training Accuracy: 0.3016859375\n",
      "Epoch 6, Batch: 11000| Training Loss: 0.22231093982268463 | Training Accuracy: 0.3017705965909091\n",
      "Epoch 6, Batch: 12000| Training Loss: 0.22232430712133647 | Training Accuracy: 0.30178125\n",
      "Epoch 6, Batch: 13000| Training Loss: 0.2223027904068048 | Training Accuracy: 0.30185606971153844\n",
      "Epoch 6, Batch: 14000| Training Loss: 0.2223072457505124 | Training Accuracy: 0.30179185267857145\n",
      "Epoch 6, Batch: 15000| Training Loss: 0.22230780723492305 | Training Accuracy: 0.30177005208333335\n",
      "Epoch 6, Batch: 16000| Training Loss: 0.22229004893824458 | Training Accuracy: 0.301772216796875\n",
      "Epoch 6, Batch: 17000| Training Loss: 0.22228638102289508 | Training Accuracy: 0.30177895220588236\n",
      "Epoch 6, Batch: 18000| Training Loss: 0.22224761137945784 | Training Accuracy: 0.30187847222222225\n",
      "Epoch 6, Batch: 19000| Training Loss: 0.22221356238189496 | Training Accuracy: 0.30190501644736845\n",
      "Epoch 6, Batch: 20000| Training Loss: 0.2221790591597557 | Training Accuracy: 0.3018857421875\n",
      "Epoch 6, Batch: 21000| Training Loss: 0.22218766014136018 | Training Accuracy: 0.3018608630952381\n",
      "Epoch 6, Batch: 22000| Training Loss: 0.22216279973089695 | Training Accuracy: 0.30190323153409093\n",
      "Epoch 6, Batch: 23000| Training Loss: 0.2221372284150642 | Training Accuracy: 0.30196518342391304\n",
      "Epoch 6, Batch: 24000| Training Loss: 0.2221071533386906 | Training Accuracy: 0.3019895833333333\n",
      "Epoch 6, Batch: 25000| Training Loss: 0.2221154729038477 | Training Accuracy: 0.3019296875\n",
      "Epoch 6, Batch: 26000| Training Loss: 0.22211221134204132 | Training Accuracy: 0.30198783052884615\n",
      "Epoch 6, Batch: 27000| Training Loss: 0.2221057276400151 | Training Accuracy: 0.3020086805555556\n",
      "Epoch 6, Batch: 28000| Training Loss: 0.22209784036023275 | Training Accuracy: 0.3019893973214286\n",
      "Epoch 6, Batch: 29000| Training Loss: 0.22207129573205422 | Training Accuracy: 0.30201966594827584\n",
      "Epoch 6, Batch: 30000| Training Loss: 0.22205238877683878 | Training Accuracy: 0.3020028645833333\n",
      "Epoch 6, Batch: 31000| Training Loss: 0.22202968592461078 | Training Accuracy: 0.3020108366935484\n",
      "Epoch 6, Batch: 32000| Training Loss: 0.2220160054671578 | Training Accuracy: 0.301971923828125\n",
      "Epoch 6, Batch: 33000| Training Loss: 0.22200844198736278 | Training Accuracy: 0.3020043797348485\n",
      "Epoch 6, Batch: 34000| Training Loss: 0.2219915212064105 | Training Accuracy: 0.3020186121323529\n",
      "Epoch 6, Batch: 35000| Training Loss: 0.22197079605800765 | Training Accuracy: 0.30207600446428573\n",
      "Epoch 6, Batch: 36000| Training Loss: 0.22194331353902816 | Training Accuracy: 0.3021136067708333\n",
      "Epoch 6, Batch: 37000| Training Loss: 0.22191464491792628 | Training Accuracy: 0.30217768158783787\n",
      "Epoch 6, Batch: 38000| Training Loss: 0.22190128173365406 | Training Accuracy: 0.3022118626644737\n",
      "Epoch 6, Batch: 39000| Training Loss: 0.22188940789760686 | Training Accuracy: 0.30223487580128205\n",
      "Epoch 6, Batch: 40000| Training Loss: 0.22187058378569782 | Training Accuracy: 0.302247265625\n",
      "Epoch 6, Batch: 41000| Training Loss: 0.22184745101194556 | Training Accuracy: 0.3022771532012195\n",
      "Epoch 6, Batch: 42000| Training Loss: 0.22182062901556493 | Training Accuracy: 0.3023262648809524\n",
      "Epoch 6, Batch: 43000| Training Loss: 0.22179845325267591 | Training Accuracy: 0.30234838299418604\n",
      "Epoch 6, Batch: 44000| Training Loss: 0.22177735160223463 | Training Accuracy: 0.3023889382102273\n",
      "Epoch 6, Batch: 45000| Training Loss: 0.2217494382560253 | Training Accuracy: 0.30246840277777776\n",
      "Epoch 6, Batch: 46000| Training Loss: 0.22172686887111354 | Training Accuracy: 0.30250866168478263\n",
      "Epoch 6, Training Loss: 0.22170643181539068, Validation Error: 67.77661761346951, Validation Top-3 Accuracy: 0.0, Training Error: 69.74709197057751\n",
      "Epoch 7, Batch: 1000| Training Loss: 0.21774127320945263 | Training Accuracy: 0.30780078125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[16], line 143\u001b[0m, in \u001b[0;36mtrain_decoder\u001b[0;34m(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay)\u001b[0m\n\u001b[1;32m    140\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# For logging purposes\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m    145\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_decoder(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('training_results/transformer-exp-3-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_images/seq-transformer-exp-5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Tokenized Scheme -- Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
