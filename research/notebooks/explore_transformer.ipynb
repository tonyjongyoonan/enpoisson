{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with transformers\n",
    "Transformers remain as a promising replacement of RNNs due to their parallelizability. However, RNNs are unique in their hidden state which tends to be uniquely useful for games. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import numpy as np\n",
    "import utils\n",
    "import models\n",
    "\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.read_csv('../data/haha-longer-mid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def df_to_data_with_special_tokens(df, fixed_window=False, fixed_window_size=16, sampling_rate=1, algebraic_notation=True, old_vocab = None):\n",
    "    \"\"\"\n",
    "    Input: Dataframe of training data in which each row represents a full game played between players\n",
    "    Output: List in which each item represents some game's history up until a particular move, List in the same order in which the associated label is the following move\n",
    "    \"\"\"\n",
    "    subsequences = []\n",
    "    next_moves = []\n",
    "    vocab = old_vocab \n",
    "    if vocab is None:\n",
    "        vocab = VocabularyWithCLS()\n",
    "    board = chess.Board()\n",
    "    for game in df['moves']:\n",
    "        moves = game.split()\n",
    "        # Turn the game into a list of moves\n",
    "        encoded_moves = [1]\n",
    "        for move in moves:\n",
    "            # Create a move object from the coordinate notation\n",
    "            move_obj = chess.Move.from_uci(move)\n",
    "            if move_obj not in board.legal_moves:\n",
    "                break \n",
    "            else:\n",
    "                if algebraic_notation:\n",
    "                    algebraic_move = board.san(move_obj)\n",
    "                    board.push(move_obj)\n",
    "                    vocab.add_move(algebraic_move)\n",
    "                    encoded_move = vocab.get_id(algebraic_move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "                else:\n",
    "                    encoded_move = vocab.get_id(move)\n",
    "                    encoded_moves.append(encoded_move)\n",
    "        board.reset()\n",
    "        # Turn the list of moves into subsequences\n",
    "        for i in range(len(encoded_moves)-1):\n",
    "            if random.uniform(0, 1) <= sampling_rate:\n",
    "                subseq = encoded_moves[0:i+1]\n",
    "                if fixed_window and len(subseq) > fixed_window_size:\n",
    "                    subseq = subseq[-fixed_window_size:]\n",
    "                label = encoded_moves[i+1]\n",
    "                subsequences.append(subseq)\n",
    "                next_moves.append(label)\n",
    "\n",
    "    return subsequences, next_moves, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/black-data/combined/vocab.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)\n",
    "\n",
    "trainX, trainY, vocab = df_to_data_with_special_tokens(grouped_df, fixed_window=True, sampling_rate=0.5, old_vocab=vocab)\n",
    "trainX, trainX_seqlengths  = pad_sequences(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a memmap file\n",
    "def load_memmap(filename, dtype, shape):\n",
    "    # Load the memmap file with read-only mode\n",
    "    return np.memmap(filename, dtype=dtype, mode='r', shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trainX\n",
    "dtype_trainX = np.int32  # or the correct dtype for your data\n",
    "shape_trainX = (2161482, 750)  # replace with the correct shape\n",
    "trainX = load_memmap('./../data/transformer/jan/trainX.memmap', dtype_trainX, shape_trainX)\n",
    "\n",
    "# For trainY\n",
    "dtype_trainY = np.int32 # or the correct dtype for your data\n",
    "shape_trainY = (2161482, 7)  # replace with the correct shape\n",
    "trainY = load_memmap('./../data/transformer/jan/trainY.memmap', dtype_trainY, shape_trainY)\n",
    "\n",
    "with open('./../data/transformer/jan/vocab.pkl', 'rb') as inp:\n",
    "    vocab = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[:1,:4])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences, self.labels = sequences, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2081510\n"
     ]
    }
   ],
   "source": [
    "dataset = TransformerDataset(trainX, trainY)\n",
    "total_size = len(dataset)\n",
    "# We're scaling the model size so let's bring in more data as well\n",
    "train_size = int(0.97 * total_size)\n",
    "val_size = int(total_size * 0.02)\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "print(train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a long enough `pe` to be sliced according to any input `x` up to `max_len`\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is assumed to be of shape [batch_size, seq_length, d_model]\n",
    "        # Adjust `pe` to match the dimensions of `x`\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ChessTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformerDecoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model =d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Only decoder is needed for autoregressive models\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                       nhead=nhead, \n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True,),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # Memory is optional and could be used for incorporating encoder states in a hybrid model\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device)\n",
    "\n",
    "        # Embedding and Positional Encoding for tgt\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb).to(tgt.device)\n",
    "\n",
    "        # Autoregressive decoding using the Transformer Decoder\n",
    "        output = self.transformer_decoder(tgt_emb, memory=None,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          tgt_is_causal = True,\n",
    "                                          tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\" Generate a square mask for the sequence to mask out subsequent positions. \"\"\"\n",
    "        sz = tgt.size(1)\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=tgt.device, dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class ChessTransformerTwo(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=16, dropout=0.1):\n",
    "        super(ChessTransformerTwo, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        self.pos_encoder_two = PositionalEncoding(d_model, dropout, 1)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers, \n",
    "                                          batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Create source padding mask\n",
    "        src_padding_mask = self.create_padding_mask(src).to(src.device, non_blocking=True)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device, non_blocking=True)\n",
    "        # Embedding and Positional Encoding for src\n",
    "\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb).to(tgt.device, non_blocking=True)\n",
    "        \n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        tgt_emb = self.pos_encoder_two(tgt_emb).to(tgt.device, non_blocking=True)\n",
    "    \n",
    "        # Transformer\n",
    "        output = self.transformer(src_emb, tgt_emb, \n",
    "                                  src_key_padding_mask=src_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_padding_mask)\n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with `True`.\n",
    "        This mask ensures that for any position `i` in `tgt`, the decoder's self-attention mechanism\n",
    "        can only attend to positions at or before `i`.\n",
    "        \n",
    "        Args:\n",
    "            tgt (Tensor): The target input tensor of shape [batch_size, tgt_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A mask of shape [tgt_len, tgt_len] where `True` indicates that attention is not allowed.\n",
    "        \"\"\"\n",
    "        # tgt_len could be derived from the second dimension of tgt\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Generate an upper triangular matrix with `True` in the upper triangle\n",
    "        mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "class ChessTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformerDecoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model =d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Only decoder is needed for autoregressive models\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                       nhead=nhead, \n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True,),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # Memory is optional and could be used for incorporating encoder states in a hybrid model\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device)\n",
    "\n",
    "        # Embedding and Positional Encoding for tgt\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb).to(tgt.device)\n",
    "\n",
    "        # Autoregressive decoding using the Transformer Decoder\n",
    "        output = self.transformer_decoder(tgt_emb, memory=None,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          tgt_is_causal = True,\n",
    "                                          tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\" Generate a square mask for the sequence to mask out subsequent positions. \"\"\"\n",
    "        sz = tgt.size(1)\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=tgt.device, dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class ChessTransformer(nn.Module):\n",
    "    def __init__(self, vocab, d_model, nhead, num_layers, max_seq_length=750, dropout=0.1):\n",
    "        super(ChessTransformer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(vocab.id_to_move.keys())\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers, \n",
    "                                          batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        # Create source padding mask\n",
    "        src_padding_mask = self.create_padding_mask(src).to(src.device)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(tgt.device)\n",
    "        # Embedding and Positional Encoding for src\n",
    "\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb).to(tgt.device)\n",
    "        \n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        tgt_emb = self.pos_encoder(tgt_emb).to(tgt.device)\n",
    "    \n",
    "        # Transformer\n",
    "        output = self.transformer(src_emb, tgt_emb, \n",
    "                                  src_key_padding_mask=src_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                  tgt_is_causal = True,\n",
    "                                  src_is_causal = True, \n",
    "                                  src_mask = self.square_subsequent_mask(src).to(src.device),\n",
    "                                  tgt_mask = self.square_subsequent_mask(tgt).to(tgt.device))\n",
    "        # Linear layer to predict vocab\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def create_padding_mask(self, src):\n",
    "        PAD_IDX = 0\n",
    "        src_padding_mask = (src == PAD_IDX)\n",
    "        return src_padding_mask\n",
    "    \n",
    "    def square_subsequent_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with `True`.\n",
    "        This mask ensures that for any position `i` in `tgt`, the decoder's self-attention mechanism\n",
    "        can only attend to positions at or before `i`.\n",
    "        \n",
    "        Args:\n",
    "            tgt (Tensor): The target input tensor of shape [batch_size, tgt_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A mask of shape [tgt_len, tgt_len] where `True` indicates that attention is not allowed.\n",
    "        \"\"\"\n",
    "        # tgt_len could be derived from the second dimension of tgt\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Generate an upper triangular matrix with `True` in the upper triangle\n",
    "        mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "    def generate_sequence(self, src, src_length, start_symbol_id, sep_token_id, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate a sequence autoregressively using the trained transformer model.\n",
    "\n",
    "        Args:\n",
    "        - src (Tensor): The input source sequence tensor.\n",
    "        - src_length (Tensor): The length of the source sequence.\n",
    "        - start_symbol_id (int): The ID of the start symbol to begin generation.\n",
    "        - sep_token_id (int): The ID of the SEP token for sequence termination.\n",
    "        - max_length (int): Maximum length of the generated sequence to prevent infinite loops.\n",
    "\n",
    "        Returns:\n",
    "        - The generated sequence tensor.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure the model is in eval mode\n",
    "\n",
    "        # Initialize the target sequence with the start symbol\n",
    "        tgt = torch.tensor([start_symbol_id], dtype=torch.long).to(src.device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Assuming src_length is a tensor with the length of src. Adjust as needed.\n",
    "\n",
    "            # Perform a forward pass to get logits for the next token\n",
    "            logits = self.forward(src, src_length, tgt, src)\n",
    "            # Get the last token logits and apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            # get most likely token from probs\n",
    "            next_token = torch.max(probs, 1)\n",
    "            \n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt = torch.cat((tgt, next_token), dim=1)\n",
    "            \n",
    "            # Check if the <SEP> token is generated\n",
    "            if next_token.item() == sep_token_id:\n",
    "                break\n",
    "\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate top-3 accuracy\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    top3 = torch.topk(y_pred, 3, dim=1).indices\n",
    "    correct = top3.eq(y_true.view(-1, 1).expand_as(top3))\n",
    "    return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "def train_last_token(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "\n",
    "            # Check if your data contains zeros as padding, and modify 'greater(0)' if different padding\n",
    "            non_zero_mask = (sequences != 0) \n",
    "            last_non_zero_indices = non_zero_mask.long().argmax(dim=1)  # Gets the index of the first zero after last non-zero\n",
    "            \n",
    "            # Correction for sequences entirely non-zero or wrongly identified first zero indices\n",
    "            last_non_zero_indices = torch.where(\n",
    "                non_zero_mask.any(dim=1),\n",
    "                non_zero_mask.sum(dim=1) - 1,  # Last non-zero index\n",
    "                torch.tensor(0).to(device)  # Default to 0 if no non-zero found (edge case)\n",
    "            )\n",
    "\n",
    "            # Gather the last non-zero tokens for each sequence in the batch\n",
    "            last_non_zero_tokens = sequences[torch.arange(sequences.size(0)), last_non_zero_indices]\n",
    "\n",
    "            # Forward Pass using the last non-zero token\n",
    "\n",
    "\n",
    "            logits = model(sequences, last_non_zero_tokens.unsqueeze(1))\n",
    "            loss = criterion(logits.view(-1, model.vocab_size),labels)\n",
    "\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, dim=2)\n",
    "            predicted_last = predicted[:, -1]\n",
    "            train_correct += (predicted_last == labels).sum().item()\n",
    "            train_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "            count += 1\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count} | Training Accuracy: {train_correct/train_total}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                val_loss = 0\n",
    "\n",
    "                for sequences, labels in val_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    # Check if your data contains zeros as padding, and modify 'greater(0)' if different padding\n",
    "                    non_zero_mask = (sequences != 0) \n",
    "                    last_non_zero_indices = non_zero_mask.long().argmax(dim=1)  # Gets the index of the first zero after last non-zero\n",
    "                    \n",
    "                    # Correction for sequences entirely non-zero or wrongly identified first zero indices\n",
    "                    last_non_zero_indices = torch.where(\n",
    "                        non_zero_mask.any(dim=1),\n",
    "                        non_zero_mask.sum(dim=1) - 1,  # Last non-zero index\n",
    "                        torch.tensor(0).to(device)  # Default to 0 if no non-zero found (edge case)\n",
    "                    )\n",
    "\n",
    "                    # Gather the last non-zero tokens for each sequence in the batch\n",
    "                    last_non_zero_tokens = sequences[torch.arange(sequences.size(0)), last_non_zero_indices]\n",
    "\n",
    "                    # Forward Pass using the last non-zero token\n",
    "\n",
    "\n",
    "                    logits = model(sequences, last_non_zero_tokens.unsqueeze(1))\n",
    "                    loss = criterion(logits.view(-1, model.vocab_size),labels)\n",
    "\n",
    "                    # For logging purposes\n",
    "                    val_loss += loss.item()\n",
    "                     # Calculate accuracy\n",
    "                    _, predicted = torch.max(logits, dim=2)\n",
    "                    predicted_last = predicted[:, -1]\n",
    "                    val_correct += (predicted_last == labels).sum().item()\n",
    "                    val_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "                    count += 1\n",
    "                val_loss_values.append(val_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values\n",
    "\n",
    "def train_decoder(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            tgt_labels = torch.cat([sequences[:,1:],labels.unsqueeze(1)],dim=1).to(device)\n",
    "            logits = model(sequences, sequences)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), tgt_labels.view(-1))\n",
    "\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, dim=2)\n",
    "            # Selecting only the last token from each sequence in the batch\n",
    "            predicted_last = predicted[:, -1]\n",
    "            tgt_labels_last = tgt_labels[:, -1]\n",
    "\n",
    "            # Updating the correct count based on the last token predictions\n",
    "            train_correct += (predicted_last == tgt_labels_last).sum().item()\n",
    "            # Updating the total count to reflect that only one token per sequence is considered\n",
    "            train_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "            # train_correct += (predicted == tgt_labels).sum().item()\n",
    "            # train_total += tgt_labels.numel()\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count} | Training Accuracy: {train_correct/train_total}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_top3_correct = 0\n",
    "                val_loss = 0\n",
    "\n",
    "                for sequences, labels in val_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    tgt_labels = torch.cat([sequences[:,1:],labels.unsqueeze(1)],dim=1).to(device)\n",
    "                    logits = model(sequences, sequences)\n",
    "                    loss = criterion(logits.view(-1, model.vocab_size), tgt_labels.view(-1))\n",
    "\n",
    "                    # For logging purposes\n",
    "                    val_loss += loss.item()\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted = torch.max(logits, dim=2)\n",
    "                    # Selecting only the last token from each sequence in the batch\n",
    "                    predicted_last = predicted[:, -1]\n",
    "                    tgt_labels_last = tgt_labels[:, -1]\n",
    "\n",
    "                    # Updating the correct count based on the last token predictions\n",
    "                    val_correct += (predicted_last == tgt_labels_last).sum().item()\n",
    "                    # Updating the total count to reflect that only one token per sequence is considered\n",
    "                    val_total += predicted_last.size(0)  # Same as tgt_labels_last.size(0), which is the batch size\n",
    "\n",
    "                val_loss_values.append(val_loss / len(val_loader))\n",
    "                val_accuracy = 100 * val_correct / val_total\n",
    "                val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "                val_error.append(100 - val_accuracy)\n",
    "                val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values\n",
    "\n",
    "def train_transformer(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, learn_decay):\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    val_3_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        training_loss = 0.0\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            count += 1\n",
    "            sequences, labels = sequences.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # Forward Pass\n",
    "            logits = model(sequences, labels)\n",
    "            print(logits)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), labels.contiguous().view(-1))\n",
    "            # Backpropogate & Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip it\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            # For logging purposes\n",
    "            training_loss += loss.item()\n",
    "            # _, predicted = torch.max(output.data, 1)\n",
    "            # train_total += labels.size(0)\n",
    "            # train_correct += (predicted == labels).sum().item()\n",
    "            # Get the predicted class indices for each position in each sequence\n",
    "            _, predicted = torch.max(logits.data, dim=2)  # Shape: (batch_size, seq_length)\n",
    "            correct_predictions = predicted == labels  # Shape: (batch_size, seq_length)\n",
    "            correct_sequences = correct_predictions.all(dim=1)  # Shape: (batch_size)\n",
    "            train_correct += correct_sequences.sum().item()\n",
    "            train_total += labels.size(0) \n",
    "            break\n",
    "            if count % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch: {count}| Training Loss: {training_loss/count}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        validation_loss = 0.0\n",
    "        # if val_loader is not None:\n",
    "        #     with torch.no_grad():\n",
    "        #         val_correct = 0\n",
    "        #         val_total = 0\n",
    "        #         val_top3_correct = 0\n",
    "        #         validation_loss = 0\n",
    "\n",
    "        #         for sequences, lengths, labels in val_loader:\n",
    "        #             sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
    "        #             outputs = model.generate(sequences, lengths)\n",
    "        #             _, predicted = torch.max(outputs.data, 1)\n",
    "        #             val_total += labels.size(0)\n",
    "        #             val_correct += (predicted == labels).sum().item()\n",
    "        #             val_top3_correct += top_3_accuracy(labels, outputs) * labels.size(0)\n",
    "        #             loss = criterion(outputs, labels)\n",
    "        #             validation_loss += loss.item()\n",
    "\n",
    "        #         val_loss_values.append(validation_loss / len(val_loader))\n",
    "        #         val_accuracy = 100 * val_correct / val_total\n",
    "        #         val_top3_accuracy = 100 * val_top3_correct / val_total\n",
    "        #         val_error.append(100 - val_accuracy)\n",
    "        #         val_3_accuracy.append(val_top3_accuracy)\n",
    "        # Log Model Performance  \n",
    "        train_loss_values.append(training_loss)\n",
    "        train_error.append(100-100*train_correct/train_total)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {training_loss/len(train_loader)}, Validation Error: {val_error[-1]}, Validation Top-3 Accuracy: {val_3_accuracy[-1]}, Training Error: {train_error[-1]}')\n",
    "        for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = op_params['lr'] * learn_decay\n",
    "    return train_error,train_loss_values, val_error, val_loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207314\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 128\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.65 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 1000| Training Loss: 4.277270253896713 | Training Accuracy: 0.06821875\n",
      "Epoch 1, Batch: 2000| Training Loss: 3.3310693102478983 | Training Accuracy: 0.071109375\n",
      "Epoch 1, Batch: 3000| Training Loss: 2.7611966275374096 | Training Accuracy: 0.072140625\n",
      "Epoch 1, Batch: 4000| Training Loss: 2.406634035587311 | Training Accuracy: 0.07366796875\n",
      "Epoch 1, Batch: 5000| Training Loss: 2.1669605624198915 | Training Accuracy: 0.075484375\n",
      "Epoch 1, Batch: 6000| Training Loss: 1.9921583703259627 | Training Accuracy: 0.07734114583333333\n",
      "Epoch 1, Batch: 7000| Training Loss: 1.8581724826863835 | Training Accuracy: 0.07928571428571428\n",
      "Epoch 1, Batch: 8000| Training Loss: 1.7539799907431006 | Training Accuracy: 0.0808203125\n",
      "Epoch 1, Batch: 9000| Training Loss: 1.6695201332966487 | Training Accuracy: 0.08231770833333334\n",
      "Epoch 1, Batch: 10000| Training Loss: 1.5988527409672737 | Training Accuracy: 0.0835015625\n",
      "Epoch 1, Batch: 11000| Training Loss: 1.5384822490432046 | Training Accuracy: 0.08468039772727273\n",
      "Epoch 1, Batch: 12000| Training Loss: 1.4869812156309685 | Training Accuracy: 0.08593489583333333\n",
      "Epoch 1, Batch: 13000| Training Loss: 1.441819060435662 | Training Accuracy: 0.08707331730769231\n",
      "Epoch 1, Batch: 14000| Training Loss: 1.4017801074768816 | Training Accuracy: 0.08806473214285715\n",
      "Epoch 1, Batch: 15000| Training Loss: 1.3669130097587903 | Training Accuracy: 0.08905\n",
      "Epoch 1, Batch: 16000| Training Loss: 1.3354366956166923 | Training Accuracy: 0.09008203125\n",
      "Epoch 1, Batch: 17000| Training Loss: 1.3071811314400505 | Training Accuracy: 0.09099264705882353\n",
      "Epoch 1, Batch: 18000| Training Loss: 1.2814611663884588 | Training Accuracy: 0.09181944444444444\n",
      "Epoch 1, Batch: 19000| Training Loss: 1.2580664449710595 | Training Accuracy: 0.09247697368421053\n",
      "Epoch 1, Batch: 20000| Training Loss: 1.2361173732191324 | Training Accuracy: 0.09329609375\n",
      "Epoch 1, Batch: 21000| Training Loss: 1.216264807692596 | Training Accuracy: 0.09394345238095238\n",
      "Epoch 1, Batch: 22000| Training Loss: 1.1978358969254927 | Training Accuracy: 0.09464630681818181\n",
      "Epoch 1, Batch: 23000| Training Loss: 1.1807987880240316 | Training Accuracy: 0.09527445652173913\n",
      "Epoch 1, Batch: 24000| Training Loss: 1.164851968700687 | Training Accuracy: 0.09588736979166666\n",
      "Epoch 1, Batch: 25000| Training Loss: 1.1498702676606178 | Training Accuracy: 0.09658\n",
      "Epoch 1, Batch: 26000| Training Loss: 1.135807240236264 | Training Accuracy: 0.09717307692307692\n",
      "Epoch 1, Batch: 27000| Training Loss: 1.1224758436172098 | Training Accuracy: 0.09772280092592593\n",
      "Epoch 1, Batch: 28000| Training Loss: 1.1100279576969998 | Training Accuracy: 0.09823772321428571\n",
      "Epoch 1, Batch: 29000| Training Loss: 1.0982175959410339 | Training Accuracy: 0.09884213362068965\n",
      "Epoch 1, Batch: 30000| Training Loss: 1.0867453599393369 | Training Accuracy: 0.0993921875\n",
      "Epoch 1, Batch: 31000| Training Loss: 1.0762404553582592 | Training Accuracy: 0.0999758064516129\n",
      "Epoch 1, Batch: 32000| Training Loss: 1.0661412358041853 | Training Accuracy: 0.1004453125\n",
      "Epoch 1, Training Loss: 1.061050172405421, Validation Error: 85.85875061164573, Validation Top-3 Accuracy: 0.0, Training Error: 89.92611133263833\n",
      "Epoch 2, Batch: 1000| Training Loss: 0.6711575121879577 | Training Accuracy: 0.125109375\n",
      "Epoch 2, Batch: 2000| Training Loss: 0.6644032163918019 | Training Accuracy: 0.1235625\n",
      "Epoch 2, Batch: 3000| Training Loss: 0.6603363276521365 | Training Accuracy: 0.1233125\n",
      "Epoch 2, Batch: 4000| Training Loss: 0.6570277464985848 | Training Accuracy: 0.123578125\n",
      "Epoch 2, Batch: 5000| Training Loss: 0.6538746861815452 | Training Accuracy: 0.124078125\n",
      "Epoch 2, Batch: 6000| Training Loss: 0.651663731748859 | Training Accuracy: 0.1243203125\n",
      "Epoch 2, Batch: 7000| Training Loss: 0.6494722363267627 | Training Accuracy: 0.12511160714285716\n",
      "Epoch 2, Batch: 8000| Training Loss: 0.6483781324438751 | Training Accuracy: 0.12532421875\n",
      "Epoch 2, Batch: 9000| Training Loss: 0.646468456092808 | Training Accuracy: 0.12556423611111112\n",
      "Epoch 2, Batch: 10000| Training Loss: 0.6450688520371914 | Training Accuracy: 0.1259125\n",
      "Epoch 2, Batch: 11000| Training Loss: 0.6436825778728181 | Training Accuracy: 0.12587073863636364\n",
      "Epoch 2, Batch: 12000| Training Loss: 0.6423990042085449 | Training Accuracy: 0.12604036458333334\n",
      "Epoch 2, Batch: 13000| Training Loss: 0.6410449204949232 | Training Accuracy: 0.12621274038461539\n",
      "Epoch 2, Batch: 14000| Training Loss: 0.6398537410880838 | Training Accuracy: 0.12640290178571428\n",
      "Epoch 2, Batch: 15000| Training Loss: 0.6385808268547059 | Training Accuracy: 0.12664270833333333\n",
      "Epoch 2, Batch: 16000| Training Loss: 0.6372981672883034 | Training Accuracy: 0.12692578125\n",
      "Epoch 2, Batch: 17000| Training Loss: 0.6360604495038005 | Training Accuracy: 0.12711856617647058\n",
      "Epoch 2, Batch: 18000| Training Loss: 0.6348251039667262 | Training Accuracy: 0.1273263888888889\n",
      "Epoch 2, Batch: 19000| Training Loss: 0.6338025029944746 | Training Accuracy: 0.12739226973684212\n",
      "Epoch 2, Batch: 20000| Training Loss: 0.6326680403605104 | Training Accuracy: 0.12759765625\n",
      "Epoch 2, Batch: 21000| Training Loss: 0.6316152188096728 | Training Accuracy: 0.12777232142857142\n",
      "Epoch 2, Batch: 22000| Training Loss: 0.630477886107835 | Training Accuracy: 0.12803480113636365\n",
      "Epoch 2, Batch: 23000| Training Loss: 0.629568841472916 | Training Accuracy: 0.12821671195652173\n",
      "Epoch 2, Batch: 24000| Training Loss: 0.6284723163594802 | Training Accuracy: 0.128474609375\n",
      "Epoch 2, Batch: 25000| Training Loss: 0.6275226095092297 | Training Accuracy: 0.12852875\n",
      "Epoch 2, Batch: 26000| Training Loss: 0.6266797275600525 | Training Accuracy: 0.12857211538461538\n",
      "Epoch 2, Batch: 27000| Training Loss: 0.6256377971492432 | Training Accuracy: 0.12874768518518517\n",
      "Epoch 2, Batch: 28000| Training Loss: 0.6247817378225071 | Training Accuracy: 0.12896205357142856\n",
      "Epoch 2, Batch: 29000| Training Loss: 0.6239902548913298 | Training Accuracy: 0.12909213362068966\n",
      "Epoch 2, Batch: 30000| Training Loss: 0.6230426460792621 | Training Accuracy: 0.1292515625\n",
      "Epoch 2, Batch: 31000| Training Loss: 0.6220799236528335 | Training Accuracy: 0.12939818548387097\n",
      "Epoch 2, Batch: 32000| Training Loss: 0.6211614200938493 | Training Accuracy: 0.129517578125\n",
      "Epoch 2, Training Loss: 0.6207129528947369, Validation Error: 84.4793438497565, Validation Top-3 Accuracy: 0.0, Training Error: 87.03671853606276\n",
      "Epoch 3, Batch: 1000| Training Loss: 0.5521195216178894 | Training Accuracy: 0.139609375\n",
      "Epoch 3, Batch: 2000| Training Loss: 0.5478138211071492 | Training Accuracy: 0.139859375\n",
      "Epoch 3, Batch: 3000| Training Loss: 0.5459079184631507 | Training Accuracy: 0.140203125\n",
      "Epoch 3, Batch: 4000| Training Loss: 0.5440611904636026 | Training Accuracy: 0.14113671875\n",
      "Epoch 3, Batch: 5000| Training Loss: 0.5430519543707371 | Training Accuracy: 0.141515625\n",
      "Epoch 3, Batch: 6000| Training Loss: 0.5422872601995865 | Training Accuracy: 0.1415\n",
      "Epoch 3, Batch: 7000| Training Loss: 0.5410353049210139 | Training Accuracy: 0.14142410714285714\n",
      "Epoch 3, Batch: 8000| Training Loss: 0.5398365555964411 | Training Accuracy: 0.14148828125\n",
      "Epoch 3, Batch: 9000| Training Loss: 0.5390784113142225 | Training Accuracy: 0.14157465277777778\n",
      "Epoch 3, Batch: 10000| Training Loss: 0.5384658810526133 | Training Accuracy: 0.1419375\n",
      "Epoch 3, Batch: 11000| Training Loss: 0.5378261609294198 | Training Accuracy: 0.14206818181818182\n",
      "Epoch 3, Batch: 12000| Training Loss: 0.5372282589251797 | Training Accuracy: 0.14202864583333333\n",
      "Epoch 3, Batch: 13000| Training Loss: 0.5365548460506476 | Training Accuracy: 0.1419639423076923\n",
      "Epoch 3, Batch: 14000| Training Loss: 0.5359789961172001 | Training Accuracy: 0.14210714285714285\n",
      "Epoch 3, Batch: 15000| Training Loss: 0.535406116215388 | Training Accuracy: 0.1420625\n",
      "Epoch 3, Batch: 16000| Training Loss: 0.5348981884922832 | Training Accuracy: 0.1421728515625\n",
      "Epoch 3, Batch: 17000| Training Loss: 0.53452745602762 | Training Accuracy: 0.1422702205882353\n",
      "Epoch 3, Batch: 18000| Training Loss: 0.5339745546198553 | Training Accuracy: 0.1422152777777778\n",
      "Epoch 3, Batch: 19000| Training Loss: 0.5334619117583099 | Training Accuracy: 0.14241611842105265\n",
      "Epoch 3, Batch: 20000| Training Loss: 0.5330731067016721 | Training Accuracy: 0.14258984375\n",
      "Epoch 3, Batch: 21000| Training Loss: 0.532583053147509 | Training Accuracy: 0.14265997023809524\n",
      "Epoch 3, Batch: 22000| Training Loss: 0.5320687713826244 | Training Accuracy: 0.14275852272727274\n",
      "Epoch 3, Batch: 23000| Training Loss: 0.5316264822586723 | Training Accuracy: 0.1428695652173913\n",
      "Epoch 3, Batch: 24000| Training Loss: 0.5311584800941249 | Training Accuracy: 0.14293359375\n",
      "Epoch 3, Batch: 25000| Training Loss: 0.5308024905908107 | Training Accuracy: 0.142983125\n",
      "Epoch 3, Batch: 26000| Training Loss: 0.5303762592627452 | Training Accuracy: 0.14296454326923078\n",
      "Epoch 3, Batch: 27000| Training Loss: 0.5299710534270163 | Training Accuracy: 0.1429369212962963\n",
      "Epoch 3, Batch: 28000| Training Loss: 0.5295507034723248 | Training Accuracy: 0.14294866071428572\n",
      "Epoch 3, Batch: 29000| Training Loss: 0.5291636438102558 | Training Accuracy: 0.14306788793103448\n",
      "Epoch 3, Batch: 30000| Training Loss: 0.5287673182676236 | Training Accuracy: 0.143240625\n",
      "Epoch 3, Batch: 31000| Training Loss: 0.5283836900582236 | Training Accuracy: 0.14333568548387096\n",
      "Epoch 3, Batch: 32000| Training Loss: 0.5280057687032967 | Training Accuracy: 0.1434140625\n",
      "Epoch 3, Training Loss: 0.5278094835600471, Validation Error: 82.87158934687886, Validation Top-3 Accuracy: 0.0, Training Error: 85.6540203986529\n",
      "Epoch 4, Batch: 1000| Training Loss: 0.49459830272197725 | Training Accuracy: 0.1494375\n",
      "Epoch 4, Batch: 2000| Training Loss: 0.49147701229155066 | Training Accuracy: 0.151515625\n",
      "Epoch 4, Batch: 3000| Training Loss: 0.48944265352686245 | Training Accuracy: 0.15161979166666667\n",
      "Epoch 4, Batch: 4000| Training Loss: 0.48830185513198376 | Training Accuracy: 0.15156640625\n",
      "Epoch 4, Batch: 5000| Training Loss: 0.4868272895336151 | Training Accuracy: 0.151409375\n",
      "Epoch 4, Batch: 6000| Training Loss: 0.4862123900602261 | Training Accuracy: 0.15089322916666667\n",
      "Epoch 4, Batch: 7000| Training Loss: 0.48586709325228417 | Training Accuracy: 0.15121428571428572\n",
      "Epoch 4, Batch: 8000| Training Loss: 0.4852582731060684 | Training Accuracy: 0.1514296875\n",
      "Epoch 4, Batch: 9000| Training Loss: 0.4844727109571298 | Training Accuracy: 0.15150520833333334\n",
      "Epoch 4, Batch: 10000| Training Loss: 0.4840474545687437 | Training Accuracy: 0.15164375\n",
      "Epoch 4, Batch: 11000| Training Loss: 0.4836522693363103 | Training Accuracy: 0.15158522727272727\n",
      "Epoch 4, Batch: 12000| Training Loss: 0.4833195228825013 | Training Accuracy: 0.15162239583333334\n",
      "Epoch 4, Batch: 13000| Training Loss: 0.48292500425302065 | Training Accuracy: 0.1517235576923077\n",
      "Epoch 4, Batch: 14000| Training Loss: 0.4824795344918966 | Training Accuracy: 0.15173325892857142\n",
      "Epoch 4, Batch: 15000| Training Loss: 0.48225703591505686 | Training Accuracy: 0.151625\n",
      "Epoch 4, Batch: 16000| Training Loss: 0.48201838686317205 | Training Accuracy: 0.1517119140625\n",
      "Epoch 4, Batch: 17000| Training Loss: 0.48176296530926926 | Training Accuracy: 0.15174816176470587\n",
      "Epoch 4, Batch: 18000| Training Loss: 0.4814439157065418 | Training Accuracy: 0.15186545138888888\n",
      "Epoch 4, Batch: 19000| Training Loss: 0.4812255045301036 | Training Accuracy: 0.15194654605263158\n",
      "Epoch 4, Batch: 20000| Training Loss: 0.4810264293447137 | Training Accuracy: 0.151946875\n",
      "Epoch 4, Batch: 21000| Training Loss: 0.4807619699026857 | Training Accuracy: 0.1519940476190476\n",
      "Epoch 4, Batch: 22000| Training Loss: 0.4804759004983035 | Training Accuracy: 0.152109375\n",
      "Epoch 4, Batch: 23000| Training Loss: 0.48020480858113457 | Training Accuracy: 0.1521297554347826\n",
      "Epoch 4, Batch: 24000| Training Loss: 0.47996317293991647 | Training Accuracy: 0.15227669270833333\n",
      "Epoch 4, Batch: 25000| Training Loss: 0.4797390858626366 | Training Accuracy: 0.152305\n",
      "Epoch 4, Batch: 26000| Training Loss: 0.47946288880247334 | Training Accuracy: 0.15241225961538463\n",
      "Epoch 4, Batch: 27000| Training Loss: 0.47924024129465775 | Training Accuracy: 0.15252025462962962\n",
      "Epoch 4, Batch: 28000| Training Loss: 0.47903610607875247 | Training Accuracy: 0.15261774553571428\n",
      "Epoch 4, Batch: 29000| Training Loss: 0.47876586949619754 | Training Accuracy: 0.1526875\n",
      "Epoch 4, Batch: 30000| Training Loss: 0.47850796821415426 | Training Accuracy: 0.15274739583333333\n",
      "Epoch 4, Batch: 31000| Training Loss: 0.47824333665640123 | Training Accuracy: 0.15274899193548386\n",
      "Epoch 4, Batch: 32000| Training Loss: 0.4779832048462704 | Training Accuracy: 0.152833984375\n",
      "Epoch 4, Training Loss: 0.4778744530520555, Validation Error: 81.75781158981289, Validation Top-3 Accuracy: 0.0, Training Error: 84.71052264942277\n",
      "Epoch 5, Batch: 1000| Training Loss: 0.45869899761676786 | Training Accuracy: 0.155953125\n",
      "Epoch 5, Batch: 2000| Training Loss: 0.45565291939675806 | Training Accuracy: 0.1558828125\n",
      "Epoch 5, Batch: 3000| Training Loss: 0.45453603101770085 | Training Accuracy: 0.15690625\n",
      "Epoch 5, Batch: 4000| Training Loss: 0.4541607052385807 | Training Accuracy: 0.156703125\n",
      "Epoch 5, Batch: 5000| Training Loss: 0.4536404236972332 | Training Accuracy: 0.156953125\n",
      "Epoch 5, Batch: 6000| Training Loss: 0.4532230011870464 | Training Accuracy: 0.15716927083333335\n",
      "Epoch 5, Batch: 7000| Training Loss: 0.45265644044109754 | Training Accuracy: 0.15739508928571427\n",
      "Epoch 5, Batch: 8000| Training Loss: 0.45195246053114535 | Training Accuracy: 0.157560546875\n",
      "Epoch 5, Batch: 9000| Training Loss: 0.45137569953335654 | Training Accuracy: 0.15770833333333334\n",
      "Epoch 5, Batch: 10000| Training Loss: 0.45131273227632046 | Training Accuracy: 0.1577203125\n",
      "Epoch 5, Batch: 11000| Training Loss: 0.45092997777462007 | Training Accuracy: 0.1578096590909091\n",
      "Epoch 5, Batch: 12000| Training Loss: 0.4506516810854276 | Training Accuracy: 0.15791536458333333\n",
      "Epoch 5, Batch: 13000| Training Loss: 0.4505171445012093 | Training Accuracy: 0.15803485576923076\n",
      "Epoch 5, Batch: 14000| Training Loss: 0.45019282679685524 | Training Accuracy: 0.15822879464285713\n",
      "Epoch 5, Batch: 15000| Training Loss: 0.4500500827928384 | Training Accuracy: 0.15842291666666666\n",
      "Epoch 5, Batch: 16000| Training Loss: 0.4497157782446593 | Training Accuracy: 0.15866015625\n",
      "Epoch 5, Batch: 17000| Training Loss: 0.44965102347380975 | Training Accuracy: 0.15871323529411765\n",
      "Epoch 5, Batch: 18000| Training Loss: 0.44923505255745516 | Training Accuracy: 0.15880208333333334\n",
      "Epoch 5, Batch: 19000| Training Loss: 0.4489847584915789 | Training Accuracy: 0.15892845394736843\n",
      "Epoch 5, Batch: 20000| Training Loss: 0.44883309031277896 | Training Accuracy: 0.158928125\n",
      "Epoch 5, Batch: 21000| Training Loss: 0.44869594980847266 | Training Accuracy: 0.15888020833333333\n",
      "Epoch 5, Batch: 22000| Training Loss: 0.4484122320210392 | Training Accuracy: 0.158953125\n",
      "Epoch 5, Batch: 23000| Training Loss: 0.44828841744557674 | Training Accuracy: 0.1589891304347826\n",
      "Epoch 5, Batch: 24000| Training Loss: 0.44811088067417343 | Training Accuracy: 0.15902994791666666\n",
      "Epoch 5, Batch: 25000| Training Loss: 0.4479936969590187 | Training Accuracy: 0.159086875\n",
      "Epoch 5, Batch: 26000| Training Loss: 0.447838217157584 | Training Accuracy: 0.15920673076923078\n",
      "Epoch 5, Batch: 27000| Training Loss: 0.4477657610818192 | Training Accuracy: 0.15926967592592592\n",
      "Epoch 5, Batch: 28000| Training Loss: 0.44759894141554835 | Training Accuracy: 0.15923939732142858\n",
      "Epoch 5, Batch: 29000| Training Loss: 0.4474891034621617 | Training Accuracy: 0.15914924568965516\n",
      "Epoch 5, Batch: 30000| Training Loss: 0.4473615257769823 | Training Accuracy: 0.15921041666666666\n",
      "Epoch 5, Batch: 31000| Training Loss: 0.44717546733156327 | Training Accuracy: 0.15932157258064517\n",
      "Epoch 5, Batch: 32000| Training Loss: 0.44705896330997347 | Training Accuracy: 0.1593916015625\n",
      "Epoch 5, Training Loss: 0.4469470358498934, Validation Error: 81.18694223734184, Validation Top-3 Accuracy: 0.0, Training Error: 84.05311528649874\n",
      "Epoch 6, Batch: 1000| Training Loss: 0.4352319273352623 | Training Accuracy: 0.16065625\n",
      "Epoch 6, Batch: 2000| Training Loss: 0.4333916566669941 | Training Accuracy: 0.1614453125\n",
      "Epoch 6, Batch: 3000| Training Loss: 0.43248614061872165 | Training Accuracy: 0.16247916666666667\n",
      "Epoch 6, Batch: 4000| Training Loss: 0.43162360978126524 | Training Accuracy: 0.16348828125\n",
      "Epoch 6, Batch: 5000| Training Loss: 0.4309717469573021 | Training Accuracy: 0.16395\n",
      "Epoch 6, Batch: 6000| Training Loss: 0.43071966718137267 | Training Accuracy: 0.1637265625\n",
      "Epoch 6, Batch: 7000| Training Loss: 0.4303593663743564 | Training Accuracy: 0.1639486607142857\n",
      "Epoch 6, Batch: 8000| Training Loss: 0.43017050956562164 | Training Accuracy: 0.16375390625\n",
      "Epoch 6, Batch: 9000| Training Loss: 0.4301757184300158 | Training Accuracy: 0.1637829861111111\n",
      "Epoch 6, Batch: 10000| Training Loss: 0.42997571325302125 | Training Accuracy: 0.1638078125\n",
      "Epoch 6, Batch: 11000| Training Loss: 0.4296303317926147 | Training Accuracy: 0.16371590909090908\n",
      "Epoch 6, Batch: 12000| Training Loss: 0.4295967272147536 | Training Accuracy: 0.16374088541666668\n",
      "Epoch 6, Batch: 13000| Training Loss: 0.42926159909826056 | Training Accuracy: 0.1638545673076923\n",
      "Epoch 6, Batch: 14000| Training Loss: 0.42915410078423366 | Training Accuracy: 0.1638794642857143\n",
      "Epoch 6, Batch: 15000| Training Loss: 0.42893869659701983 | Training Accuracy: 0.164009375\n",
      "Epoch 6, Batch: 16000| Training Loss: 0.4287779968138784 | Training Accuracy: 0.1640712890625\n",
      "Epoch 6, Batch: 17000| Training Loss: 0.428699518759461 | Training Accuracy: 0.16413786764705882\n",
      "Epoch 6, Batch: 18000| Training Loss: 0.42863897065818307 | Training Accuracy: 0.16398871527777778\n",
      "Epoch 6, Batch: 19000| Training Loss: 0.42844890563111554 | Training Accuracy: 0.16414473684210526\n",
      "Epoch 6, Batch: 20000| Training Loss: 0.42829816463440656 | Training Accuracy: 0.1641328125\n",
      "Epoch 6, Batch: 21000| Training Loss: 0.42817502906918525 | Training Accuracy: 0.16411160714285714\n",
      "Epoch 6, Batch: 22000| Training Loss: 0.42803868731043554 | Training Accuracy: 0.16420951704545456\n",
      "Epoch 6, Batch: 23000| Training Loss: 0.42792287849084193 | Training Accuracy: 0.16425\n",
      "Epoch 6, Batch: 24000| Training Loss: 0.4278801880280177 | Training Accuracy: 0.16427864583333332\n",
      "Epoch 6, Batch: 25000| Training Loss: 0.42779499895095824 | Training Accuracy: 0.16423125\n",
      "Epoch 6, Batch: 26000| Training Loss: 0.4277355244457722 | Training Accuracy: 0.16428725961538462\n",
      "Epoch 6, Batch: 27000| Training Loss: 0.42762912454097357 | Training Accuracy: 0.1642795138888889\n",
      "Epoch 6, Batch: 28000| Training Loss: 0.4275450540472354 | Training Accuracy: 0.1643286830357143\n",
      "Epoch 6, Batch: 29000| Training Loss: 0.42739319565686684 | Training Accuracy: 0.16443696120689655\n",
      "Epoch 6, Batch: 30000| Training Loss: 0.4272943704942862 | Training Accuracy: 0.16449375\n",
      "Epoch 6, Batch: 31000| Training Loss: 0.42724125705322913 | Training Accuracy: 0.16446723790322582\n",
      "Epoch 6, Batch: 32000| Training Loss: 0.42714149523712697 | Training Accuracy: 0.16449853515625\n",
      "Epoch 6, Training Loss: 0.427062805498743, Validation Error: 81.03781718200247, Validation Top-3 Accuracy: 0.0, Training Error: 83.54833750498436\n",
      "Epoch 7, Batch: 1000| Training Loss: 0.41971308320760725 | Training Accuracy: 0.164484375\n",
      "Epoch 7, Batch: 2000| Training Loss: 0.41806499084830284 | Training Accuracy: 0.166953125\n",
      "Epoch 7, Batch: 3000| Training Loss: 0.4177352369725704 | Training Accuracy: 0.16677083333333334\n",
      "Epoch 7, Batch: 4000| Training Loss: 0.41728776232898235 | Training Accuracy: 0.166671875\n",
      "Epoch 7, Batch: 5000| Training Loss: 0.4169720638334751 | Training Accuracy: 0.166934375\n",
      "Epoch 7, Batch: 6000| Training Loss: 0.416523935392499 | Training Accuracy: 0.16698697916666666\n",
      "Epoch 7, Batch: 7000| Training Loss: 0.4164100900292397 | Training Accuracy: 0.16696651785714287\n",
      "Epoch 7, Batch: 8000| Training Loss: 0.4163022871948779 | Training Accuracy: 0.16703125\n",
      "Epoch 7, Batch: 9000| Training Loss: 0.41612518872817356 | Training Accuracy: 0.16729166666666667\n",
      "Epoch 7, Batch: 10000| Training Loss: 0.4158881556421518 | Training Accuracy: 0.1674359375\n",
      "Epoch 7, Batch: 11000| Training Loss: 0.41592909452048216 | Training Accuracy: 0.16732670454545454\n",
      "Epoch 7, Batch: 12000| Training Loss: 0.41586152750998734 | Training Accuracy: 0.16730598958333334\n",
      "Epoch 7, Batch: 13000| Training Loss: 0.41582583477176155 | Training Accuracy: 0.1672920673076923\n",
      "Epoch 7, Batch: 14000| Training Loss: 0.41570627106939045 | Training Accuracy: 0.1672265625\n",
      "Epoch 7, Batch: 15000| Training Loss: 0.4156182323118051 | Training Accuracy: 0.16721041666666667\n",
      "Epoch 7, Batch: 16000| Training Loss: 0.41551424852199853 | Training Accuracy: 0.1671904296875\n",
      "Epoch 7, Batch: 17000| Training Loss: 0.41546441737869205 | Training Accuracy: 0.16725275735294118\n",
      "Epoch 7, Batch: 18000| Training Loss: 0.4153466069549322 | Training Accuracy: 0.16729947916666665\n",
      "Epoch 7, Batch: 19000| Training Loss: 0.415213895888705 | Training Accuracy: 0.16744654605263157\n",
      "Epoch 7, Batch: 20000| Training Loss: 0.41508777083158493 | Training Accuracy: 0.1675515625\n",
      "Epoch 7, Batch: 21000| Training Loss: 0.41497914593702273 | Training Accuracy: 0.16761681547619048\n",
      "Epoch 7, Batch: 22000| Training Loss: 0.4148331160856919 | Training Accuracy: 0.16756107954545454\n",
      "Epoch 7, Batch: 23000| Training Loss: 0.4147950167344964 | Training Accuracy: 0.16765625\n",
      "Epoch 7, Batch: 24000| Training Loss: 0.4147258864591519 | Training Accuracy: 0.1676875\n",
      "Epoch 7, Batch: 25000| Training Loss: 0.41467091508746146 | Training Accuracy: 0.167739375\n",
      "Epoch 7, Batch: 26000| Training Loss: 0.41448787352328115 | Training Accuracy: 0.16779146634615386\n",
      "Epoch 7, Batch: 27000| Training Loss: 0.4144253096072762 | Training Accuracy: 0.16777025462962963\n",
      "Epoch 7, Batch: 28000| Training Loss: 0.41438376961967777 | Training Accuracy: 0.16775390625\n",
      "Epoch 7, Batch: 29000| Training Loss: 0.41431359226641984 | Training Accuracy: 0.16776831896551725\n",
      "Epoch 7, Batch: 30000| Training Loss: 0.4141986496557792 | Training Accuracy: 0.16780625\n",
      "Epoch 7, Batch: 31000| Training Loss: 0.4140977683278822 | Training Accuracy: 0.16781401209677418\n",
      "Epoch 7, Batch: 32000| Training Loss: 0.41403592057246713 | Training Accuracy: 0.16777978515625\n",
      "Epoch 7, Training Loss: 0.413978304061088, Validation Error: 80.65568422769532, Validation Top-3 Accuracy: 0.0, Training Error: 83.21550220753204\n",
      "Epoch 8, Batch: 1000| Training Loss: 0.4083538945913315 | Training Accuracy: 0.1678125\n",
      "Epoch 8, Batch: 2000| Training Loss: 0.40757601109147074 | Training Accuracy: 0.170390625\n",
      "Epoch 8, Batch: 3000| Training Loss: 0.40687451724211376 | Training Accuracy: 0.170109375\n",
      "Epoch 8, Batch: 4000| Training Loss: 0.4068443677797914 | Training Accuracy: 0.17003515625\n",
      "Epoch 8, Batch: 5000| Training Loss: 0.4068409416913986 | Training Accuracy: 0.170421875\n",
      "Epoch 8, Batch: 6000| Training Loss: 0.40687988079090914 | Training Accuracy: 0.170453125\n",
      "Epoch 8, Batch: 7000| Training Loss: 0.4067479651357446 | Training Accuracy: 0.17059821428571428\n",
      "Epoch 8, Batch: 8000| Training Loss: 0.40678355825319884 | Training Accuracy: 0.17045703125\n",
      "Epoch 8, Batch: 9000| Training Loss: 0.4067523973782857 | Training Accuracy: 0.17021354166666666\n",
      "Epoch 8, Batch: 10000| Training Loss: 0.4067803908973932 | Training Accuracy: 0.1702421875\n",
      "Epoch 8, Batch: 11000| Training Loss: 0.40670261909474026 | Training Accuracy: 0.17033522727272726\n",
      "Epoch 8, Batch: 12000| Training Loss: 0.40677089051157234 | Training Accuracy: 0.17027994791666667\n",
      "Epoch 8, Batch: 13000| Training Loss: 0.4067146525360071 | Training Accuracy: 0.17042908653846153\n",
      "Epoch 8, Batch: 14000| Training Loss: 0.40670139783620834 | Training Accuracy: 0.17046763392857142\n",
      "Epoch 8, Batch: 15000| Training Loss: 0.4066473125477632 | Training Accuracy: 0.17040208333333334\n",
      "Epoch 8, Batch: 16000| Training Loss: 0.40668028490431607 | Training Accuracy: 0.1703994140625\n",
      "Epoch 8, Batch: 17000| Training Loss: 0.40664030805756063 | Training Accuracy: 0.170359375\n",
      "Epoch 8, Batch: 18000| Training Loss: 0.40651118942267367 | Training Accuracy: 0.17035763888888888\n",
      "Epoch 8, Batch: 19000| Training Loss: 0.40638401185681944 | Training Accuracy: 0.17037171052631578\n",
      "Epoch 8, Batch: 20000| Training Loss: 0.4063801406905055 | Training Accuracy: 0.17038515625\n",
      "Epoch 8, Batch: 21000| Training Loss: 0.40632851780596235 | Training Accuracy: 0.1702797619047619\n",
      "Epoch 8, Batch: 22000| Training Loss: 0.4062115273285996 | Training Accuracy: 0.17032457386363636\n",
      "Epoch 8, Batch: 23000| Training Loss: 0.40611448290166646 | Training Accuracy: 0.17041779891304348\n",
      "Epoch 8, Batch: 24000| Training Loss: 0.4060638549414774 | Training Accuracy: 0.170306640625\n",
      "Epoch 8, Batch: 25000| Training Loss: 0.40598198451757433 | Training Accuracy: 0.1703375\n",
      "Epoch 8, Batch: 26000| Training Loss: 0.40593504187235463 | Training Accuracy: 0.1703425480769231\n",
      "Epoch 8, Batch: 27000| Training Loss: 0.4058843576102345 | Training Accuracy: 0.17032233796296295\n",
      "Epoch 8, Batch: 28000| Training Loss: 0.4059040178688509 | Training Accuracy: 0.1703794642857143\n",
      "Epoch 8, Batch: 29000| Training Loss: 0.4058912663274798 | Training Accuracy: 0.17024407327586208\n",
      "Epoch 8, Batch: 30000| Training Loss: 0.40584704039990904 | Training Accuracy: 0.17026041666666666\n",
      "Epoch 8, Batch: 31000| Training Loss: 0.4057465207807479 | Training Accuracy: 0.17031804435483872\n",
      "Epoch 8, Batch: 32000| Training Loss: 0.405686811638996 | Training Accuracy: 0.170361328125\n",
      "Epoch 8, Training Loss: 0.405643593790698, Validation Error: 80.3015122212643, Validation Top-3 Accuracy: 0.0, Training Error: 82.96203237073087\n",
      "Epoch 9, Batch: 1000| Training Loss: 0.40359382882714273 | Training Accuracy: 0.170859375\n",
      "Epoch 9, Batch: 2000| Training Loss: 0.40209779070317747 | Training Accuracy: 0.17165625\n",
      "Epoch 9, Batch: 3000| Training Loss: 0.40176233504215875 | Training Accuracy: 0.17205208333333333\n",
      "Epoch 9, Batch: 4000| Training Loss: 0.4011815617009997 | Training Accuracy: 0.17225390625\n",
      "Epoch 9, Batch: 5000| Training Loss: 0.4013022011935711 | Training Accuracy: 0.172146875\n",
      "Epoch 9, Batch: 6000| Training Loss: 0.4013112168113391 | Training Accuracy: 0.17229166666666668\n",
      "Epoch 9, Batch: 7000| Training Loss: 0.4012257807127067 | Training Accuracy: 0.172140625\n",
      "Epoch 9, Batch: 8000| Training Loss: 0.4011548073776066 | Training Accuracy: 0.171728515625\n",
      "Epoch 9, Batch: 9000| Training Loss: 0.4011797513928678 | Training Accuracy: 0.17174131944444446\n",
      "Epoch 9, Batch: 10000| Training Loss: 0.4011004794239998 | Training Accuracy: 0.17150625\n",
      "Epoch 9, Batch: 11000| Training Loss: 0.40107176263765854 | Training Accuracy: 0.17124147727272726\n",
      "Epoch 9, Batch: 12000| Training Loss: 0.4009432619139552 | Training Accuracy: 0.17124739583333334\n",
      "Epoch 9, Batch: 13000| Training Loss: 0.40092733496656785 | Training Accuracy: 0.17135697115384615\n",
      "Epoch 9, Batch: 14000| Training Loss: 0.40085285179104124 | Training Accuracy: 0.17142075892857142\n",
      "Epoch 9, Batch: 15000| Training Loss: 0.40073411579529444 | Training Accuracy: 0.1713375\n",
      "Epoch 9, Batch: 16000| Training Loss: 0.4006780085042119 | Training Accuracy: 0.1711943359375\n",
      "Epoch 9, Batch: 17000| Training Loss: 0.4006867782876772 | Training Accuracy: 0.17124632352941177\n",
      "Epoch 9, Batch: 18000| Training Loss: 0.40055856409006646 | Training Accuracy: 0.17150347222222223\n",
      "Epoch 9, Batch: 19000| Training Loss: 0.4005081440806389 | Training Accuracy: 0.17154276315789474\n",
      "Epoch 9, Batch: 20000| Training Loss: 0.4004583315983415 | Training Accuracy: 0.171709375\n",
      "Epoch 9, Batch: 21000| Training Loss: 0.4004434190974349 | Training Accuracy: 0.1716763392857143\n",
      "Epoch 9, Batch: 22000| Training Loss: 0.4003682247833772 | Training Accuracy: 0.17165838068181818\n",
      "Epoch 9, Batch: 23000| Training Loss: 0.4003439983440482 | Training Accuracy: 0.17168410326086955\n",
      "Epoch 9, Batch: 24000| Training Loss: 0.40036042044932646 | Training Accuracy: 0.17168684895833333\n",
      "Epoch 9, Batch: 25000| Training Loss: 0.4002904065859318 | Training Accuracy: 0.1717575\n",
      "Epoch 9, Batch: 26000| Training Loss: 0.40024022464912673 | Training Accuracy: 0.17178004807692307\n",
      "Epoch 9, Batch: 27000| Training Loss: 0.4001693528747117 | Training Accuracy: 0.17175462962962962\n",
      "Epoch 9, Batch: 28000| Training Loss: 0.40018720870358604 | Training Accuracy: 0.17176897321428572\n",
      "Epoch 9, Batch: 29000| Training Loss: 0.400160314121123 | Training Accuracy: 0.17185344827586208\n",
      "Epoch 9, Batch: 30000| Training Loss: 0.4001386026442051 | Training Accuracy: 0.17184947916666668\n",
      "Epoch 9, Batch: 31000| Training Loss: 0.4001117541251644 | Training Accuracy: 0.1718876008064516\n",
      "Epoch 9, Batch: 32000| Training Loss: 0.40006351806223395 | Training Accuracy: 0.17189794921875\n",
      "Epoch 9, Training Loss: 0.40005117684397484, Validation Error: 80.20364890369784, Validation Top-3 Accuracy: 0.0, Training Error: 82.80671243472287\n",
      "Epoch 10, Batch: 1000| Training Loss: 0.39807792943716047 | Training Accuracy: 0.1688125\n",
      "Epoch 10, Batch: 2000| Training Loss: 0.3979387579113245 | Training Accuracy: 0.1719453125\n",
      "Epoch 10, Batch: 3000| Training Loss: 0.39761403929193817 | Training Accuracy: 0.17306770833333332\n",
      "Epoch 10, Batch: 4000| Training Loss: 0.39776186338067054 | Training Accuracy: 0.17268359375\n",
      "Epoch 10, Batch: 5000| Training Loss: 0.39752704250216486 | Training Accuracy: 0.1727125\n",
      "Epoch 10, Batch: 6000| Training Loss: 0.3974840909093618 | Training Accuracy: 0.17301302083333334\n",
      "Epoch 10, Batch: 7000| Training Loss: 0.39747695621848106 | Training Accuracy: 0.17293303571428573\n",
      "Epoch 10, Batch: 8000| Training Loss: 0.3975292502641678 | Training Accuracy: 0.172818359375\n",
      "Epoch 10, Batch: 9000| Training Loss: 0.3973897691004806 | Training Accuracy: 0.17297222222222222\n",
      "Epoch 10, Batch: 10000| Training Loss: 0.39745566565692425 | Training Accuracy: 0.1728140625\n",
      "Epoch 10, Batch: 11000| Training Loss: 0.3973017782948234 | Training Accuracy: 0.17295738636363636\n",
      "Epoch 10, Batch: 12000| Training Loss: 0.3972150003736218 | Training Accuracy: 0.17300520833333333\n",
      "Epoch 10, Batch: 13000| Training Loss: 0.39715123212337494 | Training Accuracy: 0.1729951923076923\n",
      "Epoch 10, Batch: 14000| Training Loss: 0.3971847256485905 | Training Accuracy: 0.172875\n",
      "Epoch 10, Batch: 15000| Training Loss: 0.3972005658427874 | Training Accuracy: 0.17283333333333334\n",
      "Epoch 10, Batch: 16000| Training Loss: 0.39714635092765094 | Training Accuracy: 0.172931640625\n",
      "Epoch 10, Batch: 17000| Training Loss: 0.3970866591965451 | Training Accuracy: 0.1729485294117647\n",
      "Epoch 10, Batch: 18000| Training Loss: 0.3969750682049327 | Training Accuracy: 0.17283506944444443\n",
      "Epoch 10, Batch: 19000| Training Loss: 0.3969324726117285 | Training Accuracy: 0.17281661184210526\n",
      "Epoch 10, Batch: 20000| Training Loss: 0.3969782497242093 | Training Accuracy: 0.1727953125\n",
      "Epoch 10, Batch: 21000| Training Loss: 0.3969911198133514 | Training Accuracy: 0.17285193452380954\n",
      "Epoch 10, Batch: 22000| Training Loss: 0.39689180115407163 | Training Accuracy: 0.17295880681818182\n",
      "Epoch 10, Batch: 23000| Training Loss: 0.3968624649034894 | Training Accuracy: 0.17294904891304347\n",
      "Epoch 10, Batch: 24000| Training Loss: 0.3968027205032607 | Training Accuracy: 0.17290104166666667\n",
      "Epoch 10, Batch: 25000| Training Loss: 0.39678883843302726 | Training Accuracy: 0.172915625\n",
      "Epoch 10, Batch: 26000| Training Loss: 0.3967807819591119 | Training Accuracy: 0.17291165865384617\n",
      "Epoch 10, Batch: 27000| Training Loss: 0.396756119126523 | Training Accuracy: 0.172890625\n",
      "Epoch 10, Batch: 28000| Training Loss: 0.39676333318650725 | Training Accuracy: 0.17297935267857142\n",
      "Epoch 10, Batch: 29000| Training Loss: 0.39674995122798556 | Training Accuracy: 0.1729520474137931\n",
      "Epoch 10, Batch: 30000| Training Loss: 0.3967455799271663 | Training Accuracy: 0.172896875\n",
      "Epoch 10, Batch: 31000| Training Loss: 0.39678996218212187 | Training Accuracy: 0.17282308467741936\n",
      "Epoch 10, Batch: 32000| Training Loss: 0.3967792380815372 | Training Accuracy: 0.17283349609375\n",
      "Epoch 10, Training Loss: 0.3967705671105668, Validation Error: 80.04986369037911, Validation Top-3 Accuracy: 0.0, Training Error: 82.71034008964645\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAHWCAYAAAB0cxiaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB/GElEQVR4nO3dd1hTZ/sH8G8GhA2CylAURARF3KO4B3XW0VrXa92tWneHFVtnq9WOX2vVVt/2bR111dZqh1brrqtu3OLGiajI3sn5/YEcckhAMMBJyPdzXblMzjk5504OMefO8zz3oxAEQQARERERERE9N6XcARAREREREVk6JlZEREREREQmYmJFRERERERkIiZWREREREREJmJiRUREREREZCImVkRERERERCZiYkVERERERGQiJlZEREREREQmYmJFRERERERkIiZWRERkkps3b0KhUGDFihXistmzZ0OhUBTp+QqFArNnzy7RmNq1a4d27dqV6D6JiIgKw8SKiMiK9OzZEw4ODkhKSipwm0GDBsHW1haPHz8uw8iK78KFC5g9ezZu3rwpdyiivXv3QqFQFHhbv3693CESEVEpUcsdABERlZ1Bgwbhjz/+wKZNmzBkyBCD9ampqfjtt9/QpUsXeHh4PPdxpk+fjoiICFNCfaYLFy5gzpw5aNeuHfz8/CTr/v7771I99rNMnDgRTZs2NVgeFhYmQzRERFQWmFgREVmRnj17wtnZGWvXrjWaWP32229ISUnBoEGDTDqOWq2GWi3fV4ytra1sxwaA1q1b49VXXy3Wc3Q6HTIzM2FnZ2ewLiUlBY6OjibFlJqaCgcHB5P2QUREBWNXQCIiK2Jvb49XXnkFu3btQmxsrMH6tWvXwtnZGT179kRcXBzeffddhIaGwsnJCS4uLujatStOnz79zOMYG2OVkZGBt956C5UqVRKPcefOHYPnRkdHY+zYsQgKCoK9vT08PDzQt29fSZe/FStWoG/fvgCA9u3bi13t9u7dC8D4GKvY2FiMHDkSnp6esLOzQ/369bFy5UrJNrnjxT7//HN8++23CAgIgEajQdOmTXHs2LFnvu7iUCgUGD9+PNasWYOQkBBoNBps27YNK1asgEKhwL59+zB27FhUrlwZVatWFZ/3zTffiNv7+Phg3LhxiI+Pl+y7Xbt2qFu3Lk6cOIE2bdrAwcEB77//fonGT0REUmyxIiKyMoMGDcLKlSuxYcMGjB8/XlweFxeH7du3Y+DAgbC3t8f58+exefNm9O3bF/7+/njw4AH++9//om3btrhw4QJ8fHyKddzXX38dq1evxn/+8x+0aNECu3fvRvfu3Q22O3bsGA4dOoQBAwagatWquHnzJpYuXYp27drhwoULcHBwQJs2bTBx4kQsWrQI77//PmrXrg0A4r/5paWloV27drh69SrGjx8Pf39//Pzzzxg2bBji4+MxadIkyfZr165FUlISRo8eDYVCgU8//RSvvPIKrl+/Dhsbm2e+1qSkJDx69MhguYeHhyTh3L17t3geKlasCD8/P0RGRgIAxo4di0qVKmHmzJlISUkBkJOwzpkzB+Hh4XjzzTcRFRWFpUuX4tixYzh48KAktsePH6Nr164YMGAAXnvtNXh6ej4zbiIiMoFARERWJTs7W/D29hbCwsIky5ctWyYAELZv3y4IgiCkp6cLWq1Wss2NGzcEjUYjfPjhh5JlAITly5eLy2bNmiXof8VERkYKAISxY8dK9vef//xHACDMmjVLXJaammoQ8+HDhwUAwqpVq8RlP//8swBA2LNnj8H2bdu2Fdq2bSs+XrhwoQBAWL16tbgsMzNTCAsLE5ycnITExETJa/Hw8BDi4uLEbX/77TcBgPDHH38YHEvfnj17BAAF3u7fvy9uC0BQKpXC+fPnJftYvny5AEBo1aqVkJ2dLS6PjY0VbG1thU6dOknOy5IlSwQAwg8//CB5/QCEZcuWFRovERGVHHYFJCKyMiqVCgMGDMDhw4cl3evWrl0LT09PdOzYEQCg0WigVOZ8TWi1Wjx+/BhOTk4ICgrCyZMni3XMrVu3Asgp6qBv8uTJBtva29uL97OysvD48WPUrFkTbm5uxT6u/vG9vLwwcOBAcZmNjQ0mTpyI5ORk7Nu3T7J9//79UaFCBfFx69atAQDXr18v0vFmzpyJHTt2GNzc3d0l27Vt2xZ16tQxuo833ngDKpVKfLxz505kZmZi8uTJ4nnJ3c7FxQVbtmyRPF+j0WD48OFFipeIiEzHxIqIyArlFqdYu3YtAODOnTvYv38/BgwYIF7M63Q6fPnllwgMDIRGo0HFihVRqVIlnDlzBgkJCcU6XnR0NJRKJQICAiTLg4KCDLZNS0vDzJkz4evrKzlufHx8sY+rf/zAwEBJQgLkdR2Mjo6WLK9WrZrkcW6S9eTJkyIdLzQ0FOHh4Qa3/EU1/P39C9xH/nW5MeZ/z2xtbVGjRg2D11ClShXZi3gQEVkTJlZERFaocePGCA4Oxrp16wAA69atgyAIkmqAH3/8Md5++220adMGq1evxvbt27Fjxw6EhIRAp9OVWmwTJkzAvHnz0K9fP2zYsAF///03duzYAQ8Pj1I9rj79liJ9giCU6HH0W+eKs87UfRMRUclj8QoiIis1aNAgzJgxA2fOnMHatWsRGBgomXvpl19+Qfv27fH9999LnhcfH4+KFSsW61jVq1eHTqfDtWvXJC0uUVFRBtv+8ssvGDp0KP7v//5PXJaenm5Q+S5/1cFnHf/MmTPQ6XSSVqtLly6J681dboxRUVGoUaOGuDwzMxM3btxAeHi4XKERERHYYkVEZLVyW6dmzpyJyMhIg7mrVCqVQQvNzz//jLt37xb7WF27dgUALFq0SLJ84cKFBtsaO+7ixYuh1Woly3LndcqfcBnTrVs3xMTE4KeffhKXZWdnY/HixXByckLbtm2L8jJklduVcNGiRZL35/vvv0dCQoLRCotERFR22GJFRGSl/P390aJFC/z2228AYJBYvfTSS/jwww8xfPhwtGjRAmfPnsWaNWskrSVF1aBBAwwcOBDffPMNEhIS0KJFC+zatQtXr1412Pall17Cjz/+CFdXV9SpUweHDx/Gzp074eHhYbBPlUqFTz75BAkJCdBoNOjQoQMqV65ssM9Ro0bhv//9L4YNG4YTJ07Az88Pv/zyCw4ePIiFCxfC2dm52K+pMPv370d6errB8nr16qFevXrPtc9KlSph2rRpmDNnDrp06YKePXsiKioK33zzDZo2bYrXXnvN1LCJiMgETKyIiKzYoEGDcOjQITRr1gw1a9aUrHv//feRkpKCtWvX4qeffkKjRo2wZcsWREREPNexfvjhB1SqVAlr1qzB5s2b0aFDB2zZsgW+vr6S7b766iuoVCqsWbMG6enpaNmyJXbu3InOnTtLtvPy8sKyZcswf/58jBw5ElqtFnv27DGaWNnb22Pv3r2IiIjAypUrkZiYiKCgICxfvhzDhg17rtdTmPwtc7lmzZr13IkVkDOPVaVKlbBkyRK89dZbcHd3x6hRo/Dxxx8XaX4tIiIqPQqhpEfiEhERERERWRmOsSIiIiIiIjIREysiIiIiIiITMbEiIiIiIiIyERMrIiIiIiIiEzGxIiIiIiIiMhETKyIiIiIiIhOV+3msdDod7t27B2dnZygUCrnDISIiIiIimQiCgKSkJPj4+ECpLNk2pnKfWN27d89g8kkiIiIiIrJet2/fRtWqVUt0n+U+sXJ2dgaQ8+a5uLjIHA0REREREcklMTERvr6+Yo5Qksp9YpXb/c/FxYWJFRERERERlcoQIRavICIiIiIiMhETKyIiIiIiIhMxsSIiIiIiIjJRuR9jRURERETmR6vVIisrS+4wqJxRqVRQq9WyTLPExIqIiIiIylRycjLu3LkDQRDkDoXKIQcHB3h7e8PW1rZMj8vEioiIiIjKjFarxZ07d+Dg4IBKlSrJ0rJA5ZMgCMjMzMTDhw9x48YNBAYGlvgkwIVhYkVEREREZSYrKwuCIKBSpUqwt7eXOxwqZ+zt7WFjY4Po6GhkZmbCzs6uzI7N4hVEREREVObYUkWlpSxbqSTHleWoRERERERE5QgTKyIiIiIiIhMxsSIiIiIiKgPt2rXD5MmTxcd+fn5YuHBhoc9RKBTYvHmzyccuqf1QwZhYEREREREVokePHujSpYvRdfv374dCocCZM2eKvd9jx45h1KhRpoYnMXv2bDRo0MBg+f3799G1a9cSPVZ+K1asgJubW6kew5wxsSIiIiIiKsTIkSOxY8cO3Llzx2Dd8uXL0aRJE9SrV6/Y+61UqRIcHBxKIsRn8vLygkajKZNjWSsmVkRmbPW/0Xhz9QlkZuvkDoWIiKhUCIKA1MxsWW5FnaD4pZdeQqVKlbBixQrJ8uTkZPz8888YOXIkHj9+jIEDB6JKlSpwcHBAaGgo1q1bV+h+83cFvHLlCtq0aQM7OzvUqVMHO3bsMHjO1KlTUatWLTg4OKBGjRqYMWMGsrKyAOS0GM2ZMwenT5+GQqGAQqEQY87fFfDs2bPo0KED7O3t4eHhgVGjRiE5OVlcP2zYMPTu3Ruff/45vL294eHhgXHjxonHeh63bt1Cr1694OTkBBcXF/Tr1w8PHjwQ158+fRrt27eHs7MzXFxc0LhxYxw/fhwAEB0djR49eqBChQpwdHRESEgItm7d+tyxlAbOY0VkxqZvPgcAaHPyDgY2qyZzNERERCUvLUuLOjO3y3LsCx92hoPtsy+H1Wo1hgwZghUrVuCDDz4QS8X//PPP0Gq1GDhwIJKTk9G4cWNMnToVLi4u2LJlCwYPHoyAgAA0a9bsmcfQ6XR45ZVX4OnpiSNHjiAhIUEyHiuXs7MzVqxYAR8fH5w9exZvvPEGnJ2d8d5776F///44d+4ctm3bhp07dwIAXF1dDfaRkpKCzp07IywsDMeOHUNsbCxef/11jB8/XpI87tmzB97e3tizZw+uXr2K/v37o0GDBnjjjTee+XqMvb7cpGrfvn3Izs7GuHHj0L9/f+zduxcAMGjQIDRs2BBLly6FSqVCZGQkbGxsAADjxo1DZmYm/vnnHzg6OuLChQtwcnIqdhylSfYWq6SkJEyePBnVq1eHvb09WrRogWPHjgHImUBu6tSpCA0NhaOjI3x8fDBkyBDcu3dP5qiJylZyerbcIRAREVm1ESNG4Nq1a9i3b5+4bPny5ejTpw9cXV1RpUoVvPvuu2jQoAFq1KiBCRMmoEuXLtiwYUOR9r9z505cunQJq1atQv369dGmTRt8/PHHBttNnz4dLVq0gJ+fH3r06IF3331XPIa9vT2cnJygVqvh5eUFLy8vo5Mwr127Funp6Vi1ahXq1q2LDh06YMmSJfjxxx8lLUgVKlTAkiVLEBwcjJdeegndu3fHrl27ivvWAQB27dqFs2fPYu3atWjcuDGaN2+OVatWYd++feK1/61btxAeHo7g4GAEBgaib9++qF+/vriuZcuWCA0NRY0aNfDSSy+hTZs2zxVLaZG9xer111/HuXPn8OOPP8LHxwerV69GeHi4mIWePHkSM2bMQP369fHkyRNMmjQJPXv2FJsFiayBgKJ1VSAiIrI09jYqXPiws2zHLqrg4GC0aNECP/zwA9q1a4erV69i//79+PDDDwEAWq0WH3/8MTZs2IC7d+8iMzMTGRkZRR5DdfHiRfj6+sLHx0dcFhYWZrDdTz/9hEWLFuHatWtITk5GdnY2XFxcivw6co9Vv359ODo6istatmwJnU6HqKgoeHp6AgBCQkKgUuW9R97e3jh79myxjqV/TF9fX/j6+orL6tSpAzc3N1y8eBFNmzbF22+/jddffx0//vgjwsPD0bdvXwQEBAAAJk6ciDfffBN///03wsPD0adPn+ca11aaZG2xSktLw8aNG/Hpp5+iTZs2qFmzJmbPno2aNWti6dKlcHV1xY4dO9CvXz8EBQXhhRdewJIlS3DixAncunXL6D4zMjKQmJgouRERERGReVIoFHCwVctyy+3SV1QjR47Exo0bkZSUhOXLlyMgIABt27YFAHz22Wf46quvMHXqVOzZsweRkZHo3LkzMjMzS+y9Onz4MAYNGoRu3brhzz//xKlTp/DBBx+U6DH05XbDy6VQKKDTld6479mzZ+P8+fPo3r07du/ejTp16mDTpk0Achpjrl+/jsGDB+Ps2bNo0qQJFi9eXGqxPA9ZE6vs7GxotVrY2dlJltvb2+PAgQNGn5OQkACFQlFgKcf58+fD1dVVvOlnxUSWqohja4mIiKgU9evXD0qlEmvXrsWqVaswYsQIMTk7ePAgevXqhddeew3169dHjRo1cPny5SLvu3bt2rh9+zbu378vLvv3338l2xw6dAjVq1fHBx98gCZNmiAwMBDR0dGSbWxtbaHVap95rNOnTyMlJUVcdvDgQSiVSgQFBRU55uLIfX23b98Wl124cAHx8fGoU6eOuKxWrVp466238Pfff+OVV17B8uXLxXW+vr4YM2YMfv31V7zzzjv47rvvSiXW5yVrYuXs7IywsDB89NFHuHfvHrRaLVavXo3Dhw9L/qhypaenY+rUqRg4cGCBTZ7Tpk1DQkKCeNM/eUREREREz8vJyQn9+/fHtGnTcP/+fQwbNkxcFxgYiB07duDQoUO4ePEiRo8eLRmv9Czh4eGoVasWhg4ditOnT2P//v344IMPJNsEBgbi1q1bWL9+Pa5du4ZFixaJLTq5/Pz8cOPGDURGRuLRo0fIyMgwONagQYNgZ2eHoUOH4ty5c9izZw8mTJiAwYMHi90An5dWq0VkZKTkdvHiRYSHhyM0NBSDBg3CyZMncfToUQwZMgRt27ZFkyZNkJaWhvHjx2Pv3r2Ijo7GwYMHcezYMdSuXRsAMHnyZGzfvh03btzAyZMnsWfPHnGduZC9eMWPP/4IQRBQpUoVaDQaLFq0CAMHDoRSKQ0tKysL/fr1gyAIWLp0aYH702g0cHFxkdyIiIiIiErCyJEj8eTJE3Tu3FkyHmr69Olo1KgROnfujHbt2sHLywu9e/cu8n6VSiU2bdqEtLQ0NGvWDK+//jrmzZsn2aZnz5546623MH78eDRo0ACHDh3CjBkzJNv06dMHXbp0Qfv27VGpUiWjJd8dHBywfft2xMXFoWnTpnj11VfRsWNHLFmypHhvhhHJyclo2LCh5NajRw8oFAr89ttvqFChAtq0aYPw8HDUqFEDP/30EwBApVLh8ePHGDJkCGrVqoV+/fqha9eumDNnDoCchG3cuHGoXbs2unTpglq1auGbb74xOd6SpBCKWsC/lKWkpCAxMRHe3t7o378/kpOTsWXLFgB5SdX169exe/dueHh4FHm/iYmJcHV1RUJCApMssjh+ETmfgYiuwRjTNkDmaIiIiEyXnp6OGzduwN/f32A4CFFJKOxvrDRzA9lbrHI5OjrC29sbT548wfbt29GrVy8AeUnVlStXsHPnzmIlVURERERERGVB9nLr27dvhyAICAoKwtWrVzFlyhQEBwdj+PDhyMrKwquvvoqTJ0/izz//hFarRUxMDADA3d0dtra2MkdPVDbMo12ZiIiIiAoie2KVkJCAadOm4c6dO3B3d0efPn0wb9482NjY4ObNm/j9998BAA0aNJA8b8+ePWjXrl3ZB0xERERERJSP7IlVv3790K9fP6Pr/Pz8YCZDwIhkxQmCiYiIiMyb2YyxIiIiIiLrwR/PqbTI9bfFxIqIiIiIyoxKpQIAZGZmyhwJlVepqakAABsbmzI9ruxdAYmIiIjIeqjVajg4OODhw4ewsbExmLuU6HkJgoDU1FTExsbCzc1NTOLLChMrIgvA3hJERFReKBQKeHt748aNG4iOjpY7HCqH3Nzc4OXlVebHZWJFRERERGXK1tYWgYGB7A5IJc7GxqbMW6pyMbEiIiIiojKnVCphZ2cndxhEJYadWomIiIiIiEzExIqIiIiIiMhETKyIiIiIiIhMxMSKyAJwEkUiIiIi88bEioiIiIiIyERMrIgsABusiIiIiMwbEysiIiIiIiITMbEiIiIiIiIyERMrIiIiIiIiEzGxIrIAHGJFREREZN6YWBEREREREZmIiRWRBWBVQCIiIiLzxsSKiIiIiIjIREysiCyAwFFWRERERGaNiRUREREREZGJmFgRERERERGZiIkVERERERGRiZhYEVkAVgUkIiIiMm9MrIiIiIiIiEzExIrIArDBioiIiMi8MbEiIiIiIiIyERMrIiIiIiIiEzGxIiIiIiIiMhETKyJLwLKARERERGaNiRWRBWBaRURERGTemFgRERERERGZiIkVERERERGRiWRPrJKSkjB58mRUr14d9vb2aNGiBY4dOyauFwQBM2fOhLe3N+zt7REeHo4rV67IGDEREREREZGU7InV66+/jh07duDHH3/E2bNn0alTJ4SHh+Pu3bsAgE8//RSLFi3CsmXLcOTIETg6OqJz585IT0+XOXKisrPjwgO5QyAiIiKiQsiaWKWlpWHjxo349NNP0aZNG9SsWROzZ89GzZo1sXTpUgiCgIULF2L69Ono1asX6tWrh1WrVuHevXvYvHmznKETlalLMUlyh0BEREREhZA1scrOzoZWq4WdnZ1kub29PQ4cOIAbN24gJiYG4eHh4jpXV1c0b94chw8fNrrPjIwMJCYmSm5ERERERESlSdbEytnZGWFhYfjoo49w7949aLVarF69GocPH8b9+/cRExMDAPD09JQ8z9PTU1yX3/z58+Hq6irefH19S/11EBERERGRdZN9jNWPP/4IQRBQpUoVaDQaLFq0CAMHDoRS+XyhTZs2DQkJCeLt9u3bJRwxERERERGRlOyJVUBAAPbt24fk5GTcvn0bR48eRVZWFmrUqAEvLy8AwIMH0oH7Dx48ENflp9Fo4OLiIrkRERERERGVJtkTq1yOjo7w9vbGkydPsH37dvTq1Qv+/v7w8vLCrl27xO0SExNx5MgRhIWFyRgtERERERFRHrXcAWzfvh2CICAoKAhXr17FlClTEBwcjOHDh0OhUGDy5MmYO3cuAgMD4e/vjxkzZsDHxwe9e/eWO3SiMtOypofcIRARERFRIWRPrBISEjBt2jTcuXMH7u7u6NOnD+bNmwcbGxsAwHvvvYeUlBSMGjUK8fHxaNWqFbZt22ZQSZCoPHO0lf2jSkRERESFUAiCIMgdRGlKTEyEq6srEhISON6KLI5fxBYAQHhtT/xvaBOZoyEiIiKybKWZG5jNGCsiKky5/v2DiIiIyOIxsSKyADrmVURERERmjYkVkQUo5z12iYiIiCweEysiC8AWKyIiIiLzxsSKyALo2GJFREREZNaYWBFZAOZVREREROaNiRWRBRBYFZCIiIjIrDGxIiIiIiIiMhETKyIiIiIiIhMxsSKyABxjRURERGTemFgRWQAmVkRERETmjYkVERERERGRiZhYEVkAVgUkIiIiMm9MrIgsALsCEhEREZk3JlZEREREREQmYmJFRERERERkIiZWRBaAPQGJiIiIzBsTKyJLwMyKiIiIyKwxsSIiIiIiIjIREysiC8By60RERETmjYkVERERERGRiZhYEVkAzmNFREREZN6YWBEREREREZmIiRWRBWCDFREREZF5Y2JVxhLTs+QOgSyQwL6ARERERGaNiVUZ+vafa6g3+29sOHZb7lCIiIiIiKgEMbEqQx9vvQQAeG/jGZkjIUvD9ioiIiIi88bEioiIiIiIyERMrIgsAIdYEREREZk3JlZEREREREQmYmJFZAHYYEVERERk3phYEVkC9gUkIiIiMmtMrIgsANMqIiIiIvPGxIqIiIiIiMhEsiZWWq0WM2bMgL+/P+zt7REQEICPPvoIgl63p+TkZIwfPx5Vq1aFvb096tSpg2XLlskYNRERERERkZRazoN/8sknWLp0KVauXImQkBAcP34cw4cPh6urKyZOnAgAePvtt7F7926sXr0afn5++PvvvzF27Fj4+PigZ8+ecoZPVGY4xIqIiIjIvMnaYnXo0CH06tUL3bt3h5+fH1599VV06tQJR48elWwzdOhQtGvXDn5+fhg1ahTq168v2YaIiIiIiEhOsiZWLVq0wK5du3D58mUAwOnTp3HgwAF07dpVss3vv/+Ou3fvQhAE7NmzB5cvX0anTp2M7jMjIwOJiYmSG5GlE1i+goiIiMisydoVMCIiAomJiQgODoZKpYJWq8W8efMwaNAgcZvFixdj1KhRqFq1KtRqNZRKJb777ju0adPG6D7nz5+POXPmlNVLICoT7ApIREREZN5kbbHasGED1qxZg7Vr1+LkyZNYuXIlPv/8c6xcuVLcZvHixfj333/x+++/48SJE/i///s/jBs3Djt37jS6z2nTpiEhIUG83b59u6xeDlGpYWJFREREZN5kbbGaMmUKIiIiMGDAAABAaGgooqOjMX/+fAwdOhRpaWl4//33sWnTJnTv3h0AUK9ePURGRuLzzz9HeHi4wT41Gg00Gk2Zvg4iIiIiIrJusrZYpaamQqmUhqBSqaDT6QAAWVlZyMrKKnQbIiIiIiIiucnaYtWjRw/MmzcP1apVQ0hICE6dOoUvvvgCI0aMAAC4uLigbdu2mDJlCuzt7VG9enXs27cPq1atwhdffCFn6ERlij0BiYiIiMybrInV4sWLMWPGDIwdOxaxsbHw8fHB6NGjMXPmTHGb9evXY9q0aRg0aBDi4uJQvXp1zJs3D2PGjJExcqKyJXCQFREREZFZkzWxcnZ2xsKFC7Fw4cICt/Hy8sLy5cvLLigiIiIiIqJiknWMFRERERERUXnAxIqIiIiIiMhETKyIiIiIiIhMxMSKZJel1WHsmhP48d9ouUMxW6xdQURERGTemFiR7DafuoutZ2MwY/M5uUMxWwILrhMRERGZNSZWJLuk9Gy5QzB7bLEiIiIiMm9MrIiIiIiIiEzExIqIiIiIiMhETKxIdgqF3BGYP/YEJCIiIjJvTKxIdhw/9GwC3yQiIiIis8bEioiIiIiIyERMrIgsANuriIiIiMwbEysiS8DMioiIiMisMbEi2bF4BRERERFZOiZWREREREREJmJiRWQB2BOQiIiIyLwxsSKyACy3TkRERGTemFgRERERERGZiIkVkQVgexURERGReWNiRWQB2BOQiIiIyLwxsSIiIiIiIjIREyuSHaexIiIiIiJLx8SKyAIIHGVFREREZNaYWBFZAI6xIiIiIjJvTKxIdvo5g1bHDIKIiIiILA8TKzIrOy7EyB2CWWKLFREREZF5Y2Ilk8T0LLlDMBv6xSuS0rNli4OIiIiI6HkxsZIJWyCM0/GNISIiIiILxMSKzIpWJ3cE5ulufJrcIRARERFRIZhYyYUNM0axxYqIiIiILBETK5lwXqI8CkXeKCsmVkRERERkiZhYySQtSyt3CGaJ5daJiIiIyBIxsZLJd//cMFiWma1DepYWSVZWMVDQa6ViXkVEREREloiJlUx+OHgDKRl5pcUfJWeg1vS/EDxjG1p/ugepmdZZdlzHzIqIiIiILJCsiZVWq8WMGTPg7+8Pe3t7BAQE4KOPPpK0YADAxYsX0bNnT7i6usLR0RFNmzbFrVu3ZIq65ITM2i7eX3U4Wrwfn5qFk9HxMkRkKD41E6+vPAa/iC1YdfhmqRxDf4yVlmOsiIiIiMgCqeU8+CeffIKlS5di5cqVCAkJwfHjxzF8+HC4urpi4sSJAIBr166hVatWGDlyJObMmQMXFxecP38ednZ2coZeopIzsrFo1xXJstE/Hsf5D7vIFFFO97x/r8dh4Hf/istm/nYeM387DwBY9lojdKnrXeLHZfEKIiIiIrJEsiZWhw4dQq9evdC9e3cAgJ+fH9atW4ejR4+K23zwwQfo1q0bPv30U3FZQEBAmcdamj7ddslgWUqmFvfi0+DjZl/m8VyNTUL4F/8Uus2Y1ScBAK80rIK3XqwFX3eHEjk2uwISERERkSWStStgixYtsGvXLly+fBkAcPr0aRw4cABdu3YFAOh0OmzZsgW1atVC586dUblyZTRv3hybN28ucJ8ZGRlITEyU3Mzd0RtxRpe3WLBbMg6rLHy956pBUrXstcb4akADo9v/euouWn+6B9/svVoix+cEwURERERkiWRNrCIiIjBgwAAEBwfDxsYGDRs2xOTJkzFo0CAAQGxsLJKTk7FgwQJ06dIFf//9N15++WW88sor2Ldvn9F9zp8/H66uruLN19e3LF/Sc7kUk1TgupBZ2zF8+VEkpJZupcAsrQ6XHyThs+1RkuXHPghHl7pe6NWgCg5MbY9LH3XB5nEtDZ7/6bYo+EVswR+n75kUB7sCEpnmv/uu4a2fItn6S0REVMZkTaw2bNiANWvWYO3atTh58iRWrlyJzz//HCtXrgSQ02IFAL169cJbb72FBg0aICIiAi+99BKWLVtmdJ/Tpk1DQkKCeLt9+3aZvZ7nodMJCPZyFh9fndcVNiqFZJs9UQ/x3sbTpXJ8QRAw/6+LCPzgL3T6Mq+l6tvBjXFzQXdUctaIy6pWcICdjQoNfN1wc0F3XPqoC3a901ayvwnrTuGFj3dh18UHSMnINihEAgAxCenYfj4GV2OTsedSLH45cUdcx8SKyDTz/7qETafu4uC1R3KHQkREZFVkHWM1ZcoUsdUKAEJDQxEdHY358+dj6NChqFixItRqNerUqSN5Xu3atXHgwAGj+9RoNNBoNEbXyW1suwB8s/eaZFmWToeKThoASZj3cl2oVUpkaQ2Ti+3nH+BRcsbTbUvG9vMxGP3jCYPl818JRacQr2c+385GhYBKTrg8tysGf38ER552aYxJTMfIlcfF7Ya18ENSeja8XDVwtrPBgr8Mx5TlYmJFVDLSMjkJORERUVmSNbFKTU2FUiltNFOpVGJLla2tLZo2bYqoKGn3tMuXL6N69eplFmdJqVfVzWBZtlZAytM5q3KTptaBFbH/iuGvzU3m7sTNBd1NikEQBGTrBKw7ekus8Jdf/ybF6z5pq1bip9FhSMvUov3nexGTmC5Zv+LQzSLvi2OsiIiIiMgSyZpY9ejRA/PmzUO1atUQEhKCU6dO4YsvvsCIESPEbaZMmYL+/fujTZs2aN++PbZt24Y//vgDe/fulS/wEpStFcRflh1tc07H1C7BcNJcxc6LDwxar7K1Omw7H4Pxa0/hlzFhaOLnXuRjXXmQhBe/NKz217O+D97tFIRsnQ7+FR0l80oVh72tCv++3xGpmdn4777r+OfKQ5y6FW902zWvN8e2czGo5eWMBlXd0GNJTgskW6yIiIiIyBLJmlgtXrwYM2bMwNixYxEbGwsfHx+MHj0aM2fOFLd5+eWXsWzZMsyfPx8TJ05EUFAQNm7ciFatWskYecnJ0unEFit7WxUAoG4VVyx9rbG4zZAfjuKfyw8BAEduxGH82lMAgFeXHcZnr9ZD3yK2MH2w6ZzBsp1vt0XNyk4mvYb8HGzVeOvFWnjrxVrQ6QTcjU+Dr7sDtDoBT1Iz4aRRw85GhZY1KwIAHiVniM/VcsA9UYl43h9IiIiI6PnImlg5Oztj4cKFWLhwYaHbjRgxQtKKZakaVnMzWCZpsdKojD5v1Yhm8IvYAgAY9L8jknVTfjlTpMQq+nEKjt6UlnWf/0poiSdV+SmVCnGOK5VSYXSMmP7lHxMropJhrHAMERERlR5ZEytr4+lihzda++O7/TfEZVlaHVKfJlYONs93OtKztMjWCfjhwA30bVIV3q45kwrviYrF8OXHDLaPmtsFGrXxJE4O+r+ssysgEREREVkiWcutW6MPutfB5bld4azJSaIkiVUBLVYA8M2gRpLH/2leTbwfOns7Zm4+hy92XMag/x1BYnoWMrK1RpOqrwY0MKukCpC2WGWzxYqIiIiILBATKxnYqpVQP52r6nj0E3G5g23BCU+3UG9JMqVfuS9LK+DXU3cBANcfpqDe7L8RNH2b0f10rettUuylQX8oCCc1LRi7dlFxcIwVERFR2WJiJZPcsUTv/XIGQE5yYfeMlqSPXw7Fh71CMKljIOpVdcXx6eFFOlZuQrb29eawVZv3KWeLVcH41hARERGZL46xkklierbksb2NCkrls39hHhLmJ94vymTB/9e3Pvo0roqPXw4tdoxlRaHXGZDFKwqmEwSowFYIIiIiInPExMpMONg+36n4+6026PTlP7BVKzHzpTqIS8nE2HYBWHfsNoI8ndHMv+jzXMlGL1dgYlUwFvYgIiIiMl9MrMyEugitVcbU8nTGzQXdDZYPfqG6qSGVGQUTqyJhXkVERERkvsx7wI0ViUlMlzsE2UirAupki8McMekkIiIisgxMrEh20nmsZAzEDCk5xxcRERGRRWBiJRM/DwfJY1d7G5kikZ9+ixXLrUupmHQSERERWQQmVjKZ2aOOeN/V3gYnilg6vbxjuXUp/a6AnMeKiIiIyHyxeIVMOgR74tSMF+HmYGP1E3lyHFHBlGyxIiIiIrIITKxkVMHRVu4QzALnsSqYftLJMVZUHNb9cw0REVHZY1dAkh1brIqGiRURERGR+WJiRWaF5dal9HMp5lVUHPxzISIiKltMrEh2+i1WVx4k488z9zD0h6M4fy+BVQL1sMWKiIiIyHwVO7HKysqCWq3GuXPnSiMeskL6Y6ySMrIxfu0p7Lv8EN0XHcCknyLlC8zMMMek4uAYKyIiorJV7MTKxsYG1apVg1arLY14iCT+OH1P7hDMBlvviIiIiMzXc3UF/OCDD/D+++8jLi6upOMhMpClzRt3Ne3Xs/CL2AK/iC3YGxUrY1Rlj10BiYiIiMzXc5VbX7JkCa5evQofHx9Ur14djo6OkvUnT54skeCIAODLHZfxXpdgXLiXiHVHb4nLhy0/hpsLussYWdligxUVh5VPj0dERFTmniux6t27dwmHQdZMpSz8CvDPM/fxXpdgdFu0v4wiMh+CXm03tlhRcfDPhYiIqGw9V2I1a9asko6DrJhKqUAzf3ccvWG8a+mT1Ex8vPWiwXI7G+sqainwSpmIiIjIbD1XYpXrxIkTuHgx54I3JCQEDRs2LJGgyPqE1fAoMLFKSs/Gt/9cN1ienqXD7bhU+Lo7lHZ4ZoFdAak42BWQiIiobD3XT/6xsbHo0KEDmjZtiokTJ2LixIlo3LgxOnbsiIcPH5Z0jGQFitMa80qjKuL9Dv+3txSiMU/sCkhERERkvp4rsZowYQKSkpJw/vx5xMXFIS4uDufOnUNiYiImTpxY0jGSFShOa8yHveqK97O01pNs6HTP3oaIiIiI5PFcXQG3bduGnTt3onbt2uKyOnXq4Ouvv0anTp1KLDiyHvpFGp7FSaNGzcpOuBqbDADwi9iCQxEd4ONmX1rh4cajFExYdxLn7iYCAKZ3r42MbB0qOtmiT6OqUKuef7xXepYWF+8non5VNzxOyYSHoy3i07Lg7miL9Ky8bIotVkRERETm67kSK51OBxsbG4PlNjY20PFndXoOxR0/tGhAQ0mVwBYLduOrAQ0waX0kPupdF4NfqG5yTHfj0zB901nsiTLs3jp3S14xjakbz4r3z8/pDEfNsz9W0Y9T0HnhP5LE6VmYVxERERGZr+dKrDp06IBJkyZh3bp18PHxAQDcvXsXb731Fjp27FiiAZJ1yN8a08zfHT6udtgceU+y/LdxLQEANSs7Gexj0vpIAMCMzefQo5433Bxsi3z8PZdiMXzFMVRwsMHhaR0RPGNbMV9BjpBZ29GqZkXceJSCu/FpaFnTAyNb+ePyg2Qs+OsSGlVzw9XYZCSmZxd731pmVkRERERm67n6Ly1ZsgSJiYnw8/NDQEAAAgIC4O/vj8TERCxevLikYyQrkD9nOHojDpPCa0mWjWkbgPq+bgAAW7USJ2e8WOD+Xvv+SL79C9DpNYvpdAIW/HUJb2+IxMdbL2L4imMAgCepWQUmVT3q+yBqbhecm9MZw1r4oYKDDVoHVjTY7sDVR7gbnwYAOHj1MUasOI4Ff10CAJy8FW80qXqhhjvm9s4bO+Zqn9Mi/ErDvEIdMQnpBb5eIircjUcpGPLDURy5/ljuUIiIqJx6rhYrX19fnDx5Ejt37sSlSzkXjLVr10Z4eHiJBkfWo3uot0FJdf+KjhjVpoa4fEBTX8l6d0dbnJ/TGQO+/Rdn7yZI1uWOhQKAPVGxGLHiGAQBWD68KTYcu42/zsUUKa4WAR5Y2L8BKrvYics0amB2zxDM7hkiLhMEAd0WHcDF+4nGdmNgXPsAvNG6hkGr2mtGujD+euouAGD27+fRpa5XkfZPxHLrUmPXnMTF+4n45/JD3FzQXe5wiIioHCp2YpWVlQV7e3tERkbixRdfxIsvFtxqQFRU9X3dEFjZCVeeFqTINbVLMLrU9YKHoy2qezgaPM9Ro8Zv41qixvtbDdadiI7DqFUn8DglU1w2fPmxAmPIf/wzszvBxc5wLKExCoUCK0c0xaGrj9GxdmXcjU9DzUpOyNYJOH8vEX+cvoe3wmshS6eDu4MtlMriX/U+SGKLFRUde45K3U9IkzsEIiIq54qdWNnY2KBatWrQarWlEQ9ZsYBKeYlN1Qo5Ff5USgUaVatQ6POUSgX2v9ceSenZqFHJUezK12fp4SIfu5anE/6c0BoKBbD17H10qesFjVpVrPgrO9uh99Oue8FeOQmZWgU0rl4BjasX/hqIiIiIyLI9V1fADz74AO+//z5+/PFHuLu7l3RMZKXUqrxWnBAfl2I919fd4ZnbNK5eASeinwAAVgxvinZBlY1u16tBFaPL5cYWCCIiIiLz9VyJ1ZIlS3D16lX4+PigevXqcHSUdtE6efJkiQRH1sVGby4o1XN0lcu1aGBDTFx3Snxcs7ITEtOy8MuYMKw7ehsxCWloE1jJpFiJzB3HWBEREZWt50qsevfuXSIH12q1mD17NlavXo2YmBj4+Phg2LBhmD59OhRGrgrGjBmD//73v/jyyy8xefLkEomBzIdaL5kqTqn0/HrW90GnOp74+cQdhNXwkJRm/0/zaibFSERERERkTLETq+zsbCgUCowYMQJVq1Y16eCffPIJli5dipUrVyIkJATHjx/H8OHD4erqiokTJ0q23bRpE/79919x3iwqf5R6yfSbbQNM2pedjapEJgk2J34ez+7uSERERPLS6YTnKlJFlq/Y81ip1Wp89tlnyM4u/gSn+R06dAi9evVC9+7d4efnh1dffRWdOnXC0aNHJdvdvXsXEyZMwJo1a2BjU7QqbWR59Bsp3Rx4nnNVf5pQ9W3i+4wtiagg8alZevczC9mSiOj5Ldx5GU3m7cTtuFS5QyEZPNcEwR06dMC+fftMPniLFi2wa9cuXL58GQBw+vRpHDhwAF27dhW30el0GDx4MKZMmYKQkJCCdiXKyMhAYmKi5EaWR618rj/Ncqm5PwvEEJUkrY6VYIiodCzceQVxKZkY+sPRZ29M5c5zjbHq2rUrIiIicPbsWTRu3NigeEXPnj2LtJ+IiAgkJiYiODgYKpUKWq0W8+bNw6BBg8RtPvnkE6jVaoOugQWZP38+5syZU/QXQ2ZDv8XKlOIV5U3ue6HjxSBRieAniYhK2/VHKXKHQDJ4rsRq7NixAIAvvvjCYJ1CoSjyHFcbNmzAmjVrsHbtWoSEhCAyMhKTJ0+Gj48Phg4dihMnTuCrr77CyZMnjRazMGbatGl4++23xceJiYnw9WUXKsuQd47VTKxEuWPPtKy3TkRERGS2niux0ul0JXLwKVOmICIiAgMGDAAAhIaGIjo6GvPnz8fQoUOxf/9+xMbGolq1vEpuWq0W77zzDhYuXIibN28a7FOj0UCj0ZRIfFS2HiZliPc56DMPW6yIiIiIzF+xBrJ069YNCQkJ4uMFCxYgPj5efPz48WPUqVOnyPtLTU2FMt9YGpVKJSZugwcPxpkzZxAZGSnefHx8MGXKFGzfvr04oZMF2HnxgdwhmKXcFqtFu69i0a4rMkdDZPnY+EtEZUHgfzZWp1gtVtu3b0dGRl6rwscff4x+/frBzc0NQE4p9qioqCLvr0ePHpg3bx6qVauGkJAQnDp1Cl988QVGjBgBAPDw8ICHh4fkOTY2NvDy8kJQUFBxQicLUNvbBRfvs9hIfvpl6L/YcRlj2gbAVs3iHkREROZMqxOgVrEHjjUpVmKVP/M2NRNfvHgxZsyYgbFjxyI2NhY+Pj4YPXo0Zs6cadJ+yTJxXJVxqnw5VFJ6Fjyc2N2VCqcAP09ERHLK1glQq+SOgsrSc42xKinOzs5YuHAhFi5cWOTnGBtXReXD/FdCMXLlMbzbia2R+vKPN0vPLpkxjkTWSmBdQCIqA5zawfoUqz+RQqEwqM5X1Gp9RM9St4orjrwfzolw81Hl+4ztOB8jUyRE5QOHPRBRWcjW8j8ba1PsroDDhg0Tq+6lp6djzJgx4jxW+uOviKhk5J/TixUTiUyjY2ZFRKUgOSNb8ji7hKpok+UoVmI1dOhQyePXXnvNYJshQ4aYFhERSSjztVjN/O08hoT5yRMMUTnAvIqISkNsYrrkMbsCWp9iJVbLly8vrTiIqAD5W6yIyDRssSKi0pB/eEw2Eyurw5rNRGaOiRVRyWJeRUSlIf+3NVusrA8TKyIzl5iWJXcIZImYjxeIiRURlYb89dzYYmV9mFgRmblv91+XOwSyRPw+LxC7AhJRacg/Jjpby+IV1oaJFZGZM3YNaOrk3ETWjJ8eIioNbLEiJlZEFiiTv4IRPTe2WBFRachfvIJjrKwPEysiM2dsDu7M7JzEKiE1C2fuxJdtQGQZOMaqQGzxJaLSkL/WFFusrE+xyq0TUdlTKRTIznchmJGtg5MgoP6HfwMAvh3cGBtP3kHfxr4Ir+MpR5hEFoN5FRGVBgXyt1ixd4m1YYsVkZlTGim3npGtw7WHKeLjUT+ewPbzD/D6quNIzcw22J6I8vBHZCIqC5cfJMsdApUxJlZEZu6N1v4GyzKytHicnGF0+xWHbpZyRESWjWOsiKgsTPv1rNwhUBljYkVk5no3qGKwbNm+a7jxKMXI1sCn26Lwv/3X0WzeTqRkGG+92nf5IfwituDrPVcly2OT0pHFwhhUzjGvIjLNrcepGPLDURy48kjuUMyKwJqjVo+JFZGZ0+8KaGeT85HdcPwOIgr5JWzulouITcrA+5vytknP0uLaw5xuCUN/OAoA+Gx7FHp/fRC341LhF7EFzebtQuAHf+FRcgbO3knA6dvxBR7j3N0ELNp1hYUAyOIcvMqLQSJTfLLtEv65/BCvfX9E7lDMCr8OicUriMyc/ggrR1s10rMyi/zc3yLvwdXeBqsOR4vLqrk7SLaJvB2P1p/ukSxrMX+3WNJ9evfaGNCsGpw0amRpdcjS6nArLhUvLT4AAPhix2VEznwRbg62RY4rM1sHrU7AvssPcT8hDb9F3kPk0yRu2WuN0KWud5H3RXmY5BbNd/uv4402NeQOg8hi3U9IkzsEs8T/gYmJFZEFsbdVAcZ7ABZIP6kCgFtxqc98jv48WXO3XMTcLRcL3b7BhzvE+0Gezoh6kAQAmPdyXRy6+hi7L8UiLUtbpHjHrD75zG3qVnHB4oGN4F/RsUj7JNLHix8i0+Sfr4mIcrArIJEFcbQt3d9CmvpVMHkfuUkVAHyw6Ry2nL1f5KSqqM7dTUT7z/di1eGbJbpfsg5s2CMyDdMq49hrgJhYEVkQe1uVwbL3ugQBAKq42UuWDwmrXui+lg9viq51vbBvSjv8Mb4V3mjtjw2jw+CsyUvevv5Po0L3ETW3CwIqFb/VKMTHBXW8XfD1fxrhf0OaYOfbbXBzQXccntahWPuZ+dv5Yh+7PNP/Tn+cXPQuo2TdPt8ehembWb2Mio4NVsYxryJ2BSQyc96ueQmTsYp9I1r6o4KDLdrWqoSjN+Iw+adIAMCAptUwpXMQLsUkoamfOzaeuIN3fj4NAKhRyRHtgyqjfVBlcT+hVV0BAGdmd8Jf52LQ3N8dHk4aNK8RDpVCgQqOxsdQ/TKmBZIzsuHjZg+VUoHbcanQqJVo9vEuAMDIVv54uWEV1K3iWqTXenNBd6RnaWFnk5NEanUCrj9MRqCns7idX8SWZ+7L2s3dcgGvNq4qdxhmilc/+pY8rQ46slUNdq+lIsk/ES4R5WBiRWTm7G1VOPpBR6gUCjSeu1OyrmVND9jZqDCwWTUAQO+GVXDnSSruxqch2MsZSqUCTf3cn25bUXzeX5NaF3g8hUKBbqF5xSMqOmkKja+Co60k6fJ9Whzj5oLuRXyFhnKTKgBQKRWSpIqKJj41S+4QyALod13iVAtUZMyriIxiYkVkASo72xksW/BKKLrXM6yeN75DoNF9eLna4eSMF+GkUcNWzV7AZL3YXYfINMyrjOP/LcSrKyIL1bthFTjb2RTrOe6OtkyqyjF+pxcN36c8vBCk56HV5f3hPErOkDESIvPCKywiCzK1S7B4X7+7HBHR89DPq3ZdjJUtDrIs2XqJVVpmyVZ9tWQCf7axekysiCzIqDY1sH7UC7j4YRe5QzE7MQnpuPmomJN8kVXqVMdT7hDM0ifbLskdAlmg7edj5A7BbLAFmJhYEVkQlVKBF2p4GC27bu1emL8L7T7fi8dW3C2Fc6gUTW1vF7lDMBv8m6Hnof93c/DqIxkjMS/5P03605eQdWBiRUQWT/9LvvHcnXiQmC5jNGTudEwmRHwn6Hno9QRERjarSRZEy/9rrA4TKyKyaIIgSPr7A8A3T+flITJGx2sdIpPojyXiDxV58rcAp3L8mdVhYkVEFk0nGM6/E59mnXM48fKmaNj9LQ/fCjKVjg1WImMfJ2vunm6NmFgRkcXLypZ+nfm42csUCVkCJhN5WMWMnoeDbd7YIbZYFe4hEyurwsSKiCyaIAj4/cw9yTIbFf9ro4LxQjAP3wp6HiE+eQVglEpOF5zL2OdJo2axKWvCqw8isjhV8rVI7bkknX8nNSO7LMMhC8Ncgsg0+glE+6DK8gVidnLeGDcHG3GJloM6rQoTKyKyOLbqvP+6HqdkYnf+xCrLOgcMs/WhaNhiRVRyFGywMqAAUNHJFgATK2vDxIqILI7+9/i3/1w3WH85JqnsgiGLw7wqD98LMhUThzz6nyfV0y6S+YsrUfkma2Kl1WoxY8YM+Pv7w97eHgEBAfjoo4/Eik1ZWVmYOnUqQkND4ejoCB8fHwwZMgT37t17xp6JyFoYm7PqePQT6PhlTwVgVUAi0+h/hvh/bZ7cd0KhUECtzLnEzj8dCJVvsk4J/cknn2Dp0qVYuXIlQkJCcPz4cQwfPhyurq6YOHEiUlNTcfLkScyYMQP169fHkydPMGnSJPTs2RPHjx+XM3QikpNek1VcSqbRTWKTMuDlaldGAZkHVngrGl7n5OHfDJmKiUOe3HxTAUCtyvmi0rIevVWRNbE6dOgQevXqhe7duwMA/Pz8sG7dOhw9ehQA4Orqih07dkies2TJEjRr1gy3bt1CtWrVDPaZkZGBjIy80paJiYml+AqISA76XQEPXXss3lcpFWK3lA//PI9vBjUu48jIEnCMVR6+FfQ89P9s+HkypFAAarErIN8fayJrV8AWLVpg165duHz5MgDg9OnTOHDgALp27VrgcxISEqBQKODm5mZ0/fz58+Hq6irefH19SyN0IjJDrWpWFO9vPRsjYyQl63FyBkb/eBx+EVuw+9IDucOxSP2aVBXv8zqQyDT6nyG2WOXRbwHOnfaDY9Csi6wtVhEREUhMTERwcDBUKhW0Wi3mzZuHQYMGGd0+PT0dU6dOxcCBA+Hi4mJ0m2nTpuHtt98WHycmJjK5IrISn/Wth2bzdskdRpHodAKSMrLham9T4DYvf3MQp27FS5aNWJHXDfqrAQ3Qq0GV0gqxXHFzsBXvc4xVHr4T9Dz0EwiOscqT91+LQkw4M1m8wqrImlht2LABa9aswdq1axESEoLIyEhMnjwZPj4+GDp0qGTbrKws9OvXD4IgYOnSpQXuU6PRQKPRlHboRCQjhZH6vk4aNSo72+HzvvXx7s+nAQAZ2VqznJxRqxPwxqrj+OfyQ6wY3gytAisiPUuLTafuYtqvZ4u8n0nrIzFpfSQ8HG1xYGoHlj0uhGSwPa8DRUwyyVRskTGkUABXY5MBAL8cv8O5vqyIrInVlClTEBERgQEDBgAAQkNDER0djfnz50sSq9ykKjo6Grt37y6wtYqIrIOx/OGHYU0BAG1q5XUH/HbfdUzoGFhGURXdOxsixbm3Xvv+SJGes+CVUPRv6os3V5/EtvPSbo6PUzKxdN81jG0XUOKxlkcs2FCw5QdvYGiYH5RKZulUMHYFNC53vJn+pyfydrwssZA8ZE2sUlNToVRKh3mpVCro9Cqo5CZVV65cwZ49e+Dh4VHWYRKRmTHWMpNbgamyc14lwP/bcdksEqtsrQ6XYpJQs7ITlu27hs2RRZsyolMdTyx9rbE4HwoALBucU5DjUXIGBn13BFEPcubsWrTrCv6333BOLzLE68A8+d+KOX9cwNXYZMx7OVSWeMgysHiFcbmFKmxUSjSpXgHHo59gYDMOR7EmsiZWPXr0wLx581CtWjWEhITg1KlT+OKLLzBixAgAOUnVq6++ipMnT+LPP/+EVqtFTEzOL7Xu7u6wtbUtbPdEZEVslOY537kgCAiZtR0Z2Yb97De+GYaFO69g/5VHAHKqSG2b3AY1Kzs9c78VnTTY/lYbZGt1qPnBXwCA1ExtyQZfjuhf+/FCMI+xt2LNkVtMrKhQbLEyLvvpeCoblQLB3s44Hv2EVQGtjKyJ1eLFizFjxgyMHTsWsbGx8PHxwejRozFz5kwAwN27d/H7778DABo0aCB57p49e9CuXbsyjpiIzIHCSGdAG7Xxrkt7o2LRTsb+7ePXnjKaVB2e1gHervb4cWRzk/avVimxeVxL9P76oEn7sSq8ziEqMSxekSc3iVKrlLBV5YzvZfEK6yJrYuXs7IyFCxdi4cKFRtf7+flxYC0RGTDaFbCAFqsP/7hQZolVUnoWQmf/DQD4+OVQKBXAlrP3Dbb7eUwYvF3tS+y4DXzdcHNBd6RmZqPOzO0ltt/yii1WevhW0HPJ+8Nh8Yo82U+HsqiVCtiqc76TMrKYWFkT8+w7Q0RUTDaqvGzrs1frifevP0rB2xsiS/34J6KfiEkVALy/6Swi9Cr8bRrbAsNa+OGjXiFo6udeKjE42Mr6W5lZk44JkS0Ms8NCHvQ89H+bYGKVJ7dbpFqlgOZpYpWpZRdta8LEiojKBbUq77+zqhUcJOt+PXm3VL/8BUFAn6WHCly/5D8N0bBaBczuGYLBYX6lFgcVDRusiEqOlh8oUW4vK6Uir8Vq9b+35AyJyhgTKyIqF5zt8lprnDSGLTcpmdm4FJOImIT0EjlellaHi/cTcexmHPZGPRSXq5UKnJndSXz8/dAmeKmeT4kc0xKlZWoLTGqT0rPw6tJD+Hx7VKnHIf2FnV1zcvGamJ4Hi1cYl/u+KABce5gsLn+UnCFPQFTm2G+EiCyO0sggK2e9ZMrdybBi6PGbcRix4jgA4OaC7gbrox+nwNvVXvyVUd+6o7fwMCkD+688xKg2AfB2tcNLiw8YbDc0rDrm9KoLAIia2wWPkzPh41ZyY6ksRUa2Fl/tvIJv9l4zWDeylT++P3BDsux49BMs2XNVsuyXMWFoUkpdJvkLe56C3gmtTpCU+SfSp9+FdMuZ+1gyUDA6cbu1Ef9rUSjwx+m8aTWMFTCi8omJFRFZPLVSIflSr+Jmj+Et/bD84E1xWW5SBeS0NtnodR386dgtTN2YMx5KP+kSBAH3EtIxTW+s1LGbefvJ74UaefPsadQqq02qWszfjccpmUbX50+qCvLqssMY0zYAtb2dMWl9JNoHVcKX/RvAzcH0aTZYpOvZMrK1HLNHBcr/28TZuwmoV9VNlljMiZhXIbd6bc6SRTuv4BO9sb9UfvF/TSKyeMZ+We8QXFmSWOk7diMOLWpWFB/nJlUA4BexBQDw9ou18MWOy8WKI7Sqa7G2t3SCIOCPM/fRoKobMrU6dPtqf7FLC7va26CauwPO3k0wWLdsX16L156oh2jw4Q682rgqPnu1XrF/HRckVcyYWeUqqPJuRpYOJZDDkpXgHHqGWtb0wJ6n3cQ3nLjNxMpKMLEiIouT/5pabSSxaly9QoHP/8//juDA1PaoWsEBNx+lGN2mqEnV90Ob4OzdBLSqWdGgaEZ55z9ta6Hrv+hXH8383VG1ggMSUrOw+kg0rsYmo7m/O/wrOqKpnzuURs7d1dhkhH+xz+g+fzlxB7+cuCM+/n18S1RwsEXfZYeRkpmNHW+1hZerXaFxcUxInoLeCXZdosLk/7vhXFY5cn+oUCiAD3vVRetP9zxdLmdUVJaYWBGRxcmfWBm7OHewVePSR13w84k7mLH5nMH6H/+NxshW/mj3+d5nHq9RNTcs7N8QT1Iz0e+/h5GRrcNHvfIq/HWs7fk8L8OiPUwqfDB2/jFSrg42GNe+ZpH2XbOyE7ZObI0J607i2sMULBrYELU8ndBl4X6DbXsukU6M/ML8XTgzuxNc7Gwkyy2xPLRWJyAxLQsVHIvWdLT9fAwu3k/EhA6BJo+PyshmCwQVLH+iYCEfqVKn3xXQ1926fmijHEysiMjiGWuxAgA7GxUGv1Adg1+ojvsJaVi06yrWHc0pfWurUuL8vUTJ9j3r+6CJXwVcuJcIL1c7xKVkopq7A15vXQMAUM3DAVFzu5bui7EAOp2ApvN2Gl3XvZ43vv5PI5OPUcfHBbveaSdZdv3jbuj4xT7cKKCVMVe92X/j+sfdjCbcgGW0WP1y4g7e/fm0+NhWrcTnfesjLTNb7Lr6frdg2KiUmPPHBclzF+68AgBY8EooXm1cVTIVQX4F/ZKezklNcedJKj7YdA6j2tRAS72uw2Q4/xnrnOQQqwLm+/XvhRqlU4iHzA8TKyKyOCNa+uPtDXkXnSrls2eO8Ha1x/xXQsXEavHuq5JKgsuHN0X7oMolH2w5o9UJ+PCP85JlDXzd8NWABqju4Viqx1YqFdjzbjvxsX7y0TnEE0FeLli0KyepqPF+wd0UtVrzSqwEQUBGtg7bzsXg+sNkbDx5F3fj0yTbZGbrMHHdKcmyj7deKnS/Eb/mTFJ99IOO8HDUGG3FKmiCYLZYAe/+fBr/Xo/DvssPjVYSpTwF/YhhfZ52BXz6qH8TX/x0/Dbqs7CH1WBiRUQWp3eDKvkSq6I/d1z7AHy9J6coQlJGNgCgT6OqTKqK4ER0HPosPSxZdumjLrCzUckSz6uNq+LVxlUly27HpWLTqbuFPs/cyq1P+eWMZNzY86riZm+QkAFAs3m74OZgg8iZnYw8yziOsQJiEzn3UIHyfYTM7CMlm7wWq5x/c7vxWkIrOZUMJlZEZPHURWixyhVaxc1gWTp/nX+mNp/uwa24VMmyX8e2kC2pKsinr9ZDo2pumPHb+QK3yT/Q/mpsEsK/+Aeze9TBsJb+JR6TTidIftFPzsjGwG//RUUnW6RkanH0RpzR541rH4ApnYMBAInpWbjyIBk/HbuFl+r5oH5VN8zdcgFaQcCIlv6oW0VakTJbq0Pbz/aKiVZ8ahb8IrZg35R20pbFArsC8jPBaZkKZlC8gpkVAP0xVjl/PLnd1C1lXCeZjokVEVmc/Bc8xRmo36mOYaGJ11uV/MV0ebLjwgODpAoAGlUruPKiXGxUSgwO8xMLi+SKT83Ez8fvYN7WiwYXgeFf/AMAmP3HhRJPrP63/zrmbrkoPnbWqMWWUmPsbHJ+JJjXOxR99FrjXOxs0Lh6BUm1y8/61i9wP2qVEpvHtTQYC9d54T/Y8VZbVK1gD52AAserZXCMldGJyMk4Jg45BP3qFcjrIsn3x3owsSIii5N/YLCxi/6CKJUKXPqoC8Lm70Jzfw8sG9y4pMMrd95YlTcp8jsv1kLPBj6oYmGTH7s52ML9abec0hhilZapxb2ENARUcgIApGRkw8FWJUmqABSYVK15vXmJF0io5KzBzQXdIQgCvtt/HR9vvYT0LJ1YArow7ArIFqvC5J//jC1WUrl/OrktVuwKaD2YWBGR1bGzUeFUMcabWKOHSRl4nJKBIE9nyfIJHQNlish0uS2b+l0BUwppPSqqs3cS0GPJAfFxc393HCmge1+uulVccO5uTlXK/KXpS5pCocCoNgFoHVgJvb8+WKSkiV0B2WJVGHYFNC5/MRiV2GLFHyqsBRMrIiKSSEzPEruQrX2jubhcvyKfJTLWLWfnxQeSbTafugtnO3WR5ya7HZcqSaoAGCRV9X3dsOnNFrgVl4rv9l/HjJfqwM5GBZ1OgEJh2AJbWmp7u+CPCa3Q6cucro/N/N0xtUsw+iw9ZLAtW6zK7rxYovx5lJZ/LgAMi1fkJVYyBURljokVERFJ7LyQl2z857sjAIAQHxf4VyzdcuqlTfX0ake/KmBaprRlZvJPkQCAE9PD4eGkKXR/7/1yGhuOP7ua3+SOgVAqFfCr6Ih5L4eKy+UoUV3L07lIpcNZbj2vOxc9G1uschRcvIKZlbUoRpFiIiIqb24+SoFfxBb4RWxBllaHrWfvS0rZ5zK36n/PI7csv35XwIKGPuQmWAWJTUo3SKpCfFzE+61qVkRgZSe83y0Y7YMtr5Q/W6w4xqowBl0BOYYIQN7Ys9y/ndyE89C1x3KFRGWMLVZEZPGCvZyfvREZ1e7zveL9T7ddwnf7bxjd7uWGVcoootKjzNditfbILby/6azRbfdfeYTkjGw4aaRfk3ujYrE36iGqezhIlr/zYi280aYGUjO1sFEp4KRRW3RXMo6x4hirwhgWr8ipvOlqb2PRf/clJfctWHkoGgAQm8Q50awFEysisnhNS3Hgf3lnZ6NE+tPS2gUlVQDw2gvVyyqkUpN7oZz743pBSVWumIQ01Kycl7Tfi0/DsOXHDLZb+3pztHha0a88tOwBJdditf/KQySmZaN1rYpwtrBkU4aemhYjf/vUiegnGLf2JHo38MHCAQ0LfW6DD/9GfGoWRrT0x7RuwbApzgzvZk4cY/W0K2DVCtJJu2OT0rH631sY0NQXPhZWWZWKpvz8NROR1VKreAX0vEJ8XJ+5zc6325RBJKXPWFXAXA183QyWjVyZU2b+1uNUhM3fhRYLdhts09zfXUyqyoM63jndGUtiHqsL9xIx+PujGLf2JOrN/hsbjt8W191PSMM/lx/idlwqOn25D8v2XTNoBZGdBSWBcvvhYM6PMpsj7xW63fl7CYhPzRKf0+2r/eZ33k2QWxUw909nkl4V1dO34zFh7Sks2nVF/L+Fyh+2WBERWbEHiekGy1oHVsTS1xpj/dFbaFmzoqTVxpIVNlmnl4sdLn7YBamZ2Wg8N6ciYvTjnPnR2nxW8LxPI8rZ5NKOmpwWt3QTi1d89891zNsqncNr6saz6N+0GgBg+PJjuBSTJK5b8Ncl7LjwABvfbGHScYsjNTMbT1KzxDnZbselYuyakzh7N6HMYrBYRciFtDoBsUnp8HbNeX8T0rLQfZG0guaV2GTsu/wQ7YKk4xDf/ikS287H4KdRYQit+uwff8yVi72NeP9E9BOxYujF+4lyhUSljIkVEVm8jSfuYFaPELnDsDiCICA20bDv/w/DmsJGpcTrrWvIEFXpUYldAQ2vCrO0OtjbqmBvW/SufC52arQLqlRi8ZkDe9ucy4LnbbHaGxWLxbuv4kT0E6Pr/SK2YOWIZpKkKteJ6CeYuO4UXm/tj0W7rmLhgAYGY9xM9TApA3+du4+5Wy4i82l3x9FtaqBx9QoY9eOJEj1WeZZ/viZjZv52DmuO3MKPI5uhdWAlfPfPdaPb6Xev7VHfB61qeuDXU3dzHi85gFk96mBYCz8oFApkaXVm3XUw/38t+l2DD1x9JFn347/RGFwOuliTFBMrIrJ4iemmT/JqTf46e1+cTynz6QQr/xvSBNM3n8MfE1qZ9YWLKZRPX5axFqt3OweJ9ze+2UKc28lY2fELH3bGk9Qs+LjaWdSYoaLIfvr38Kxy6w8S0/Fb5F1cf5iCeS+Hit0sjY1Be7dTLXz+92Xx8dAfjha4399P38Pvp3O6k9WdtR1rXm+OFgEeiEvJfGb5+1wnop/gXnwaVh66ieMFJHj6/lvABb85EAQBs38/D1d7G7z1Yi2L+ntbc+QWgJzWyNaBlYqUjP1x+h7+OC3tTjjnjwuY88cFjG9fE8v2XcN7XYIwqk1AqcRsqrx5rHLOk51N3v+luy/FSradsfkcsrJ15a7V29oxsSIii7TkPw0xfu0pucOwSG+uOQkAePmbnOTB29UO4XU8EV6naJPiWir9eazyJw61vfNKpTfUG281cZ30b2zJfxrCwVYNB9vy+fWZ+7rSC2mx2nf5oSQ5+uP0PTTzd0cTI0Vk9r7bDn4VHSWJVa4uIV5Y+lojAMCG47cxdaNhMZFB/zsiefzt4MZoF1QZ5+8loIGvmyTREAQBk3+KxG/PGOdTmNxCJHfj09DSyJi60qLVCQh4f6v4uF1QJeyNeijZpl1wZTSqVqHMYirMs4ZFRT9OEe9ffpCELWfui39Ttb1dMKVzLYxYUbxxRkv2XAUAfLz1Eno3qAJXBxto1CrcjkvFjUcpaB1YUfbEM28eqxwVHGwL3f7DPy8wsSpnyuc3AxGVe54uduJ9GxaveCZBELD84E1JApGrub91VFXUL16RrNfKue6NFyTb6U/cu/183mTJy15rjM4h5Tv5bB1YETsvPpAknrfjUjH0h6MY1tIPQ8L8DFqcUjK12BP1EHvyJQL6kyxf/LALIn49I0l6+jfzFS+E+zethkPXHovr7W1USDNS8j1/d73OIZ5YNLAhDl17jOFGWsuMWfBKKAY0yxnrdeVBEl788h8AwKGIDmKltiplWLFNEATUmv6XZFn+pAoAXvnmEPw8HHDzcWqRqu/ll5qZDY1aJX4Otp69j/sJ6ajt7YxG1SrgVlwqqrjZw95G9czJq5+VWLX9bK94P0srYNzak+LjF2tXRodgTywf1hTDVxR8zm4u6I5hy48afS+afbwLADClcxA+2x4FAAiv7Yn/DW1SeGClLP88Vo4aNaq4SSsD5vfj4ZsYHOZXBtFRWWBiRUQWyV6v73p57bpWkvZfeYQP/7xgdF3FInaxsnS5F4s6AUjOyEmsHG1VCAvwKNLzu9T1KrXYzEXu30LuGKtHyRlYsO0Srj9KwczfzksmQS7MzQXdJY/tbVX4akBDfNmvATaduovQqq6o5SktivLVgIb4Si9ZaPfZHtx8WkCkINvPP0DQ9G1G19mqlDg180U4FjJOK9DT2SDWsrDy0E3M+v08FrwSig3HbxvtnmpM7vuxOfIe0rN08HGzx+ut/SWlu5cfvIE5f+R91p91YW/MVwMaoGtdb9iqjf/fWpRufQWp9PRHsfbBlfG/IU1Q2UWDelXdAAA/H7+Nw9cf49XGVQEA3w1pgrFrTmLHhQdG95WbVAHAzosP4BexBZvGtoCHowau9jZwdbAx+rzSkr/FCgDe6xKESesjC3zOjN/OSxKrlIxsJGdkS348JMvBxIqILJL+oOCCvvwpj37XnPzcnQrvrlJeiBME6wSx5HNKpvGxRPvfa4/WnxZcDbC8cnhavCO3tajJ0wqJufosPSzeH9isGtYdvWWwj3HtCx7/olQq0OfpRfOz7HqnHf7v7yjU93VDQ1833IlPwytPu68W5vLcrmb/f8Ks388DACJ+lXZ/bBdUCVXc7MXxSRvfbIHQKq4GLVoAsO18DIC8UucFKW5SBQCT1kdiEiKNrls9snmhz91/5SFaB1bE/iuPjK73rZCXBObvfty3iS/6NvEVH9uolPhuSBMkZ2TjamwyHG1VYgtjQV7W+xvZ/157+Lo7FLJ1Ccs3xgoANMX4WxQEAa0+2Y0nqVmS+fHIcjCxIiKLpP9lpVaa90WUuRtVzqr/FUS/KuD6Y7cL3bZqBWlXsEsfdSm1uMxJbmJ19m4CEtOzCt12evfaYmLVsqYHGvpWQMNqbgals5+XSqnAe12CxceVXezw77SOuP4wGc1r5BS0aDovL/H7dnBjdAox71bFc3cTMHKl8e5ve95tB/+KjgCAeS+HStZd+7gbPth0FrW9XaBUKjBj8zmTY1EpFfBxs4Onsx2ORz9B+6BKBt05jXnt+yOFrh/8/VHUf1oi/aV63vjzzH3J+qoVip/oOGnU4lxzN+Z3Q6ZWh8sxyeixJKd8+4iW/kYTzDdWHce2yWU3D584j5XesiQjxZW6hHiJiTGQU15eY6NCj/reePL0R5///O+ILK2pZBomVkRkkVR6YwC8XK2jK1txCYKQ98tpAYO6j30QDrWVdKXUrwro4Vh4K51CocAPw5rg5+N38GX/BpIW0vKqUTU3Sbe5uQV0HQVyWlYcNWq8UMMd/16Pw5yeIWUy35mXqx28XHO6SFVy1uCnUS9ApVSgUbUKzxwXVNYys3UGLWcf/nEBD4xMcfD7+JZiUmWMSqnAgj71xMc7LjzAP5cf4qNeIZjzxwVk63UlHNnKHwOa+uJBYgYUCqBmZadidSvT6QR8f+CGwTxkxgxo6mv0R4rTd3LmAhvYrBqW/KcR/CK2iOtMHb+mUCigUasQWtUVp2d1gkIBuNjZYGaPOkjOyMb4tSfFcVmXYpLwxqrj8HKxQ8NqbuhZ3wdqlRLpWVpo1MoSL3aRVxUwb1lAZSfJNo62Kiwb3FjynuSWl8/fAnzjUQqquTtIvu/IvDGxIiKLpNYrWDGnZ10ZIzFfV2KTEVjZCdGPU5FhpBAAkHNxai1yL05ikzKQkpnzK/KwFn4Fbt8h2BMdgst3sQp9rzSqKrZYAcCG43cK3HZEy5xKZqtHNkemVidblcTmNYo2Pq6snYh+goHf/Yu3X6yF0W1qiBfwR2/GSbarUdER/x3cGIGexUtKV41oJt4fHOaHzGwdjt2MQxO/CtCoc85hcfeZS6lU4I02NTCylT/2RMUiLMBDPL/bz8dgtF4BkWfN+5b793Rgant8+McFvN66RrHminsWV3vpGConjRorhjdDWqYWtWfmjL3LHZ/147/ReHvD6QL31S6oEoa28EP7AlpcdToB9xLSsHjXVbzTuRYqOxeWrOZ9P+lXGQWAv99uCwDo38QXPx0vvOW8/ed7AQBj2gbg7wsxmPFSnQLjI/PAxIqILJJ+97/cX7BJKjEtC/7Ttha4/rdxLcswGvmp9H5GXn7wJoCcSX4ph0KBQgs96GsdmDP2Q61SWk2LZ3F8sOksMrN1WPDXJSz46xKuf9wN2nyl9K7O61pi752tWomWJTweR6lUoGNt6Q8LnUO8sPHNMHGsnf0zWnJzxzVWreCAb4eUXcU+e1sVPu9bH+/+XHAild/eqIeSCoTOdmokpWdjTNsALNt3TbLtT8dvo0+jqvjs1XpIz9YiLVOLvVEP9cbM5Z1rhUKBVSOa4ZcTd/BhrxC4PS3BvqBP6DMTq1y5xx++/Jike+D1h8kY/P1RDAmrjtFtA5CepcXmU3ehUAA1KjmhcQEtucLT7tB/nrmH0CpumBweiAeJ6aju4SjZ5mpsMvZdfogBzaqV+GTd5RXfJSKySOwa8WzXHiYbXT6lcxD6NfG1qtYqAEYvMDRW0MWvOByK0JrQzN9d9vmCzEX04xR89OcFzO4ZIo4d+vPMPVyKSZJs997GMxjfvqb4OGpuF4tNSJ00ea1Ez0qs3J/R5bY0vdywCi7dT8Tx6CfQ6gRkZusQ9SDp2U98KndsVP6kKtfGk3ew8aTxVt38hTva1KqENrUqSZY972coITULrg42EAQB7/1yBnfj0zD/r0uITcrAqVtPcPJWvMFzTs/shAXbLmLdUcNE7uDVx+JrfKVhFbQKrGjQsvfx1os4FNGRP2IWgayJlVarxezZs7F69WrExMTAx8cHw4YNw/Tp08U/OEEQMGvWLHz33XeIj49Hy5YtsXTpUgQGBsoZOhGRRRoaVh3j9C7wrInKyIVMcSp2WYOCuvT9OaEVXlqcUyjgdU5oCiCnLHbufE07L8bixvxuUCgURicu/+XEHXG+vVqeTmJ3PUuk/zF6Vre+Mq3Il49KqcD0l+oUuo0gCEjN1MJRo8blB0lYvPsq/jhtfILpSs4aBHk6Y2z7AKz+Nxpbz8YY3Q4AMrILnmBb3x/jW4kFOKZ1Dcb8vy498zkbT97Byw2roOFHOyTLvz9QcHXI+h/+XaR4fj11VxzvpU8nAC/M34W/32qDfy4/xLAWfuIPA/pjeU/eeoI+Sw9BEHIKdPRv6otm/u5FbgkvD2R9pZ988gmWLl2KlStXIiQkBMePH8fw4cPh6uqKiRMnAgA+/fRTLFq0CCtXroS/vz9mzJiBzp0748KFC7CzY+ZMZK0c9b7Qn1WIwFplaQ3nmrGXaSyMOTA28w4TKymVUoF3XqyF/9txWbLcQ68kv08ZTp5rLjaeuAN3J1vJ+Ja/L0gvrFcdjjZoldCX21qQmGZYJc6S6P88YelFXRQKhXjRX8vTGYsHNsTsHnWw/8oj1K3iCo1aif/7Owqze+Z14QOAsBoe+PHfaGw8cQdN/dxx83EqGlV3w6fbogo6lFGhVV1xZnYn2KqUsLNRoVOIFxLSsnD9YTLe3nAaC14JRR0fF/RcclB8zod/XhCT9MK0qVUJ/1wuuMpjdQ8HbJvUBn2WHsKF+4lGt1ErFXjthepYcegmAKDT01L3c7c8u7DJtvMxYuVDhQJ4tVFVzOkVItt4zLIi66s7dOgQevXqhe7dc/qL+vn5Yd26dTh6NGdWd0EQsHDhQkyfPh29evUCAKxatQqenp7YvHkzBgwYIFvsRCQvtUqJE9PDoRMs/8u9tGRpDX81teYeXMbeD0tuOShpiqeXzBM6BhokVvpdukyt6maJ3nk6Vkd/fMtbP0m7S836/TzsbKSJ+r4p7cRWrVzTugWjvLAtpDvjP1Pal2EkJcfDSYPeDauIjxfqTVqdS6FQYEiYH4boTewLoNiJFZBT0TBXbmXIBr5uCK/jKa67uaA7ui/aj/P3chKgGb+dF58zuk0NHLz2COfu5qwLr10Z/xvaFADwJCVT0rI1rIUfpnQOQmJ6FpztbGBvq8LWSa0BAHuiYnE5JgkLd15BRNdgDNUr7JObWD0vQQB+PnEHl2KS8MeEVibty9zJ+lNdixYtsGvXLly+nPMf+OnTp3HgwAF07doVAHDjxg3ExMQgPDxcfI6rqyuaN2+Ow4cPG91nRkYGEhMTJTciKp88nDRWN06oOIwlEknPmJuoPLMxchGosWGLVS79pNs731gKjVqFVSOa4YdhTVDBiluIBUFAepYW2UY+WwCQnpW3/Pj0cFT3cISvuzQR7dWgSv6nWRTJjzP5fqiZ0zNEvF/NQ75ugOWBfsIFAFsmtjbYZnz7mpjWrTb+nNAac3vXRY/6PvhKLxGs4GiLve+2AwD4uNphVo86cNSo4e1qb1CMon1QZYxuG4CLH3WRJFUA8D+9wiO5hWsKEjnzRZyd3QnrR72AUW2kcySevZuA1EzLbrF9FllbrCIiIpCYmIjg4GCoVCpotVrMmzcPgwYNAgDExOQ0IXp6SqvSeHp6iuvymz9/PubMmVO6gRMRWYBkIxNTdjbzCVRLk7F5ggr7xd2aKfWunj99On9SYd3crEXXr/YbFKYwxt3RFhWdcn702f9eB8mcRZZPYeReTmtI/6a++OfyQ7QLZknw0hA1twuCpm8TH7/ZLkC8/9oL1fHaC9UNnuNX0dHkiYbD63jiYEQHeLvYQalUQKsTcCsuFX4eDlAoFNDpBJy6/QShVdzEudteqOGBF2p44P1utQHktIi1CPAo970EZP1G2bBhA9asWYO1a9fi5MmTWLlyJT7//HOsXLnyufc5bdo0JCQkiLfbt4tWypKIqLxZtu+6wbL8875Yu5KcU8fS6V8k64/h6NukatkHY6aMJVU73mpjsCwuJVPy+OcxYQjydMaa15uXWmxyWT6sKUa09Mf07rVhZ6PC98OaYrCRC3wynUatEluBwmp4lGlRiCpu9mJlVZVSAf+KjmLRCqVSgcbV3Q0mxNbXPqhyuU+qAJlbrKZMmYKIiAhxrFRoaCiio6Mxf/58DB06FF5eOb+sPnjwAN7e3uLzHjx4gAYNGhjdp0ajgUbDrkFERJlGuivl715i7bxdrW+8UEH0u3hVreCAm49Tny634oF5RVDJWYP3ugRJxteMbivtAtXUzx3bjSRglkj/z0GhUKB9cGW0ZwtVmZnWNRh9GlVFzcpOcodCRsjaYpWamgqlUhqCSqWCTpdzMeDv7w8vLy/s2rVLXJ+YmIgjR44gLCysTGMlIioPijJPkTXhvCzGuTowAS8qe1sV0jO1kmVvhdeSKZrSV8gQKyoDCoUCQV7OnMvRTMnaYtWjRw/MmzcP1apVQ0hICE6dOoUvvvgCI0aMAJDzxzN58mTMnTsXgYGBYrl1Hx8f9O7dW87QiYgshrujrdg1Sc4JO81BA183RN6OFx8XpWyxtVDwMvm52KqU0ArSYv7WUqmUjZlEUrImVosXL8aMGTMwduxYxMbGwsfHB6NHj8bMmTPFbd577z2kpKRg1KhRiI+PR6tWrbBt2zbOYUVEVERxKZk4N6czFIA4qaO1craTfu3xV1/j+K4UXW7p7a/3XJM7lDLBrqFEBZP1G9bZ2RkLFy5EdHQ00tLScO3aNcydOxe2tnm/qCoUCnz44YeIiYlBeno6du7ciVq1ym8TOxFRaXDSqMt0oLOlUCutO9EsSG1vF7lDsCieLnbYMDpniEL+5L28kXQFZI5FJFG+P/1ERISxeiV5SYotVoCdjRLpWTq8UMNDXDaylT/Ss7TowKIERdbM3x0bRoehuhXN38Tuo0RSTKyIiMqRYS38sOLQTcmy/OM/iPQdn/4i4lMzUbVCXkJgZ6PCO52CZIzKMkzoUFPyuJm/u0yRlB1pVUD54iAyR+wDQURUjmiMzCNy50maDJGQpXDSqCVJFRVOf6Jka+wyyVYqooIxsSIiKkd0RlqnUjOyZYjEPNWr6ire3/l2+ZhXiMpWQmre5L++TEiJSA+7AhIRlSM6I73+kplYiSZ0CISdWoUXQzxRs7Kz3OGQBUrJ1GLD6DDciktFqF6ibi3yTxBMRHmYWBERlSPGhlP5uNmXfSBmys5GhQkdA+UOgyxY1Qr2aObvbhXjqZ6FaRWRFLsCEhGVI8a6Ak7vXkeGSIjKp7m968odAhGZKSZWRETliLHEqpKzRoZIiMqfaV2Drb7QB6sCEhWMiRURUTmSP6+q5ekkTyBE5UzrwIoY3ZZzwuljhUAiKSZWRETlSP45qy4/SJYpEqLyhZNJ59AvWMEWKyIpJlZEROVI7wZV5A6BqFxSM7ECwIIVRIVhYkVEVI4083dHpzqe4uNP+oTKGA1R+cEWK0N8R4ikmFgREZUzNSrljasKreImXyBE5QgTqxwsXkFUMCZWRETljP71n42KVz5EJUGl5CUTkL9gBf9/IdLH/yWIiMoZ/fIV/JWdqGQcuvpI7hDMDlusiKSYWBERlTMrD90U79uo+N88UUl4nJIpdwhmQdIVUL4wiMwSv3GJiMqZ1EyteF/NroBEVIIkHQHZZEUkwcSKiKgcU3NcCBGVEqZVRFL8xiUiKsdYvIKoZLg52MgdgnngfylEBWJiRURUzrzzYi3xPotXEJUMFbu9GeBbQiTFxIqIqJyp4+Mi3mfxCqKSwfFEOfTLrfMtIZLiNy4RUTmmZosVUYlgEpFDWhWQbwqRPiZWRETlGLsCEpUMfpSM4HtCJMHEioionJH8osyf2YlKhJKfJQD5yq3LFgWReWJiRURUzrB7DlHJm9I5SO4QzAJ/rCEqmFruAIiIqGRVqWAvdwhE5crhaR3g7crPVX5MsoikmFgREZUztTydsbB/A3i52skdClG5wKQqj34qJQiCbHGYM+ab1ouJFRFROdS7YRW5QyCicohJw7PxLbJeHGNFRERERMXGroDG8X2xXkysiIiIiKhIWBzn2fgOWS8mVkRERERUNMwanokNVtaLiRURERERFRvzB+PYqme9mFgRERERUZGwNaYI+B5ZLSZWREREREQlhHmV9ZI1sfLz84NCoTC4jRs3DgAQExODwYMHw8vLC46OjmjUqBE2btwoZ8hEREREVotJw7OxVc96yZpYHTt2DPfv3xdvO3bsAAD07dsXADBkyBBERUXh999/x9mzZ/HKK6+gX79+OHXqlJxhExEREVkllhIv2ICmvgCAyeG1ZI6E5CJrYlWpUiV4eXmJtz///BMBAQFo27YtAODQoUOYMGECmjVrhho1amD69Olwc3PDiRMn5AybiIiIyOoxx5L6+OVQ7Hy7DUa3qSF3KCQTsxljlZmZidWrV2PEiBHiryEtWrTATz/9hLi4OOh0Oqxfvx7p6elo165dgfvJyMhAYmKi5EZEREREpmMuVTClUoGalZ3ZqmfFzCax2rx5M+Lj4zFs2DBx2YYNG5CVlQUPDw9oNBqMHj0amzZtQs2aNQvcz/z58+Hq6irefH19yyB6IiIiovKPOQNRwcwmsfr+++/RtWtX+Pj4iMtmzJiB+Ph47Ny5E8ePH8fbb7+Nfv364ezZswXuZ9q0aUhISBBvt2/fLovwiYiIiKwK52siklLLHQAAREdHY+fOnfj111/FZdeuXcOSJUtw7tw5hISEAADq16+P/fv34+uvv8ayZcuM7kuj0UCj0ZRJ3ERERETWhMkUUcHMosVq+fLlqFy5Mrp37y4uS01NBQAoldIQVSoVdDpdmcZHREREROwKSFQY2RMrnU6H5cuXY+jQoVCr8xrQgoODUbNmTYwePRpHjx7FtWvX8H//93/YsWMHevfuLV/ARERERERE+cieWO3cuRO3bt3CiBEjJMttbGywdetWVKpUCT169EC9evWwatUqrFy5Et26dZMpWiIiIiIiIkOyj7Hq1KkTBEEwui4wMBAbN24s44iIiIiIyBh2BSQqmOwtVkRERERkeZhkEUkxsSIiIiKiImFVQKKCMbEiIiIioiJhKxVRwZhYEREREVGxMccikmJiRURERERFwmSKqGBMrIiIiIio+JhlEUkwsSIiIiKiIlHoD7IyPlsOkdViYkVERERERcJGKqKCMbEiIiIiouJjlkUkwcSKiIiIiIqE5daJCsbEioiIiIiKRMHMiqhATKyIiIiIqNgU7AtIJMHEioiIiIiIyERMrIiIiIiIiEzExIqIiIiIiMhETKyIiIiIiIhMxMSKiIiIiIjIREysiIiIiKjYWHmdSIqJFRERERERkYmYWBERERFRsbHBikiKiRUREREREZGJmFgRERERERGZiIkVERERERWbgtUriCSYWBERERFRsQmCIHcIRGaFiRUREREREZGJmFgRERER5fNmuwAAwBut/WWOxHzV8nSWOwQis6IQynk7bmJiIlxdXZGQkAAXFxe5wyEiIiILIAgCrsYmI6CSE5RKjiXSl5CahbQsLbxc7eQOhajYSjM3UJfo3oiIiIjKAYVCgUC2yBjl6mADV9jIHQaR2WFXQCIiIiIiIhMxsSIiIiIiIjIREysiIiIiIiITMbEiIiIiIiIyERMrIiIiIiIiEzGxIiIiIiIiMpGsiZWfnx8UCoXBbdy4ceI2hw8fRocOHeDo6AgXFxe0adMGaWlpMkZNREREREQkJes8VseOHYNWqxUfnzt3Di+++CL69u0LICep6tKlC6ZNm4bFixdDrVbj9OnTUCrZ0EZEREREROZDIQiCIHcQuSZPnow///wTV65cgUKhwAsvvIAXX3wRH3300XPvszRnVyYiIiIiIstRmrmB2TT9ZGZmYvXq1RgxYgQUCgViY2Nx5MgRVK5cGS1atICnpyfatm2LAwcOFLqfjIwMJCYmSm5ERERERESlyWwSq82bNyM+Ph7Dhg0DAFy/fh0AMHv2bLzxxhvYtm0bGjVqhI4dO+LKlSsF7mf+/PlwdXUVb76+vmURPhERERERWTGzSay+//57dO3aFT4+PgAAnU4HABg9ejSGDx+Ohg0b4ssvv0RQUBB++OGHAvczbdo0JCQkiLfbt2+XSfxERERERGS9ZC1ekSs6Oho7d+7Er7/+Ki7z9vYGANSpU0eybe3atXHr1q0C96XRaKDRaEonUCIiIiIiIiPMosVq+fLlqFy5Mrp37y4u8/Pzg4+PD6KioiTbXr58GdWrVy/rEImIiIiIiAoke4uVTqfD8uXLMXToUKjVeeEoFApMmTIFs2bNQv369dGgQQOsXLkSly5dwi+//FLk/ecWPWQRCyIiIiIi65abE5RGYXTZE6udO3fi1q1bGDFihMG6yZMnIz09HW+99Rbi4uJQv3597NixAwEBAUXef1JSEgCwiAUREREREQHIyRFcXV1LdJ9mNY9VadDpdLh37x6cnZ2hUChkjSUxMRG+vr64ffs259QyUzxH5o/nyPzxHJk/niPzx3Nk/niOzJ+xcyQIApKSkuDj4wOlsmRHRcneYlXalEolqlatKncYEi4uLvwAmjmeI/PHc2T+eI7MH8+R+eM5Mn88R+Yv/zkq6ZaqXGZRvIKIiIiIiMiSMbEiIiIiIiIyEROrMqTRaDBr1izOs2XGeI7MH8+R+eM5Mn88R+aP58j88RyZv7I+R+W+eAUREREREVFpY4sVERERERGRiZhYERERERERmYiJFRERERERkYmYWBEREREREZmIiVUZ+vrrr+Hn5wc7Ozs0b94cR48elTukcumff/5Bjx494OPjA4VCgc2bN0vWC4KAmTNnwtvbG/b29ggPD8eVK1ck28TFxWHQoEFwcXGBm5sbRo4cieTkZMk2Z86cQevWrWFnZwdfX198+umnpf3Syo358+ejadOmcHZ2RuXKldG7d29ERUVJtklPT8e4cePg4eEBJycn9OnTBw8ePJBsc+vWLXTv3h0ODg6oXLkypkyZguzsbMk2e/fuRaNGjaDRaFCzZk2sWLGitF9eubB06VLUq1dPnFQxLCwMf/31l7ie58f8LFiwAAqFApMnTxaX8TzJa/bs2VAoFJJbcHCwuJ7nxzzcvXsXr732Gjw8PGBvb4/Q0FAcP35cXM/rBnn5+fkZfI4UCgXGjRsHwMw+RwKVifXr1wu2trbCDz/8IJw/f1544403BDc3N+HBgwdyh1bubN26Vfjggw+EX3/9VQAgbNq0SbJ+wYIFgqurq7B582bh9OnTQs+ePQV/f38hLS1N3KZLly5C/fr1hX///VfYv3+/ULNmTWHgwIHi+oSEBMHT01MYNGiQcO7cOWHdunWCvb298N///resXqZF69y5s7B8+XLh3LlzQmRkpNCtWzehWrVqQnJysrjNmDFjBF9fX2HXrl3C8ePHhRdeeEFo0aKFuD47O1uoW7euEB4eLpw6dUrYunWrULFiRWHatGniNtevXxccHByEt99+W7hw4YKwePFiQaVSCdu2bSvT12uJfv/9d2HLli3C5cuXhaioKOH9998XbGxshHPnzgmCwPNjbo4ePSr4+fkJ9erVEyZNmiQu53mS16xZs4SQkBDh/v374u3hw4fiep4f+cXFxQnVq1cXhg0bJhw5ckS4fv26sH37duHq1aviNrxukFdsbKzkM7Rjxw4BgLBnzx5BEMzrc8TEqow0a9ZMGDdunPhYq9UKPj4+wvz582WMqvzLn1jpdDrBy8tL+Oyzz8Rl8fHxgkajEdatWycIgiBcuHBBACAcO3ZM3Oavv/4SFAqFcPfuXUEQBOGbb74RKlSoIGRkZIjbTJ06VQgKCirlV1Q+xcbGCgCEffv2CYKQc05sbGyEn3/+Wdzm4sWLAgDh8OHDgiDkJNBKpVKIiYkRt1m6dKng4uIinpf33ntPCAkJkRyrf//+QufOnUv7JZVLFSpUEP73v//x/JiZpKQkITAwUNixY4fQtm1bMbHieZLfrFmzhPr16xtdx/NjHqZOnSq0atWqwPW8bjA/kyZNEgICAgSdTmd2nyN2BSwDmZmZOHHiBMLDw8VlSqUS4eHhOHz4sIyRWZ8bN24gJiZGci5cXV3RvHlz8VwcPnwYbm5uaNKkibhNeHg4lEoljhw5Im7Tpk0b2Nraitt07twZUVFRePLkSRm9mvIjISEBAODu7g4AOHHiBLKysiTnKTg4GNWqVZOcp9DQUHh6eorbdO7cGYmJiTh//ry4jf4+crfh5654tFot1q9fj5SUFISFhfH8mJlx48ahe/fuBu8lz5N5uHLlCnx8fFCjRg0MGjQIt27dAsDzYy5+//13NGnSBH379kXlypXRsGFDfPfdd+J6XjeYl8zMTKxevRojRoyAQqEwu88RE6sy8OjRI2i1WskJBQBPT0/ExMTIFJV1yn2/CzsXMTExqFy5smS9Wq2Gu7u7ZBtj+9A/BhWNTqfD5MmT0bJlS9StWxdAzntoa2sLNzc3ybb5z9OzzkFB2yQmJiItLa00Xk65cvbsWTg5OUGj0WDMmDHYtGkT6tSpw/NjRtavX4+TJ09i/vz5But4nuTXvHlzrFixAtu2bcPSpUtx48YNtG7dGklJSTw/ZuL69etYunQpAgMDsX37drz55puYOHEiVq5cCYDXDeZm8+bNiI+Px7BhwwCY3/9z6uK8GCKikjZu3DicO3cOBw4ckDsUyicoKAiRkZFISEjAL7/8gqFDh2Lfvn1yh0VP3b59G5MmTcKOHTtgZ2cndzhkRNeuXcX79erVQ/PmzVG9enVs2LAB9vb2MkZGuXQ6HZo0aYKPP/4YANCwYUOcO3cOy5Ytw9ChQ2WOjvL7/vvv0bVrV/j4+MgdilFssSoDFStWhEqlMqhQ8uDBA3h5eckUlXXKfb8LOxdeXl6IjY2VrM/OzkZcXJxkG2P70D8GPdv48ePx559/Ys+ePahataq43MvLC5mZmYiPj5dsn/88PescFLSNi4sLL2qKwNbWFjVr1kTjxo0xf/581K9fH1999RXPj5k4ceIEYmNj0ahRI6jVaqjVauzbtw+LFi2CWq2Gp6cnz5OZcXNzQ61atXD16lV+jsyEt7c36tSpI1lWu3ZtscsmrxvMR3R0NHbu3InXX39dXGZunyMmVmXA1tYWjRs3xq5du8RlOp0Ou3btQlhYmIyRWR9/f394eXlJzkViYiKOHDkinouwsDDEx8fjxIkT4ja7d++GTqdD8+bNxW3++ecfZGVlidvs2LEDQUFBqFChQhm9GsslCALGjx+PTZs2Yffu3fD395esb9y4MWxsbCTnKSoqCrdu3ZKcp7Nnz0q+zHbs2AEXFxfxSzIsLEyyj9xt+Ll7PjqdDhkZGTw/ZqJjx444e/YsIiMjxVuTJk0waNAg8T7Pk3lJTk7GtWvX4O3tzc+RmWjZsqXBdB+XL19G9erVAfC6wZwsX74clStXRvfu3cVlZvc5es6CHFRM69evFzQajbBixQrhwoULwqhRowQ3NzdJhRIqGUlJScKpU6eEU6dOCQCEL774Qjh16pQQHR0tCEJO2VQ3Nzfht99+E86cOSP06tXLaNnUhg0bCkeOHBEOHDggBAYGSsqmxsfHC56ensLgwYOFc+fOCevXrxccHBxYNrWI3nzzTcHV1VXYu3evpIRqamqquM2YMWOEatWqCbt37xaOHz8uhIWFCWFhYeL63PKpnTp1EiIjI4Vt27YJlSpVMlo+dcqUKcLFixeFr7/+mmWIiygiIkLYt2+fcOPGDeHMmTNCRESEoFAohL///lsQBJ4fc6VfFVAQeJ7k9s477wh79+4Vbty4IRw8eFAIDw8XKlasKMTGxgqCwPNjDo4ePSqo1Wph3rx5wpUrV4Q1a9YIDg4OwurVq8VteN0gP61WK1SrVk2YOnWqwTpz+hwxsSpDixcvFqpVqybY2toKzZo1E/7991+5QyqX9uzZIwAwuA0dOlQQhJzSqTNmzBA8PT0FjUYjdOzYUYiKipLs4/Hjx8LAgQMFJycnwcXFRRg+fLiQlJQk2eb06dNCq1atBI1GI1SpUkVYsGBBWb1Ei2fs/AAQli9fLm6TlpYmjB07VqhQoYLg4OAgvPzyy8L9+/cl+7l586bQtWtXwd7eXqhYsaLwzjvvCFlZWZJt9uzZIzRo0ECwtbUVatSoITkGFWzEiBFC9erVBVtbW6FSpUpCx44dxaRKEHh+zFX+xIrnSV79+/cXvL29BVtbW6FKlSpC//79JfMj8fyYhz/++EOoW7euoNFohODgYOHbb7+VrOd1g/y2b98uADB43wXBvD5HCkEQhOK1cREREREREZE+jrEiIiIiIiIyERMrIiIiIiIiEzGxIiIiIiIiMhETKyIiIiIiIhMxsSIiIiIiIjIREysiIiIiIiITMbEiIiIiIiIyERMrIiIiIiIiEzGxIiIiKoRCocDmzZvlDoOIiMwcEysiIjJbw4YNg0KhMLh16dJF7tCIiIgk1HIHQEREVJguXbpg+fLlkmUajUamaIiIiIxjixUREZk1jUYDLy8vya1ChQoAcrrpLV26FF27doW9vT1q1KiBX375RfL8s2fPokOHDrC3t4eHhwdGjRqF5ORkyTY//PADQkJCoNFo4O3tjfHjx0vWP3r0CC+//DIcHBwQGBiI33//vXRfNBERWRwmVkREZNFmzJiBPn364PTp0xg0aBAGDBiAixcvAgBSUlLQuXNnVKhQAceOHcPPP/+MnTt3ShKnpUuXYty4cRg1ahTOnj2L33//HTVr1pQcY86cOejXrx/OnDmDbt26YdCgQYiLiyvT10lEROZNIQiCIHcQRERExgwbNgyrV6+GnZ2dZPn777+P999/HwqFAmPGjMHSpUvFdS+88AIaNWqEb775Bt999x2mTp2K27dvw9HREQCwdetW9OjRA/fu3YOnpyeqVKmC4cOHY+7cuUZjUCgUmD59Oj766CMAOcmak5MT/vrrL471IiIiEcdYERGRWWvfvr0kcQIAd3d38X5YWJhkXVhYGCIjIwEAFy9eRP369cWkCgBatmwJnU6HqKgoKBQK3Lt3Dx07diw0hnr16on3HR0d4eLigtjY2Od9SUREVA4xsSIiIrPm6Oho0DWvpNjb2xdpOxsbG8ljhUIBnU5XGiEREZGF4hgrIiKyaP/++6/B49q1awMAateujdOnTyMlJUVcf/DgQSiVSgQFBcHZ2Rl+fn7YtWtXmcZMRETlD1usiIjIrGVkZCAmJkayTK1Wo2LFigCAn3/+GU2aNEGrVq2wZs0aHD16FN9//z0AYNCgQZg1axaGDh2K2bNn4+HDh5gwYQIGDx4MT09PAMDs2bMxZswYVK5cGV27dkVSUhIOHjyICRMmlO0LJSIii8bEioiIzNq2bdvg7e0tWRYUFIRLly4ByKnYt379eowdOxbe3t5Yt24d6tSpAwBwcHDA9u3bMWnSJDRt2hQODg7o06cPvvjiC3FfQ4cORXp6Or788ku8++67qFixIl599dWye4FERFQusCogERFZLIVCgU2bNqF3795yh0JERFaOY6yIiIiIiIhMxMSKiIiIiIjIRBxjRUREFou92YmIyFywxYqIiIiIiMhETKyIiIiIiIhMxMSKiIiIiIjIREysiIiIiIiITMTEioiIiIiIyERMrIiIiIiIiEzExIqIiIiIiMhETKyIiIiIiIhM9P/byCOuqJxc8QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_decoder(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('transformer-decoder-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_images/seq-transformer-exp-1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 (different training schema -- only predict last token of sequence and backprop on that alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207314\n"
     ]
    }
   ],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 128\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "model = ChessTransformerTwo(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.65 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_error,train_loss_values, val_error, val_loss_value \u001b[38;5;241m=\u001b[39m train_last_token(\u001b[43mdevice\u001b[49m, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the training error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_error,train_loss_values, val_error, val_loss_value = train_last_token(device, model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, learn_decay)\n",
    "\n",
    "# Plot the training error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('transformer-decoder-last-token-4-22.png')  # This will save the plot as an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 (higher d_model & same dim_feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data with particular batch size\n",
    "# torch.multiprocessing.set_start_method('fork', force=True)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "d_model = 512\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(vocab.id_to_move.keys())\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "model = ChessTransformer(vocab, d_model, nhead, num_layers = num_layers)\n",
    "model = model.to(device)\n",
    "# This ignores loss on pad tokens from the label's perspective\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<PAD>'))  # Assuming you have a PAD token\n",
    "lr = 2e-3\n",
    "weight_decay=1e-7\n",
    "learn_decay = 0.65 # This causes the LR to be 2e-5 by epoch 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Tokenized Scheme -- Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
